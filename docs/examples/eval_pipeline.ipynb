{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semanlink automatic tagging and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook present how to evaluate a neural search pipeline using pairs of query and answers. We will try to automatically tag arXiv papers that were manually automated by François-Paul Servant as part of the [Semanlink](http://www.semanlink.net/sl/home?lang=fr) Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as print\n",
    "from cherche import data, rank, retrieve, eval\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, query_answers = data.arxiv_tags(arxiv_title=True, arxiv_summary=False, comment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `documents` contain a list of tags. Each tag is represented as a dict and contains a set of attributes. We will try to automate the tagging of arXiv documents with a neural search pipeline that will retrieve tags based on their attributes using the title, abstract and comments of the arXiv articles as a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what a pair of responses to a query looks like. For each query, there is a list of relevant document identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' Joint Embedding of Words and Labels for Text Classification',\n",
      "  [{'uri': 'http://www.semanlink.net/tag/deep_learning_attention'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/arxiv_doc'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/nlp_text_classification'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/label_embedding'}]),\n",
      " (' A Survey on Recent Approaches for Natural Language Processing in '\n",
      "  'Low-Resource Scenarios',\n",
      "  [{'uri': 'http://www.semanlink.net/tag/bosch'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/survey'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/arxiv_doc'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/nlp_low_resource_scenarios'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/low_resource_languages'}])]\n"
     ]
    }
   ],
   "source": [
    "print(query_answers[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of attributes each tag has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prefLabel': ['Attention mechanism'],\n",
       " 'type': ['http://www.semanlink.net/2001/00/semanlink-schema#Tag'],\n",
       " 'broader': ['http://www.semanlink.net/tag/deep_learning'],\n",
       " 'creationTime': '2016-01-07T00:58:24Z',\n",
       " 'creationDate': '2016-01-07',\n",
       " 'comment': 'Good explanation is this [blog post by D. Britz](/doc/?uri=http%3A%2F%2Fwww.wildml.com%2F2016%2F01%2Fattention-and-memory-in-deep-learning-and-nlp%2F). (But the best explanation related to attention is to be found in this [post](/doc/2019/08/transformers_from_scratch_%7C_pet) about Self-Attention.) \\r\\n\\r\\nWhile simple Seq2Seq builds a single context vector out of the encoder’s last hidden state, attention creates\\r\\nshortcuts between the context vector and the entire source input: the context vector has access to the entire input sequence.\\r\\nThe decoder can “attend” to different parts of the source sentence at each step of the output generation, and the model learns what to attend to based on the input sentence and what it has produced so far.\\r\\n\\r\\nPossible to interpret what the model is doing by looking at the Attention weight matrix\\r\\n\\r\\nCost: We need to calculate an attention value for each combination of input and output word (D. Britz: -> \"attention is a bit of a misnomer: we look at everything in details before deciding what to focus on\")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n",
       " 'uri': 'http://www.semanlink.net/tag/deep_learning_attention',\n",
       " 'broader_prefLabel': ['Deep Learning'],\n",
       " 'broader_related': ['http://www.semanlink.net/tag/feature_learning',\n",
       "  'http://www.semanlink.net/tag/feature_extraction'],\n",
       " 'broader_prefLabel_text': 'Deep Learning',\n",
       " 'prefLabel_text': 'Attention mechanism'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate a first piepline made of a single retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.TfIdf(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"], \n",
    "    documents = documents,\n",
    "    tfidf = TfidfVectorizer(lowercase=True, min_df=1, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"), \n",
    "    k = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '63.06%',\n",
       " 'Precision@2': '43.47%',\n",
       " 'Precision@3': '33.12%',\n",
       " 'Precision@4': '26.67%',\n",
       " 'Precision@5': '22.55%',\n",
       " 'Recall@1': '16.79%',\n",
       " 'Recall@2': '22.22%',\n",
       " 'Recall@3': '25.25%',\n",
       " 'Recall@4': '27.03%',\n",
       " 'Recall@5': '28.54%',\n",
       " 'F1@1': '26.52%',\n",
       " 'F1@2': '29.41%',\n",
       " 'F1@3': '28.65%',\n",
       " 'F1@4': '26.85%',\n",
       " 'F1@5': '25.19%',\n",
       " 'R-Precision': '26.95%',\n",
       " 'Precision': '13.47%'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.eval(search = retriever, query_answers=query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://www.semanlink.net/tag/information_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm'},\n",
       " {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp'},\n",
       " {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/active_learning'},\n",
       " {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir'},\n",
       " {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking'},\n",
       " {'uri': 'http://www.semanlink.net/tag/information_extraction'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of Lunr are inferior to TfIdf on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '56.55%',\n",
       " 'Precision@2': '42.10%',\n",
       " 'Precision@3': '34.17%',\n",
       " 'Precision@4': '27.80%',\n",
       " 'Precision@5': '23.33%',\n",
       " 'Recall@1': '14.90%',\n",
       " 'Recall@2': '21.52%',\n",
       " 'Recall@3': '25.86%',\n",
       " 'Recall@4': '27.74%',\n",
       " 'Recall@5': '28.59%',\n",
       " 'F1@1': '23.59%',\n",
       " 'F1@2': '28.48%',\n",
       " 'F1@3': '29.44%',\n",
       " 'F1@4': '27.77%',\n",
       " 'F1@5': '25.69%',\n",
       " 'R-Precision': '27.59%',\n",
       " 'Precision': '14.00%'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = retrieve.Lunr(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"], \n",
    "    documents = documents,\n",
    "    k = 10,\n",
    ")\n",
    "\n",
    "eval.eval(search = retriever, query_answers=query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find an explanation of the metrics [here](https://amitness.com/2020/08/information-retrieval-evaluation/). The TfIdf retriever using caracters ngrams did well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what tagging looks like using our retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://www.semanlink.net/tag/information_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir'},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm'},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp'},\n",
       " {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking'},\n",
       " {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve those results using a ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.TfIdf(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"], \n",
    "    documents = documents,\n",
    "    tfidf = TfidfVectorizer(lowercase=True, min_df=1, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"), \n",
    "    k = 30,\n",
    ")\n",
    "\n",
    "ranker = rank.Encoder(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"],\n",
    "    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n",
    "    k = 10,\n",
    "    path = \"all-mpnet-base-v2.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is important if you want to pre-compute the queries once. 314 queries are pre-calculated to speed up evaluation. It takes roughly CPU 24 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t embeddings stored at: all-mpnet-base-v2.pkl"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.add([q for q, _ in query_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfIdf retriever\n",
       " \t key: uri\n",
       " \t on: prefLabel_text, altLabel_text\n",
       " \t documents: 433\n",
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t embeddings stored at: all-mpnet-base-v2.pkl"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = retriever + ranker\n",
    "search.add(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '63.69%',\n",
       " 'Precision@2': '42.83%',\n",
       " 'Precision@3': '33.01%',\n",
       " 'Precision@4': '26.75%',\n",
       " 'Precision@5': '22.55%',\n",
       " 'Recall@1': '17.15%',\n",
       " 'Recall@2': '22.64%',\n",
       " 'Recall@3': '25.75%',\n",
       " 'Recall@4': '27.34%',\n",
       " 'Recall@5': '28.55%',\n",
       " 'F1@1': '27.02%',\n",
       " 'F1@2': '29.63%',\n",
       " 'F1@3': '28.93%',\n",
       " 'F1@4': '27.04%',\n",
       " 'F1@5': '25.20%',\n",
       " 'R-Precision': '27.43%',\n",
       " 'Precision': '13.18%'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.eval(search = search, query_answers = query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bert Sentence classifier improved the results of the extractor a little. We managed to increase the F1@k score, precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are proposed tags for Bert using retriever ranker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm',\n",
       "  'similarity': 0.5449115},\n",
       " {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval',\n",
       "  'similarity': 0.42808777},\n",
       " {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir',\n",
       "  'similarity': 0.42719522},\n",
       " {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval',\n",
       "  'similarity': 0.42641854},\n",
       " {'uri': 'http://www.semanlink.net/tag/information_retrieval',\n",
       "  'similarity': 0.40513232},\n",
       " {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking',\n",
       "  'similarity': 0.352889},\n",
       " {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval',\n",
       "  'similarity': 0.346283},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp',\n",
       "  'similarity': 0.32937074},\n",
       " {'uri': 'http://www.semanlink.net/tag/active_learning',\n",
       "  'similarity': 0.28093448},\n",
       " {'uri': 'http://www.semanlink.net/tag/cognitive_search',\n",
       "  'similarity': 0.25938064}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use using Flash as a retriever. Flash Text will retrieve tags labels inside the title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flash retriever\n",
       " \t key: uri\n",
       " \t on: prefLabel, altLabel\n",
       " \t documents: 605\n",
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t embeddings stored at: all-mpnet-base-v2.pkl"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = retrieve.Flash(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel\", \"altLabel\"], \n",
    "    k = 30,\n",
    ")\n",
    "\n",
    "search = retriever + ranker\n",
    "search.add(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlashText as a retriever provides fewer candidates than TfIdf but has higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '72.80%',\n",
       " 'Precision@2': '61.90%',\n",
       " 'Precision@3': '59.90%',\n",
       " 'Precision@4': '59.27%',\n",
       " 'Precision@5': '59.37%',\n",
       " 'Recall@1': '16.33%',\n",
       " 'Recall@2': '19.54%',\n",
       " 'Recall@3': '20.11%',\n",
       " 'Recall@4': '20.16%',\n",
       " 'Recall@5': '20.20%',\n",
       " 'F1@1': '26.67%',\n",
       " 'F1@2': '29.71%',\n",
       " 'F1@3': '30.11%',\n",
       " 'F1@4': '30.08%',\n",
       " 'F1@5': '30.15%',\n",
       " 'R-Precision': '20.20%',\n",
       " 'Precision': '59.37%'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.eval(search = search, query_answers = query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the best of both worlds by using pipeline union. It gets a bit complicated, but the union allows us to retrieve the best candidates from the first model, then add the candidates from the second model without duplicates (etc, no matter how many models are in the union). Our first retriever and ranker (Flash + Encoder) has low recall and high precision. The second retriever has lower precision but higher recall. We can mix things up and offer Flash and Ranker candidates first, then TfIdf and Ranker candidates seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Union Pipeline\n",
       "-----\n",
       "Flash retriever\n",
       " \t key: uri\n",
       " \t on: prefLabel, altLabel\n",
       " \t documents: 605\n",
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t embeddings stored at: all-mpnet-base-v2.pkl\n",
       "TfIdf retriever\n",
       " \t key: uri\n",
       " \t on: prefLabel_text, altLabel_text\n",
       " \t documents: 433\n",
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t embeddings stored at: all-mpnet-base-v2.pkl\n",
       "-----"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = retrieve.Flash(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel\", \"altLabel\"], \n",
    "    k = 30,\n",
    ") + ranker\n",
    "\n",
    "recall = retrieve.TfIdf(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"], \n",
    "    documents = documents,\n",
    "    tfidf = TfidfVectorizer(lowercase=True, min_df=1, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"), \n",
    "    k = 30,\n",
    ") + ranker\n",
    "\n",
    "search = precision | recall\n",
    "\n",
    "search.add(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '69.11%',\n",
       " 'Precision@2': '49.84%',\n",
       " 'Precision@3': '39.07%',\n",
       " 'Precision@4': '31.13%',\n",
       " 'Precision@5': '25.92%',\n",
       " 'Recall@1': '18.74%',\n",
       " 'Recall@2': '25.89%',\n",
       " 'Recall@3': '30.10%',\n",
       " 'Recall@4': '31.58%',\n",
       " 'Recall@5': '32.57%',\n",
       " 'F1@1': '29.49%',\n",
       " 'F1@2': '34.08%',\n",
       " 'F1@3': '34.00%',\n",
       " 'F1@4': '31.35%',\n",
       " 'F1@5': '28.87%',\n",
       " 'R-Precision': '31.96%',\n",
       " 'Precision': '13.97%'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.eval(search = search, query_answers = query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did improves F1 and recall scores using union of pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our tags for BERT's article with best of both worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm'},\n",
       " {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir'},\n",
       " {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/information_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking'},\n",
       " {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval'},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp'},\n",
       " {'uri': 'http://www.semanlink.net/tag/active_learning'},\n",
       " {'uri': 'http://www.semanlink.net/tag/cognitive_search'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://www.semanlink.net/tag/knowledge_base'},\n",
       " {'uri': 'http://www.semanlink.net/tag/knowledge_distillation'},\n",
       " {'uri': 'http://www.semanlink.net/tag/embeddings'},\n",
       " {'uri': 'http://www.semanlink.net/tag/knowledge_graph_embeddings'},\n",
       " {'uri': 'http://www.semanlink.net/tag/knowledge_driven_embeddings'},\n",
       " {'uri': 'http://www.semanlink.net/tag/hierarchy_aware_knowledge_graph_embeddings'},\n",
       " {'uri': 'http://www.semanlink.net/tag/text_kg_and_embeddings'},\n",
       " {'uri': 'http://www.semanlink.net/tag/text_aware_kg_embedding'},\n",
       " {'uri': 'http://www.semanlink.net/tag/knowledge_graph_completion'},\n",
       " {'uri': 'http://www.semanlink.net/tag/knowledge_graph_deep_learning'},\n",
       " {'uri': 'http://www.semanlink.net/tag/combining_knowledge_graphs'},\n",
       " {'uri': 'http://www.semanlink.net/tag/multiple_knowledge_bases'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"Knowledge Base Embedding By Cooperative Knowledge Distillation\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b170744ab9cf7446ed3e27cb2734f2273f9ffda6b52a7d603d13471f7231bb1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
