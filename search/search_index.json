{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Cherche <p>Neural search</p>"},{"location":"#installation","title":"Installation \ud83e\udd16","text":"<p>To install Cherche for use with a simple retriever on CPU, such as TfIdf, Flash or Lunr, use the following command:</p> <pre><code>pip install cherche\n</code></pre> <p>To install Cherche for use with any semantic retriever or ranker on CPU, use the following command:</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>Finally, if you plan to use any semantic retriever or ranker on GPU, use the following command:</p> <pre><code>pip install \"cherche[gpu]\"\n</code></pre> <p>By following these installation instructions, you will be able to use Cherche with the appropriate requirements for your needs.</p> <p>Links to the documentation:</p> <ul> <li> <p>Retriever</p> </li> <li> <p>Ranker</p> </li> <li> <p>Question answering</p> </li> <li> <p>Pipeline</p> </li> <li> <p>Examples</p> </li> <li> <p>API reference</p> </li> </ul>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#compose","title":"compose","text":"<ul> <li>Intersection</li> <li>Pipeline</li> <li>Union</li> <li>Vote</li> </ul>"},{"location":"api/overview/#data","title":"data","text":"<ul> <li>arxiv_tags</li> <li>load_towns</li> </ul>"},{"location":"api/overview/#evaluate","title":"evaluate","text":"<ul> <li>evaluation</li> </ul>"},{"location":"api/overview/#index","title":"index","text":"<ul> <li>Faiss</li> </ul>"},{"location":"api/overview/#qa","title":"qa","text":"<ul> <li>QA</li> </ul>"},{"location":"api/overview/#query","title":"query","text":"<ul> <li>Norvig</li> <li>PRF</li> <li>Query</li> </ul>"},{"location":"api/overview/#rank","title":"rank","text":"<ul> <li>CrossEncoder</li> <li>DPR</li> <li>Embedding</li> <li>Encoder</li> <li>Ranker</li> </ul>"},{"location":"api/overview/#retrieve","title":"retrieve","text":"<ul> <li>DPR</li> <li>Embedding</li> <li>Encoder</li> <li>Flash</li> <li>Fuzz</li> <li>Lunr</li> <li>Retriever</li> <li>TfIdf</li> </ul>"},{"location":"api/overview/#utils","title":"utils","text":"<p>Classes</p> <ul> <li>TopK</li> </ul> <p>Functions</p> <ul> <li>quantize</li> <li>yield_batch</li> <li>yield_batch_single</li> </ul>"},{"location":"api/compose/Intersection/","title":"Intersection","text":"<p>Intersection gathers retrieved documents from multiples retrievers and ranked documents from multiples rankers only if they are proposed by all models of the intersection pipeline.</p>"},{"location":"api/compose/Intersection/#parameters","title":"Parameters","text":"<ul> <li> <p>models (list)</p> <p>List of models of the union.</p> </li> </ul>"},{"location":"api/compose/Intersection/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"town\": \"Paris\", \"country\": \"France\", \"continent\": \"Europe\"},\n...    {\"id\": 1, \"town\": \"Montreal\", \"country\": \"Canada\", \"continent\": \"North America\"},\n...    {\"id\": 2, \"town\": \"Madrid\", \"country\": \"Spain\", \"continent\": \"Europe\"},\n... ]\n\n&gt;&gt;&gt; search = (\n...     retrieve.TfIdf(key=\"id\", on=\"town\", documents=documents) &amp;\n...     retrieve.TfIdf(key=\"id\", on=\"country\", documents=documents) &amp;\n...     retrieve.Flash(key=\"id\", on=\"continent\")\n... )\n\n&gt;&gt;&gt; search = search.add(documents)\n\n&gt;&gt;&gt; print(search(\"Paris\"))\n[]\n\n&gt;&gt;&gt; print(search([\"Paris\", \"Europe\"]))\n[[], []]\n\n&gt;&gt;&gt; print(search([\"Paris\", \"Europe\", \"Paris Madrid Europe France Spain\"]))\n[[],\n[],\n[{'id': 2, 'similarity': 4.25}, {'id': 0, 'similarity': 3.0999999999999996}]]\n</code></pre>"},{"location":"api/compose/Intersection/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>q     (Union[List[List[Dict[str, str]]], List[Dict[str, str]]])    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>documents     (Optional[List[Dict[str, str]]])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> add <p>Add new documents.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>kwargs </li> </ul> reset"},{"location":"api/compose/Pipeline/","title":"Pipeline","text":"<p>Neurals search pipeline.</p>"},{"location":"api/compose/Pipeline/#parameters","title":"Parameters","text":"<ul> <li> <p>models (list)</p> <p>List of models of the pipeline.</p> </li> </ul>"},{"location":"api/compose/Pipeline/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"town\": \"Paris\", \"country\": \"France\", \"continent\": \"Europe\"},\n...    {\"id\": 1, \"town\": \"Montreal\", \"country\": \"Canada\", \"continent\": \"North America\"},\n...    {\"id\": 2, \"town\": \"Madrid\", \"country\": \"Spain\", \"continent\": \"Europe\"},\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(\n...     key=\"id\", on=[\"town\", \"country\", \"continent\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    key = \"id\",\n...    on = [\"town\", \"country\", \"continent\"],\n... )\n\n&gt;&gt;&gt; pipeline = retriever + ranker\n\n&gt;&gt;&gt; pipeline = pipeline.add(documents)\n\n&gt;&gt;&gt; print(pipeline(\"Paris Europe\"))\n[{'id': 0, 'similarity': 0.9149576}, {'id': 2, 'similarity': 0.8091332}]\n\n&gt;&gt;&gt; print(pipeline([\"Paris\", \"Europe\", \"Paris Madrid Europe France Spain\"]))\n[[{'id': 0, 'similarity': 0.69523287}],\n [{'id': 0, 'similarity': 0.7381397}, {'id': 2, 'similarity': 0.6488539}],\n [{'id': 0, 'similarity': 0.8582063}, {'id': 2, 'similarity': 0.8200009}]]\n\n&gt;&gt;&gt; pipeline = retriever + ranker + documents\n\n&gt;&gt;&gt; print(pipeline(\"Paris Europe\"))\n[{'continent': 'Europe',\n  'country': 'France',\n  'id': 0,\n  'similarity': 0.9149576,\n  'town': 'Paris'},\n {'continent': 'Europe',\n  'country': 'Spain',\n  'id': 2,\n  'similarity': 0.8091332,\n  'town': 'Madrid'}]\n\n&gt;&gt;&gt; print(pipeline([\"Paris\", \"Europe\", \"Paris Madrid Europe France Spain\"]))\n[[{'continent': 'Europe',\n   'country': 'France',\n   'id': 0,\n   'similarity': 0.69523287,\n   'town': 'Paris'}],\n [{'continent': 'Europe',\n   'country': 'France',\n   'id': 0,\n   'similarity': 0.7381397,\n   'town': 'Paris'},\n  {'continent': 'Europe',\n   'country': 'Spain',\n   'id': 2,\n   'similarity': 0.6488539,\n   'town': 'Madrid'}],\n [{'continent': 'Europe',\n   'country': 'France',\n   'id': 0,\n   'similarity': 0.8582063,\n   'town': 'Paris'},\n  {'continent': 'Europe',\n   'country': 'Spain',\n   'id': 2,\n   'similarity': 0.8200009,\n   'town': 'Madrid'}]]\n</code></pre>"},{"location":"api/compose/Pipeline/#methods","title":"Methods","text":"call <p>Pipeline main method. It takes a query and returns a list of documents. If the query is a list of queries, it returns a list of list of documents. If the batch_size_ranker, or batch_size_retriever it takes precedence over the batch_size. If the k_ranker, or k_retriever it takes precedence over the k parameter.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>documents     (Optional[List[Dict[str, str]]])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> add <p>Add new documents.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>kwargs </li> </ul> reset"},{"location":"api/compose/Union/","title":"Union","text":"<p>Union gathers retrieved documents from multiples retrievers and ranked documents from multiples rankers. The union operator concat results with respect of the orders of the models in the union.</p>"},{"location":"api/compose/Union/#parameters","title":"Parameters","text":"<ul> <li> <p>models (list)</p> <p>List of models of the union.</p> </li> </ul>"},{"location":"api/compose/Union/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"town\": \"Paris\", \"country\": \"France\", \"continent\": \"Europe\"},\n...    {\"id\": 1, \"town\": \"Montreal\", \"country\": \"Canada\", \"continent\": \"North America\"},\n...    {\"id\": 2, \"town\": \"Madrid\", \"country\": \"Spain\", \"continent\": \"Europe\"},\n... ]\n\n&gt;&gt;&gt; search = (\n...     retrieve.TfIdf(key=\"id\", on=\"town\", documents=documents) |\n...     retrieve.TfIdf(key=\"id\", on=\"country\", documents=documents) |\n...     retrieve.Flash(key=\"id\", on=\"continent\")\n... )\n\n&gt;&gt;&gt; search = search.add(documents)\n\n&gt;&gt;&gt; print(search(\"Paris\"))\n[{'id': 0, 'similarity': 1.0}]\n\n&gt;&gt;&gt; print(search([\"Paris\", \"Europe\"]))\n[[{'id': 0, 'similarity': 1.0}],\n[{'id': 0, 'similarity': 1.0}, {'id': 2, 'similarity': 0.5}]]\n</code></pre>"},{"location":"api/compose/Union/#methods","title":"Methods","text":"call <p>Parameters</p> <ul> <li>q     (Union[List[List[Dict[str, str]]], List[Dict[str, str]]])    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>documents     (Optional[List[Dict[str, str]]])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> add <p>Add new documents.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>kwargs </li> </ul> reset"},{"location":"api/compose/Vote/","title":"Vote","text":"<p>Voting operator. Computes the score for each document based on it's number of occurences and based on documents ranks: \\(nb_occurences * sum_{rank \\in ranks} 1 / rank\\). The higher the score, the higher the document is ranked in output of the vote.</p>"},{"location":"api/compose/Vote/#parameters","title":"Parameters","text":"<ul> <li> <p>models (list)</p> <p>List of models of the vote.</p> </li> </ul>"},{"location":"api/compose/Vote/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"town\": \"Paris\", \"country\": \"France\", \"continent\": \"Europe\"},\n...    {\"id\": 1, \"town\": \"Montreal\", \"country\": \"Canada\", \"continent\": \"North America\"},\n...    {\"id\": 2, \"town\": \"Madrid\", \"country\": \"Spain\", \"continent\": \"Europe\"},\n... ]\n\n&gt;&gt;&gt; search = (\n...     retrieve.TfIdf(key=\"id\", on=\"town\", documents=documents) *\n...     retrieve.TfIdf(key=\"id\", on=\"country\", documents=documents) *\n...     retrieve.Flash(key=\"id\", on=\"continent\")\n... )\n\n&gt;&gt;&gt; search = search.add(documents)\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"town\", \"country\", \"continent\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...     key=\"id\",\n...     on=[\"town\", \"country\", \"continent\"],\n...     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n... ) * rank.Encoder(\n...     key=\"id\",\n...     on=[\"town\", \"country\", \"continent\"],\n...     encoder=SentenceTransformer(\n...        \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n...     ).encode,\n... )\n\n&gt;&gt;&gt; search = retriever + ranker\n\n&gt;&gt;&gt; search = search.add(documents)\n\n&gt;&gt;&gt; print(search(\"What is the capital of Canada ? Is it paris, montreal or madrid ?\"))\n[{'id': 1, 'similarity': 2.5},\n {'id': 0, 'similarity': 1.4},\n {'id': 2, 'similarity': 1.0}]\n</code></pre>"},{"location":"api/compose/Vote/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>q     (Union[List[List[Dict[str, str]]], List[Dict[str, str]]])    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>documents     (Optional[List[Dict[str, str]]])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> add <p>Add new documents.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>kwargs </li> </ul> reset"},{"location":"api/data/arxiv-tags/","title":"arxiv_tags","text":"<p>Semanlink tags arXiv documents. The objective of this dataset is to evaluate a neural search pipeline for automatic tagging of arXiv documents. This function returns the set of tags and the pairs arXiv documents and tags.</p>"},{"location":"api/data/arxiv-tags/#parameters","title":"Parameters","text":"<ul> <li> <p>arxiv_title (bool) \u2013 defaults to <code>True</code></p> <p>Include title of the arxiv paper inside the query.</p> </li> <li> <p>arxiv_summary (bool) \u2013 defaults to <code>True</code></p> <p>Include summary of the arxiv paper inside the query.</p> </li> <li> <p>comment (bool) \u2013 defaults to <code>False</code></p> <p>Include comment of the arxiv paper inside the query.</p> </li> <li> <p>broader_prefLabel_text (bool) \u2013 defaults to <code>True</code></p> <p>Include broader_prefLabel as a text field.</p> </li> <li> <p>broader_altLabel_text (bool) \u2013 defaults to <code>True</code></p> <p>Include broader_altLabel_text as a text field.</p> </li> <li> <p>prefLabel_text (bool) \u2013 defaults to <code>True</code></p> <p>Include prefLabel_text as a text field.</p> </li> <li> <p>altLabel_text (bool) \u2013 defaults to <code>True</code></p> <p>Include altLabel_text as a text field.</p> </li> </ul>"},{"location":"api/data/arxiv-tags/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import data\n\n&gt;&gt;&gt; documents, query_answers = data.arxiv_tags()\n\n&gt;&gt;&gt; print(list(documents[0].keys()))\n['prefLabel',\n 'type',\n 'broader',\n 'creationTime',\n 'creationDate',\n 'comment',\n 'uri',\n 'broader_prefLabel',\n 'broader_related',\n 'broader_prefLabel_text',\n 'prefLabel_text']\n</code></pre>"},{"location":"api/data/load-towns/","title":"load_towns","text":"<p>Sample of Wikipedia dataset that contains informations about Toulouse, Paris, Lyon and Bordeaux.</p>"},{"location":"api/data/load-towns/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import data\n\n&gt;&gt;&gt; towns = data.load_towns()\n\n&gt;&gt;&gt; print(towns[:3])\n[{'article': 'Paris (French pronunciation: \u200b[pa\u0281i] (listen)) is the '\n             'capital and most populous city of France, with an estimated '\n             'population of 2,175,601 residents as of 2018, in an area of more '\n             'than 105 square kilometres (41 square miles).',\n  'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris'},\n {'article': \"Since the 17th century, Paris has been one of Europe's major \"\n             'centres of finance, diplomacy, commerce, fashion, gastronomy, '\n             'science, and arts.',\n  'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris'},\n {'article': 'The City of Paris is the centre and seat of government of the '\n             'region and province of \u00cele-de-France, or Paris Region, which has '\n             'an estimated population of 12,174,880, or about 18 percent of '\n             'the population of France as of 2017.',\n  'id': 2,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris'}]\n</code></pre>"},{"location":"api/evaluate/evaluation/","title":"evaluation","text":"<p>Evaluation function</p>"},{"location":"api/evaluate/evaluation/#parameters","title":"Parameters","text":"<ul> <li> <p>search</p> <p>Search function.</p> </li> <li> <p>query_answers (List[Tuple[str, List[Dict[str, str]]]])</p> <p>List of tuples (query, answers).</p> </li> <li> <p>hits_k (range) \u2013 defaults to <code>range(1, 6)</code></p> <p>List of k to compute precision, recall and F1.</p> </li> <li> <p>batch_size (Optional[int]) \u2013 defaults to <code>None</code></p> <p>Batch size.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> <p>Number of documents to retrieve.</p> </li> </ul>"},{"location":"api/evaluate/evaluation/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import data, evaluate, retrieve\n&gt;&gt;&gt; from lenlp import sparse\n\n&gt;&gt;&gt; documents, query_answers = data.arxiv_tags(\n...    arxiv_title=True, arxiv_summary=False, comment=False\n... )\n\n&gt;&gt;&gt; search = retrieve.TfIdf(\n...     key=\"uri\",\n...     on=[\"prefLabel_text\", \"altLabel_text\"],\n...     documents=documents,\n...     tfidf=sparse.TfidfVectorizer(normalize=True, ngram_range=(3, 7), analyzer=\"char\"),\n... ) + documents\n\n&gt;&gt;&gt; scores = evaluate.evaluation(search=search, query_answers=query_answers, k=10)\n\n&gt;&gt;&gt; print(scores)\n{'F1@1': '26.52%',\n 'F1@2': '29.41%',\n 'F1@3': '28.65%',\n 'F1@4': '26.85%',\n 'F1@5': '25.19%',\n 'Precision@1': '63.06%',\n 'Precision@2': '43.47%',\n 'Precision@3': '33.12%',\n 'Precision@4': '26.67%',\n 'Precision@5': '22.55%',\n 'R-Precision': '26.95%',\n 'Recall@1': '16.79%',\n 'Recall@2': '22.22%',\n 'Recall@3': '25.25%',\n 'Recall@4': '27.03%',\n 'Recall@5': '28.54%'}\n</code></pre>"},{"location":"api/index/Faiss/","title":"Faiss","text":"<p>Faiss index dedicated to vector search.</p>"},{"location":"api/index/Faiss/#parameters","title":"Parameters","text":"<ul> <li> <p>key</p> <p>Identifier field for each document.</p> </li> <li> <p>index \u2013 defaults to <code>None</code></p> <p>Faiss index to use.</p> </li> <li> <p>normalize (bool) \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/index/Faiss/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import index\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"title\": \"Paris France\"},\n...    {\"id\": 1, \"title\": \"Madrid Spain\"},\n...    {\"id\": 2, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n\n&gt;&gt;&gt; faiss_index = index.Faiss(key=\"id\")\n&gt;&gt;&gt; faiss_index = faiss_index.add(\n...    documents = documents,\n...    embeddings = encoder.encode([document[\"title\"] for document in documents]),\n... )\n\n&gt;&gt;&gt; print(faiss_index(embeddings=encoder.encode([\"Spain\", \"Montreal\"])))\n[[{'id': 1, 'similarity': 0.6544566197822951},\n  {'id': 0, 'similarity': 0.5405466290777285},\n  {'id': 2, 'similarity': 0.48717489472604614}],\n [{'id': 2, 'similarity': 0.7372165680578416},\n  {'id': 0, 'similarity': 0.5185646665953703},\n  {'id': 1, 'similarity': 0.4834444940712032}]]\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 3, \"title\": \"Paris France\"},\n...    {\"id\": 4, \"title\": \"Madrid Spain\"},\n...    {\"id\": 5, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; faiss_index = faiss_index.add(\n...    documents = documents,\n...    embeddings = encoder.encode([document[\"title\"] for document in documents]),\n... )\n\n&gt;&gt;&gt; print(faiss_index(embeddings=encoder.encode([\"Spain\", \"Montreal\"]), k=4))\n[[{'id': 1, 'similarity': 0.6544566197822951},\n  {'id': 4, 'similarity': 0.6544566197822951},\n  {'id': 0, 'similarity': 0.5405466290777285},\n  {'id': 3, 'similarity': 0.5405466290777285}],\n [{'id': 2, 'similarity': 0.7372165680578416},\n  {'id': 5, 'similarity': 0.7372165680578416},\n  {'id': 0, 'similarity': 0.5185646665953703},\n  {'id': 3, 'similarity': 0.5185646665953703}]]\n</code></pre>"},{"location":"api/index/Faiss/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>embeddings     (numpy.ndarray)    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul> add <p>Add documents to the faiss index and export embeddings if the path is provided. Streaming friendly.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>embeddings     (numpy.ndarray)    </li> </ul>"},{"location":"api/index/Faiss/#references","title":"References","text":"<ol> <li>Faiss</li> </ol>"},{"location":"api/qa/QA/","title":"QA","text":"<p>Question Answering model. QA models needs input documents contents to run.</p>"},{"location":"api/qa/QA/#parameters","title":"Parameters","text":"<ul> <li> <p>on (Union[str, list])</p> <p>Fields to use to answer to the question.</p> </li> <li> <p>model</p> <p>Hugging Face question answering model available here.</p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>32</code></p> </li> </ul>"},{"location":"api/qa/QA/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve, qa\n&gt;&gt;&gt; from transformers import pipeline\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"title\": \"Paris France\"},\n...    {\"id\": 1, \"title\": \"Madrid Spain\"},\n...    {\"id\": 2, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\"], documents=documents)\n\n&gt;&gt;&gt; qa_model = qa.QA(\n...     model = pipeline(\"question-answering\", model = \"deepset/roberta-base-squad2\", tokenizer = \"deepset/roberta-base-squad2\"),\n...     on = [\"title\"],\n...  )\n\n&gt;&gt;&gt; pipeline = retriever + documents + qa_model\n\n&gt;&gt;&gt; pipeline\nTfIdf retriever\n    key      : id\n    on       : title\n    documents: 3\nMapping to documents\nQuestion Answering\n    on: title\n\n&gt;&gt;&gt; print(pipeline(q=\"what is the capital of france?\"))\n[{'answer': 'Paris',\n  'end': 5,\n  'id': 0,\n  'question': 'what is the capital of france?',\n  'score': 0.05615315958857536,\n  'similarity': 0.5962847939999439,\n  'start': 0,\n  'title': 'Paris France'},\n {'answer': 'Montreal',\n  'end': 8,\n  'id': 2,\n  'question': 'what is the capital of france?',\n  'score': 0.01080897357314825,\n  'similarity': 0.0635641726163728,\n  'start': 0,\n  'title': 'Montreal Canada'}]\n\n&gt;&gt;&gt; print(pipeline([\"what is the capital of France?\", \"what is the capital of Canada?\"]))\n[[{'answer': 'Paris',\n   'end': 5,\n   'id': 0,\n   'question': 'what is the capital of France?',\n   'score': 0.1554129421710968,\n   'similarity': 0.5962847939999439,\n   'start': 0,\n   'title': 'Paris France'},\n  {'answer': 'Montreal',\n   'end': 8,\n   'id': 2,\n   'question': 'what is the capital of France?',\n   'score': 1.2884755960840266e-05,\n   'similarity': 0.0635641726163728,\n   'start': 0,\n   'title': 'Montreal Canada'}],\n [{'answer': 'Montreal',\n   'end': 8,\n   'id': 2,\n   'question': 'what is the capital of Canada?',\n   'score': 0.05316793918609619,\n   'similarity': 0.5125692857821978,\n   'start': 0,\n   'title': 'Montreal Canada'},\n  {'answer': 'Paris France',\n   'end': 12,\n   'id': 0,\n   'question': 'what is the capital of Canada?',\n   'score': 4.7594025431862974e-07,\n   'similarity': 0.035355339059327376,\n   'start': 0,\n   'title': 'Paris France'}]]\n</code></pre>"},{"location":"api/qa/QA/#methods","title":"Methods","text":"call <p>Question answering main method.</p> <p>Parameters</p> <ul> <li>q     (Union[str, List[str]])    </li> <li>documents     (Union[List[List[Dict[str, str]]], List[Dict[str, str]]])    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> get_question_context"},{"location":"api/query/Norvig/","title":"Norvig","text":"<p>Spelling corrector written by Peter Norvig: How to Write a Spelling Corrector</p>"},{"location":"api/query/Norvig/#parameters","title":"Parameters","text":"<ul> <li> <p>on (Union[str, List])</p> <p>Fields to use for fitting the spelling corrector on.</p> </li> <li> <p>lower (bool) \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/query/Norvig/#attributes","title":"Attributes","text":"<ul> <li>type</li> </ul>"},{"location":"api/query/Norvig/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from cherche import query, data\n\n&gt;&gt;&gt; documents = data.load_towns()\n\n&gt;&gt;&gt; corrector = query.Norvig(on = [\"title\", \"article\"], lower=True)\n\n&gt;&gt;&gt; corrector.add(documents)\nQuery Norvig\n     Vocabulary: 967\n\n&gt;&gt;&gt; corrector(q=\"tha citi af Parisa is in Fronce\")\n'the city of paris is in france'\n\n&gt;&gt;&gt; corrector(q=[\"tha citi af Parisa is in Fronce\", \"parisa\"])\n['the city of paris is in france', 'paris']\n</code></pre>"},{"location":"api/query/Norvig/#methods","title":"Methods","text":"call <p>Correct spelling errors in a given query.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>kwargs </li> </ul> add <p>Fit Nervig spelling corrector.</p> <p>Parameters</p> <ul> <li>documents     (Union[List[Dict], str])    </li> </ul> correct <p>Most probable spelling correction for word.</p> <p>Parameters</p> <ul> <li>word     (str)    </li> </ul> reset <p>Wipe dictionary.</p>"},{"location":"api/query/Norvig/#references","title":"References","text":"<ol> <li>How to Write a Spelling Corrector</li> </ol>"},{"location":"api/query/PRF/","title":"PRF","text":"<p>Pseudo (or blind) Relevance-Feedback module. The Query-Augmentation method applies a fast document retrieving method and then extracts keywords from relevant documents. Thus, we have to retrieve top words from relevant documents to give a proper augmentation of a given query.</p>"},{"location":"api/query/PRF/#parameters","title":"Parameters","text":"<ul> <li> <p>on (Union[str, list])</p> <p>Fields to use for fitting the spelling corrector on.</p> </li> <li> <p>documents (list)</p> </li> <li> <p>tf (sklearn.feature_extraction.text.CountVectorizer) \u2013 defaults to <code>sparse.TfidfVectorizer()</code></p> <p>defaults to sklearn.feature_extraction.text.sparse.TfidfVectorizer. If you want to implement your own tf, it needs to follow the sklearn base API and provides the <code>transform</code> <code>fit_transform</code> and <code>get_feature_names_out</code> methods. See sklearn documentation for more information.</p> </li> <li> <p>nb_docs (int) \u2013 defaults to <code>5</code></p> <p>Number of documents from which to retrieve top-terms.</p> </li> <li> <p>nb_terms_per_doc (int) \u2013 defaults to <code>3</code></p> <p>Number of terms to extract from each top documents retrieved.</p> </li> </ul>"},{"location":"api/query/PRF/#attributes","title":"Attributes","text":"<ul> <li>type</li> </ul>"},{"location":"api/query/PRF/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from cherche import query, data\n\n&gt;&gt;&gt; documents = data.load_towns()\n\n&gt;&gt;&gt; prf = query.PRF(\n...     on=[\"title\", \"article\"],\n...     nb_docs=8, nb_terms_per_doc=1,\n...     documents=documents\n... )\n\n&gt;&gt;&gt; prf\nQuery PRF\n    on       : title, article\n    documents: 8\n    terms    : 1\n\n&gt;&gt;&gt; prf(q=\"Europe\")\n'Europe art metro space science bordeaux paris university significance'\n\n&gt;&gt;&gt; prf(q=[\"Europe\", \"Paris\"])\n['Europe art metro space science bordeaux paris university significance', 'Paris received paris club subway billion source tour tournament']\n</code></pre>"},{"location":"api/query/PRF/#methods","title":"Methods","text":"call <p>Augment a given query with new terms.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>kwargs </li> </ul>"},{"location":"api/query/PRF/#references","title":"References","text":"<ol> <li>Relevance feedback and pseudo relevance feedback</li> <li>Blind Feedback</li> </ol>"},{"location":"api/query/Query/","title":"Query","text":"<p>Abstract class for models working on a query.</p>"},{"location":"api/query/Query/#parameters","title":"Parameters","text":"<ul> <li>on (Union[str, list])</li> </ul>"},{"location":"api/query/Query/#attributes","title":"Attributes","text":"<ul> <li>type</li> </ul>"},{"location":"api/query/Query/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>kwargs </li> </ul>"},{"location":"api/rank/CrossEncoder/","title":"CrossEncoder","text":"<p>Cross-Encoder as a ranker. CrossEncoder takes both the query and the document as input and outputs a score. The score is a similarity score between the query and the document. The CrossEncoder cannot pre-compute the embeddings of the documents since it need both the query and the document.</p>"},{"location":"api/rank/CrossEncoder/#parameters","title":"Parameters","text":"<ul> <li> <p>on (Union[List[str], str])</p> <p>Fields to use to match the query to the documents.</p> </li> <li> <p>encoder</p> <p>Sentence Transformer cross-encoder.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>64</code></p> </li> </ul>"},{"location":"api/rank/CrossEncoder/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve, rank, evaluate, data\n&gt;&gt;&gt; from sentence_transformers import CrossEncoder\n\n&gt;&gt;&gt; documents, query_answers = data.arxiv_tags(\n...    arxiv_title=True, arxiv_summary=False, comment=False\n... )\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(\n...    key=\"uri\",\n...    on=[\"prefLabel_text\", \"altLabel_text\"],\n...    documents=documents,\n...    k=100,\n... )\n\n&gt;&gt;&gt; ranker = rank.CrossEncoder(\n...     on = [\"prefLabel_text\", \"altLabel_text\"],\n...     encoder = CrossEncoder(\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\").predict,\n... )\n\n&gt;&gt;&gt; pipeline = retriever + documents + ranker\n\n&gt;&gt;&gt; match = pipeline(\"graph neural network\", k=5)\n\n&gt;&gt;&gt; for m in match:\n...     print(m.get(\"uri\", \"\"))\n'http://www.semanlink.net/tag/graph_neural_networks'\n'http://www.semanlink.net/tag/artificial_neural_network'\n'http://www.semanlink.net/tag/dans_deep_averaging_neural_networks'\n'http://www.semanlink.net/tag/recurrent_neural_network'\n'http://www.semanlink.net/tag/convolutional_neural_network'\n</code></pre>"},{"location":"api/rank/CrossEncoder/#methods","title":"Methods","text":"call <p>Rank inputs documents based on query.</p> <p>Parameters</p> <ul> <li>q     (str)    </li> <li>documents     (list)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul>"},{"location":"api/rank/CrossEncoder/#references","title":"References","text":"<ol> <li>Sentence Transformers Cross-Encoders</li> <li>Cross-Encoders Hub</li> </ol>"},{"location":"api/rank/DPR/","title":"DPR","text":"<p>Dual Sentence Transformer as a ranker. This ranker is compatible with any SentenceTransformer. DPR is a dual encoder model, it uses two SentenceTransformer, one for encoding documents and one for encoding queries.</p>"},{"location":"api/rank/DPR/#parameters","title":"Parameters","text":"<ul> <li> <p>on (Union[str, List[str]])</p> <p>Fields on wich encoder will perform similarity matching.</p> </li> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>encoder</p> <p>Encoding function dedicated documents.</p> </li> <li> <p>query_encoder</p> <p>Encoding function dedicated to queries.</p> </li> <li> <p>normalize (bool) \u2013 defaults to <code>True</code></p> <p>If set to True, the similarity measure is cosine similarity, if set to False, similarity measure is dot product.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>64</code></p> </li> </ul>"},{"location":"api/rank/DPR/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; ranker = rank.DPR(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base').encode,\n...    query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base').encode,\n...    normalize = True,\n... )\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"title\": \"Paris France\"},\n...    {\"id\": 1, \"title\": \"Madrid Spain\"},\n...    {\"id\": 2, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; ranker.add(documents=documents)\nDPR ranker\n    key       : id\n    on        : title, article\n    normalize : True\n    embeddings: 3\n\n&gt;&gt;&gt; match = ranker(\n...     q=\"Paris\",\n...     documents=documents\n... )\n\n&gt;&gt;&gt; print(match)\n[{'id': 0, 'similarity': 7.806636, 'title': 'Paris France'},\n {'id': 1, 'similarity': 6.239272, 'title': 'Madrid Spain'},\n {'id': 2, 'similarity': 6.168748, 'title': 'Montreal Canada'}]\n\n&gt;&gt;&gt; match = ranker(\n...     q=[\"Paris\", \"Madrid\"],\n...     documents=[documents + [{\"id\": 3, \"title\": \"Paris\"}]] * 2,\n...     k=2,\n... )\n\n&gt;&gt;&gt; print(match)\n[[{'id': 3, 'similarity': 7.906666, 'title': 'Paris'},\n  {'id': 0, 'similarity': 7.806636, 'title': 'Paris France'}],\n [{'id': 1, 'similarity': 8.07025, 'title': 'Madrid Spain'},\n  {'id': 0, 'similarity': 6.1131663, 'title': 'Paris France'}]]\n</code></pre>"},{"location":"api/rank/DPR/#methods","title":"Methods","text":"call <p>Encode input query and ranks documents based on the similarity between the query and the selected field of the documents.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>documents     (Union[List[List[Dict[str, str]]], List[Dict[str, str]]])    </li> <li>k     (int)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> add <p>Pre-compute embeddings and store them at the selected path.</p> <p>Parameters</p> <ul> <li>documents     (List[Dict[str, str]])    </li> <li>batch_size     (int)     \u2013 defaults to <code>64</code> </li> </ul> encode_rank <p>Encode documents and rank them according to the query.</p> <p>Parameters</p> <ul> <li>embeddings_queries     (numpy.ndarray)    </li> <li>documents     (List[List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul> rank <p>Rank inputs documents ordered by relevance among the top k.</p> <p>Parameters</p> <ul> <li>embeddings_documents     (Dict[str, numpy.ndarray])    </li> <li>embeddings_queries     (numpy.ndarray)    </li> <li>documents     (List[List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul>"},{"location":"api/rank/Embedding/","title":"Embedding","text":"<p>Collaborative filtering as a ranker. Recommend is compatible with the library Implicit.</p>"},{"location":"api/rank/Embedding/#parameters","title":"Parameters","text":"<ul> <li> <p>key ('str')</p> <p>Field identifier of each document.</p> </li> <li> <p>normalize ('bool') \u2013 defaults to <code>True</code></p> <p>If set to True, the similarity measure is cosine similarity, if set to False, similarity measure is dot product.</p> </li> <li> <p>k ('typing.Optional[int]') \u2013 defaults to <code>None</code></p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>1024</code></p> </li> </ul>"},{"location":"api/rank/Embedding/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": \"a\", \"title\": \"Paris\"},\n...    {\"id\": \"b\", \"title\": \"Madrid\"},\n...    {\"id\": \"c\", \"title\": \"Montreal\"},\n... ]\n\n&gt;&gt;&gt; encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n&gt;&gt;&gt; embeddings_documents = encoder.encode([\n...    document[\"title\"] for document in documents\n... ])\n\n&gt;&gt;&gt; recommend = rank.Embedding(\n...    key=\"id\",\n... )\n\n&gt;&gt;&gt; recommend.add(\n...    documents=documents,\n...    embeddings_documents=embeddings_documents,\n... )\nEmbedding ranker\n    key      : id\n    documents: 3\n    normalize: True\n\n&gt;&gt;&gt; match = recommend(\n...     q=encoder.encode(\"Paris\"),\n...     documents=documents,\n...     k=2\n... )\n\n&gt;&gt;&gt; print(match)\n[{'id': 'a', 'similarity': 1.0, 'title': 'Paris'},\n {'id': 'c', 'similarity': 0.57165134, 'title': 'Montreal'}]\n\n&gt;&gt;&gt; queries = [\n...    \"Paris\",\n...    \"Madrid\",\n...    \"Montreal\"\n... ]\n\n&gt;&gt;&gt; match = recommend(\n...     q=encoder.encode(queries),\n...     documents=[documents] * 3,\n...     k=2\n... )\n\n&gt;&gt;&gt; print(match)\n[[{'id': 'a', 'similarity': 1.0, 'title': 'Paris'},\n  {'id': 'c', 'similarity': 0.57165134, 'title': 'Montreal'}],\n [{'id': 'b', 'similarity': 1.0, 'title': 'Madrid'},\n  {'id': 'a', 'similarity': 0.49815434, 'title': 'Paris'}],\n [{'id': 'c', 'similarity': 0.9999999, 'title': 'Montreal'},\n  {'id': 'a', 'similarity': 0.5716514, 'title': 'Paris'}]]\n</code></pre>"},{"location":"api/rank/Embedding/#methods","title":"Methods","text":"call <p>Retrieve documents from user id.</p> <p>Parameters</p> <ul> <li>q     ('np.ndarray')    </li> <li>documents     ('typing.Union[typing.List[typing.List[typing.Dict[str, str]]], typing.List[typing.Dict[str, str]]]')    </li> <li>k     ('typing.Optional[int]')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('typing.Optional[int]')     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> add <p>Add embeddings both documents and users.</p> <p>Parameters</p> <ul> <li>documents     ('list')    </li> <li>embeddings_documents     ('typing.List[np.ndarray]')    </li> <li>kwargs </li> </ul> encode_rank <p>Encode documents and rank them according to the query.</p> <p>Parameters</p> <ul> <li>embeddings_queries     (numpy.ndarray)    </li> <li>documents     (List[List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul> rank <p>Rank inputs documents ordered by relevance among the top k.</p> <p>Parameters</p> <ul> <li>embeddings_documents     (Dict[str, numpy.ndarray])    </li> <li>embeddings_queries     (numpy.ndarray)    </li> <li>documents     (List[List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul>"},{"location":"api/rank/Encoder/","title":"Encoder","text":"<p>Sentence Transformer as a ranker. This ranker is compatible with any SentenceTransformer.</p>"},{"location":"api/rank/Encoder/#parameters","title":"Parameters","text":"<ul> <li> <p>on (Union[str, List[str]])</p> <p>Fields on wich encoder will perform similarity matching.</p> </li> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>encoder</p> <p>Encoding function dedicated to both documents and queries.</p> </li> <li> <p>normalize (bool) \u2013 defaults to <code>True</code></p> <p>If set to True, the similarity measure is cosine similarity, if set to False, similarity measure is dot product.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>64</code></p> </li> </ul>"},{"location":"api/rank/Encoder/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"title\": \"Paris France\"},\n...    {\"id\": 1, \"title\": \"Madrid Spain\"},\n...    {\"id\": 2, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    key = \"id\",\n...    on = [\"title\"],\n... )\n\n&gt;&gt;&gt; ranker.add(documents=documents)\nEncoder ranker\n    key       : id\n    on        : title\n    normalize : True\n    embeddings: 3\n\n&gt;&gt;&gt; match = ranker(\n...     q=\"Paris\",\n...     documents=documents\n... )\n\n&gt;&gt;&gt; print(match)\n[{'id': 0, 'similarity': 0.7127624, 'title': 'Paris France'},\n {'id': 1, 'similarity': 0.5497405, 'title': 'Madrid Spain'},\n {'id': 2, 'similarity': 0.50252455, 'title': 'Montreal Canada'}]\n\n&gt;&gt;&gt; match = ranker(\n...     q=[\"Paris France\", \"Madrid Spain\"],\n...     documents=[documents + [{\"id\": 3, \"title\": \"Paris\"}]] * 2,\n...     k=2,\n... )\n\n&gt;&gt;&gt; print(match)\n[[{'id': 0, 'similarity': 0.99999994, 'title': 'Paris France'},\n  {'id': 1, 'similarity': 0.856435, 'title': 'Madrid Spain'}],\n [{'id': 1, 'similarity': 1.0, 'title': 'Madrid Spain'},\n  {'id': 0, 'similarity': 0.856435, 'title': 'Paris France'}]]\n</code></pre>"},{"location":"api/rank/Encoder/#methods","title":"Methods","text":"call <p>Encode input query and ranks documents based on the similarity between the query and the selected field of the documents.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>documents     (Union[List[List[Dict[str, str]]], List[Dict[str, str]]])    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> add <p>Pre-compute embeddings and store them at the selected path.</p> <p>Parameters</p> <ul> <li>documents     (List[Dict[str, str]])    </li> <li>batch_size     (int)     \u2013 defaults to <code>64</code> </li> </ul> encode_rank <p>Encode documents and rank them according to the query.</p> <p>Parameters</p> <ul> <li>embeddings_queries     (numpy.ndarray)    </li> <li>documents     (List[List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul> rank <p>Rank inputs documents ordered by relevance among the top k.</p> <p>Parameters</p> <ul> <li>embeddings_documents     (Dict[str, numpy.ndarray])    </li> <li>embeddings_queries     (numpy.ndarray)    </li> <li>documents     (List[List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul>"},{"location":"api/rank/Ranker/","title":"Ranker","text":"<p>Abstract class for ranking models.</p>"},{"location":"api/rank/Ranker/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (Union[str, List[str]])</p> <p>Fields of the documents to use for ranking.</p> </li> <li> <p>encoder</p> <p>Encoding function to computes embeddings of the documents.</p> </li> <li> <p>normalize (bool)</p> <p>Normalize the embeddings in order to measure cosine similarity if set to True, dot product if set to False.</p> </li> <li> <p>batch_size (int)</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/rank/Ranker/#methods","title":"Methods","text":"call <p>Rank documents according to the query.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>documents     (Union[List[List[Dict[str, str]]], List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> add <p>Pre-compute embeddings and store them at the selected path.</p> <p>Parameters</p> <ul> <li>documents     (List[Dict[str, str]])    </li> <li>batch_size     (int)     \u2013 defaults to <code>64</code> </li> </ul> encode_rank <p>Encode documents and rank them according to the query.</p> <p>Parameters</p> <ul> <li>embeddings_queries     (numpy.ndarray)    </li> <li>documents     (List[List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul> rank <p>Rank inputs documents ordered by relevance among the top k.</p> <p>Parameters</p> <ul> <li>embeddings_documents     (Dict[str, numpy.ndarray])    </li> <li>embeddings_queries     (numpy.ndarray)    </li> <li>documents     (List[List[Dict[str, str]]])    </li> <li>k     (int)    </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> </ul>"},{"location":"api/retrieve/DPR/","title":"DPR","text":"<p>DPR as a retriever using Faiss Index.</p>"},{"location":"api/retrieve/DPR/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (Union[str, list])</p> <p>Field to use to retrieve documents.</p> </li> <li> <p>encoder</p> </li> <li> <p>query_encoder</p> </li> <li> <p>normalize (bool) \u2013 defaults to <code>True</code></p> <p>Whether to normalize the embeddings before adding them to the index in order to measure cosine similarity.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>64</code></p> </li> <li> <p>index \u2013 defaults to <code>None</code></p> <p>Faiss index that will store the embeddings and perform the similarity search.</p> </li> </ul>"},{"location":"api/retrieve/DPR/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"title\": \"Paris France\"},\n...    {\"id\": 1, \"title\": \"Madrid Spain\"},\n...    {\"id\": 2, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.DPR(\n...    key = \"id\",\n...    on = [\"title\"],\n...    encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base').encode,\n...    query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base').encode,\n...    normalize = True,\n... )\n\n&gt;&gt;&gt; retriever.add(documents)\nDPR retriever\n    key      : id\n    on       : title\n    documents: 3\n\n&gt;&gt;&gt; print(retriever(\"Spain\", k=2))\n[{'id': 1, 'similarity': 0.5534179127892946},\n {'id': 0, 'similarity': 0.48604427456660426}]\n\n&gt;&gt;&gt; print(retriever([\"Spain\", \"Montreal\"], k=2))\n[[{'id': 1, 'similarity': 0.5534179492996913},\n  {'id': 0, 'similarity': 0.4860442182428353}],\n [{'id': 2, 'similarity': 0.5451990410703741},\n  {'id': 0, 'similarity': 0.47405722260691213}]]\n</code></pre>"},{"location":"api/retrieve/DPR/#methods","title":"Methods","text":"call <p>Retrieve documents from the index.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> add <p>Add documents to the index.</p> <p>Parameters</p> <ul> <li>documents     (List[Dict[str, str]])    </li> <li>batch_size     (int)     \u2013 defaults to <code>64</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul>"},{"location":"api/retrieve/Embedding/","title":"Embedding","text":"<p>The Embedding retriever is dedicated to perform IR on embeddings calculated by the user rather than Cherche.</p>"},{"location":"api/retrieve/Embedding/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>index \u2013 defaults to <code>None</code></p> <p>Faiss index that will store the embeddings and perform the similarity search.</p> </li> <li> <p>normalize (bool) \u2013 defaults to <code>True</code></p> <p>Whether to normalize the embeddings before adding them to the index in order to measure cosine similarity.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>1024</code></p> </li> </ul>"},{"location":"api/retrieve/Embedding/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; recommend = retrieve.Embedding(\n...    key=\"id\",\n... )\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": \"a\", \"title\": \"Paris\", \"author\": \"Paris\"},\n...    {\"id\": \"b\", \"title\": \"Madrid\", \"author\": \"Madrid\"},\n...    {\"id\": \"c\", \"title\": \"Montreal\", \"author\": \"Montreal\"},\n... ]\n\n&gt;&gt;&gt; encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n&gt;&gt;&gt; embeddings_documents = encoder.encode([\n...    document[\"title\"] for document in documents\n... ])\n\n&gt;&gt;&gt; recommend.add(\n...    documents=documents,\n...    embeddings_documents=embeddings_documents,\n... )\nEmbedding retriever\n    key      : id\n    documents: 3\n\n&gt;&gt;&gt; queries = [\n...    \"Paris\",\n...    \"Madrid\",\n...    \"Montreal\"\n... ]\n\n&gt;&gt;&gt; embeddings_queries = encoder.encode(queries)\n&gt;&gt;&gt; print(recommend(embeddings_queries, k=2))\n[[{'id': 'a', 'similarity': 1.0},\n  {'id': 'c', 'similarity': 0.5385907831761005}],\n [{'id': 'b', 'similarity': 1.0},\n  {'id': 'a', 'similarity': 0.4990788711758875}],\n [{'id': 'c', 'similarity': 1.0},\n  {'id': 'a', 'similarity': 0.5385907831761005}]]\n\n&gt;&gt;&gt; embeddings_queries = encoder.encode(\"Paris\")\n&gt;&gt;&gt; print(recommend(embeddings_queries, k=2))\n[{'id': 'a', 'similarity': 0.9999999999989104},\n {'id': 'c', 'similarity': 0.5385907485958683}]\n</code></pre>"},{"location":"api/retrieve/Embedding/#methods","title":"Methods","text":"call <p>Retrieve documents from the index.</p> <p>Parameters</p> <ul> <li>q     (numpy.ndarray)    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> add <p>Add embeddings both documents and users.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>embeddings_documents     (numpy.ndarray)    </li> <li>kwargs </li> </ul>"},{"location":"api/retrieve/Encoder/","title":"Encoder","text":"<p>Encoder as a retriever using Faiss Index.</p>"},{"location":"api/retrieve/Encoder/#parameters","title":"Parameters","text":"<ul> <li> <p>encoder</p> </li> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (Union[str, list])</p> <p>Field to use to retrieve documents.</p> </li> <li> <p>normalize (bool) \u2013 defaults to <code>True</code></p> <p>Whether to normalize the embeddings before adding them to the index in order to measure cosine similarity.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>64</code></p> </li> <li> <p>index \u2013 defaults to <code>None</code></p> <p>Faiss index that will store the embeddings and perform the similarity search.</p> </li> </ul>"},{"location":"api/retrieve/Encoder/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"title\": \"Paris France\"},\n...    {\"id\": 1, \"title\": \"Madrid Spain\"},\n...    {\"id\": 2, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Encoder(\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    key = \"id\",\n...    on = [\"title\"],\n... )\n\n&gt;&gt;&gt; retriever.add(documents, batch_size=1)\nEncoder retriever\n    key      : id\n    on       : title\n    documents: 3\n\n&gt;&gt;&gt; print(retriever(\"Spain\", k=2))\n[{'id': 1, 'similarity': 0.6544566453117681},\n {'id': 0, 'similarity': 0.5405465419981407}]\n\n&gt;&gt;&gt; print(retriever([\"Spain\", \"Montreal\"], k=2))\n[[{'id': 1, 'similarity': 0.6544566453117681},\n  {'id': 0, 'similarity': 0.54054659424589}],\n [{'id': 2, 'similarity': 0.7372165680578416},\n  {'id': 0, 'similarity': 0.5185645704259234}]]\n</code></pre>"},{"location":"api/retrieve/Encoder/#methods","title":"Methods","text":"call <p>Retrieve documents from the index.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> add <p>Add documents to the index.</p> <p>Parameters</p> <ul> <li>documents     (List[Dict[str, str]])    </li> <li>batch_size     (int)     \u2013 defaults to <code>64</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul>"},{"location":"api/retrieve/Flash/","title":"Flash","text":"<p>FlashText Retriever. Flash aims to find documents that contain keywords such as a list of tags for example.</p>"},{"location":"api/retrieve/Flash/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (Union[str, list])</p> <p>Fields to use to match the query to the documents.</p> </li> <li> <p>keywords (flashtext.keyword.KeywordProcessor) \u2013 defaults to <code>None</code></p> <p>Keywords extractor from FlashText. If set to None, a default one is created.</p> </li> <li> <p>lowercase (bool) \u2013 defaults to <code>True</code></p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/retrieve/Flash/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"title\": \"paris\", \"article\": \"eiffel tower\"},\n...     {\"id\": 1, \"title\": \"paris\", \"article\": \"paris\"},\n...     {\"id\": 2, \"title\": \"montreal\", \"article\": \"montreal is in canada\"},\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Flash(key=\"id\", on=[\"title\", \"article\"])\n\n&gt;&gt;&gt; retriever.add(documents=documents)\nFlash retriever\n    key      : id\n    on       : title, article\n    documents: 4\n\n&gt;&gt;&gt; print(retriever(q=\"paris\", k=2))\n[{'id': 1, 'similarity': 0.6666666666666666},\n {'id': 0, 'similarity': 0.3333333333333333}]\n</code></pre> <p>[{'id': 0, 'similarity': 1}, {'id': 1, 'similarity': 1}]</p> <pre><code>&gt;&gt;&gt; print(retriever(q=[\"paris\", \"montreal\"]))\n[[{'id': 1, 'similarity': 0.6666666666666666},\n  {'id': 0, 'similarity': 0.3333333333333333}],\n [{'id': 2, 'similarity': 1.0}]]\n</code></pre>"},{"location":"api/retrieve/Flash/#methods","title":"Methods","text":"call <p>Retrieve documents from the index.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> add <p>Add keywords to the retriever.</p> <p>Parameters</p> <ul> <li>documents     (List[Dict[str, str]])    </li> <li>kwargs </li> </ul>"},{"location":"api/retrieve/Flash/#references","title":"References","text":"<ol> <li>FlashText</li> <li>Replace or Retrieve Keywords In Documents at Scale</li> </ol>"},{"location":"api/retrieve/Fuzz/","title":"Fuzz","text":"<p>RapidFuzz wrapper. Rapid fuzzy string matching in Python and C++ using the Levenshtein Distance.</p>"},{"location":"api/retrieve/Fuzz/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (Union[str, list])</p> <p>Fields to use to match the query to the documents.</p> </li> <li> <p>fuzzer \u2013 defaults to <code>&lt;cyfunction partial_ratio at 0x12fcc13c0&gt;</code></p> <p>RapidFuzz scorer: fuzz.ratio, fuzz.partial_ratio, fuzz.token_set_ratio, fuzz.partial_token_set_ratio, fuzz.token_sort_ratio, fuzz.partial_token_sort_ratio, fuzz.token_ratio, fuzz.partial_token_ratio, fuzz.WRatio, fuzz.QRatio, string_metric.levenshtein, string_metric.normalized_levenshtein</p> </li> <li> <p>default_process (bool) \u2013 defaults to <code>True</code></p> <p>Pre-processing step. If set to True, documents processed by RapidFuzz default process.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/retrieve/Fuzz/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from rapidfuzz import fuzz\n\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"title\": \"Paris\", \"article\": \"Eiffel tower\"},\n...     {\"id\": 1, \"title\": \"Paris\", \"article\": \"Paris is in France.\"},\n...     {\"id\": 2, \"title\": \"Montreal\", \"article\": \"Montreal is in Canada.\"},\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Fuzz(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    fuzzer = fuzz.partial_ratio,\n... )\n\n&gt;&gt;&gt; retriever.add(documents=documents)\nFuzz retriever\n    key      : id\n    on       : title, article\n    documents: 3\n\n&gt;&gt;&gt; print(retriever(q=\"paris\", k=2))\n[{'id': 0, 'similarity': 100.0}, {'id': 1, 'similarity': 100.0}]\n\n&gt;&gt;&gt; print(retriever(q=[\"paris\", \"montreal\"], k=2))\n[[{'id': 0, 'similarity': 100.0}, {'id': 1, 'similarity': 100.0}],\n [{'id': 2, 'similarity': 100.0}, {'id': 1, 'similarity': 37.5}]]\n\n&gt;&gt;&gt; print(retriever(q=[\"unknown\", \"montreal\"], k=2))\n[[{'id': 2, 'similarity': 40.0}, {'id': 0, 'similarity': 36.36363636363637}],\n [{'id': 2, 'similarity': 100.0}, {'id': 1, 'similarity': 37.5}]]\n</code></pre>"},{"location":"api/retrieve/Fuzz/#methods","title":"Methods","text":"call <p>Retrieve documents from the index.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> add <p>Fuzz is streaming friendly.</p> <p>Parameters</p> <ul> <li>documents     (List[Dict[str, str]])    </li> <li>kwargs </li> </ul>"},{"location":"api/retrieve/Fuzz/#references","title":"References","text":"<ol> <li>RapidFuzz</li> </ol>"},{"location":"api/retrieve/Lunr/","title":"Lunr","text":"<p>Lunr is a Python implementation of Lunr.js by Oliver Nightingale. Lunr is a retriever dedicated for small and middle size corpus.</p>"},{"location":"api/retrieve/Lunr/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (Union[str, list])</p> <p>Fields to use to match the query to the documents.</p> </li> <li> <p>documents (list)</p> <p>Documents in Lunr retriever are static. The retriever must be reseted to index new documents.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/retrieve/Lunr/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"title\": \"Paris\", \"article\": \"Eiffel tower\"},\n...     {\"id\": 1, \"title\": \"Paris\", \"article\": \"Paris is in France.\"},\n...     {\"id\": 2, \"title\": \"Montreal\", \"article\": \"Montreal is in Canada.\"},\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Lunr(\n...     key=\"id\",\n...     on=[\"title\", \"article\"],\n...     documents=documents,\n... )\n\n&gt;&gt;&gt; retriever\nLunr retriever\n    key      : id\n    on       : title, article\n    documents: 3\n\n&gt;&gt;&gt; print(retriever(q=\"paris\", k=2))\n[{'id': 1, 'similarity': 0.268}, {'id': 0, 'similarity': 0.134}]\n\n&gt;&gt;&gt; print(retriever(q=[\"paris\", \"montreal\"], k=2))\n[[{'id': 1, 'similarity': 0.268}, {'id': 0, 'similarity': 0.134}],\n [{'id': 2, 'similarity': 0.94}]]\n</code></pre>"},{"location":"api/retrieve/Lunr/#methods","title":"Methods","text":"call <p>Retrieve documents from the index.</p> <p>Parameters</p> <ul> <li>q     (Union[str, List[str]])    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul>"},{"location":"api/retrieve/Lunr/#references","title":"References","text":"<ol> <li>Lunr.py</li> <li>Lunr.js</li> <li>Solr</li> </ol>"},{"location":"api/retrieve/Retriever/","title":"Retriever","text":"<p>Retriever base class.</p>"},{"location":"api/retrieve/Retriever/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (Union[str, list])</p> <p>Fields to use to match the query to the documents.</p> </li> <li> <p>k (Optional[int])</p> </li> <li> <p>batch_size (int)</p> </li> </ul>"},{"location":"api/retrieve/Retriever/#methods","title":"Methods","text":"call <p>Retrieve documents from the index.</p> <p>Parameters</p> <ul> <li>q     (Union[List[str], str])    </li> <li>k     (Optional[int])    </li> <li>batch_size     (Optional[int])    </li> <li>kwargs </li> </ul>"},{"location":"api/retrieve/TfIdf/","title":"TfIdf","text":"<p>TfIdf retriever based on cosine similarities.</p>"},{"location":"api/retrieve/TfIdf/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> <p>Field identifier of each document.</p> </li> <li> <p>on (Union[str, list])</p> <p>Fields to use to match the query to the documents.</p> </li> <li> <p>documents (List[Dict[str, str]]) \u2013 defaults to <code>None</code></p> <p>Documents in TFIdf retriever are static. The retriever must be reseted to index new documents.</p> </li> <li> <p>tfidf (sklearn.feature_extraction.text.sparse.TfidfVectorizer) \u2013 defaults to <code>None</code></p> <p>sparse.TfidfVectorizer class of Sklearn to create a custom TfIdf retriever.</p> </li> <li> <p>k (Optional[int]) \u2013 defaults to <code>None</code></p> <p>Number of documents to retrieve. Default is <code>None</code>, i.e all documents that match the query will be retrieved.</p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>1024</code></p> </li> <li> <p>fit (bool) \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/retrieve/TfIdf/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from lenlp import sparse\n\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"title\": \"Paris\", \"article\": \"Eiffel tower\"},\n...     {\"id\": 1, \"title\": \"Montreal\", \"article\": \"Montreal is in Canada.\"},\n...     {\"id\": 2, \"title\": \"Paris\", \"article\": \"Eiffel tower\"},\n...     {\"id\": 3, \"title\": \"Montreal\", \"article\": \"Montreal is in Canada.\"},\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(\n...     key=\"id\",\n...     on=[\"title\", \"article\"],\n...     documents=documents,\n... )\n\n&gt;&gt;&gt; documents = [\n...     {\"id\": 4, \"title\": \"Paris\", \"article\": \"Eiffel tower\"},\n...     {\"id\": 5, \"title\": \"Montreal\", \"article\": \"Montreal is in Canada.\"},\n...     {\"id\": 6, \"title\": \"Paris\", \"article\": \"Eiffel tower\"},\n...     {\"id\": 7, \"title\": \"Montreal\", \"article\": \"Montreal is in Canada.\"},\n... ]\n\n&gt;&gt;&gt; retriever = retriever.add(documents)\n\n&gt;&gt;&gt; print(retriever(q=[\"paris\", \"canada\"], k=4))\n[[{'id': 6, 'similarity': 0.5404109029445249},\n  {'id': 0, 'similarity': 0.5404109029445249},\n  {'id': 2, 'similarity': 0.5404109029445249},\n  {'id': 4, 'similarity': 0.5404109029445249}],\n [{'id': 7, 'similarity': 0.3157669764669935},\n  {'id': 5, 'similarity': 0.3157669764669935},\n  {'id': 3, 'similarity': 0.3157669764669935},\n  {'id': 1, 'similarity': 0.3157669764669935}]]\n\n&gt;&gt;&gt; print(retriever([\"unknown\", \"montreal paris\"], k=2))\n[[],\n [{'id': 7, 'similarity': 0.7391866872635209},\n  {'id': 5, 'similarity': 0.7391866872635209}]]\n\n&gt;&gt;&gt; print(retriever(q=\"paris\"))\n[{'id': 6, 'similarity': 0.5404109029445249},\n {'id': 0, 'similarity': 0.5404109029445249},\n {'id': 2, 'similarity': 0.5404109029445249},\n {'id': 4, 'similarity': 0.5404109029445249}]\n</code></pre>"},{"location":"api/retrieve/TfIdf/#methods","title":"Methods","text":"call <p>Retrieve documents from batch of queries.</p> <p>Parameters</p> <ul> <li>q     (Union[str, List[str]])    </li> <li>k     (Optional[int])     \u2013 defaults to <code>None</code>      Number of documents to retrieve. Default is <code>None</code>, i.e all documents that match the query will be retrieved.</li> <li>batch_size     (Optional[int])     \u2013 defaults to <code>None</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> add <p>Add new documents to the TFIDF retriever. The tfidf won't be refitted.</p> <p>Parameters</p> <ul> <li>documents     (list)       Documents in TFIdf retriever are static. The retriever must be reseted to index new documents.</li> <li>batch_size     (int)     \u2013 defaults to <code>100000</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>False</code> </li> <li>kwargs </li> </ul> top_k <p>Return the top k documents for each query.</p> <p>Parameters</p> <ul> <li>similarities     (scipy.sparse._csc.csc_matrix)    </li> <li>k     (int)       Number of documents to retrieve. Default is <code>None</code>, i.e all documents that match the query will be retrieved.</li> </ul>"},{"location":"api/retrieve/TfIdf/#references","title":"References","text":"<ol> <li>sklearn.feature_extraction.text.sparse.TfidfVectorizer</li> <li>Python: tf-idf-cosine: to find document similarity</li> </ol>"},{"location":"api/utils/TopK/","title":"TopK","text":"<p>Filter top k documents in pipeline.</p>"},{"location":"api/utils/TopK/#parameters","title":"Parameters","text":"<ul> <li> <p>k (int)</p> <p>Number of documents to keep.</p> </li> </ul>"},{"location":"api/utils/TopK/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import retrieve, rank, utils\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"title\": \"Paris France\"},\n...    {\"id\": 1, \"title\": \"Madrid Spain\"},\n...    {\"id\": 2, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    key = \"id\",\n...    on = [\"title\"],\n... )\n\n&gt;&gt;&gt; pipeline = retriever + ranker + utils.TopK(k=2)\n&gt;&gt;&gt; pipeline.add(documents=documents)\nTfIdf retriever\n    key      : id\n    on       : title, article\n    documents: 3\nEncoder ranker\n    key       : id\n    on        : title\n    normalize : True\n    embeddings: 3\nFilter TopK\n    k: 2\n\n&gt;&gt;&gt; print(pipeline(q=\"Paris Madrid Montreal\", k=2))\n[{'id': 0, 'similarity': 0.62922895}, {'id': 2, 'similarity': 0.61419094}]\n</code></pre>"},{"location":"api/utils/TopK/#methods","title":"Methods","text":"call <p>Filter top k documents in pipeline.</p> <p>Parameters</p> <ul> <li>documents     (List[List[Dict[str, str]]])    </li> <li>kwargs </li> </ul>"},{"location":"api/utils/quantize/","title":"quantize","text":"<p>Quantize model to speedup inference. May reduce accuracy.</p>"},{"location":"api/utils/quantize/#parameters","title":"Parameters","text":"<ul> <li> <p>model</p> <p>Transformer model to quantize.</p> </li> <li> <p>dtype \u2013 defaults to <code>None</code></p> <p>Dtype to apply to selected layers.</p> </li> <li> <p>layers \u2013 defaults to <code>None</code></p> <p>Layers to quantize.</p> </li> <li> <p>engine \u2013 defaults to <code>qnnpack</code></p> <p>The qengine specifies which backend is to be used for execution.</p> </li> </ul>"},{"location":"api/utils/quantize/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pprint import pprint as print\n&gt;&gt;&gt; from cherche import utils, retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\"id\": 0, \"title\": \"Paris France\"},\n...    {\"id\": 1, \"title\": \"Madrid Spain\"},\n...    {\"id\": 2, \"title\": \"Montreal Canada\"}\n... ]\n\n&gt;&gt;&gt; encoder = utils.quantize(SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\"))\n\n&gt;&gt;&gt; retriever = retrieve.Encoder(\n...    encoder = encoder.encode,\n...    key = \"id\",\n...    on = [\"title\"],\n... )\n\n&gt;&gt;&gt; retriever = retriever.add(documents)\n\n&gt;&gt;&gt; print(retriever(\"paris\"))\n[{'id': 0, 'similarity': 0.6361529519968355},\n {'id': 2, 'similarity': 0.42750324298964354},\n {'id': 1, 'similarity': 0.42645383885361576}]\n</code></pre>"},{"location":"api/utils/quantize/#references","title":"References","text":"<ol> <li>PyTorch Quantization</li> </ol>"},{"location":"api/utils/yield-batch-single/","title":"yield_batch_single","text":"<p>Yield successive n-sized chunks from array.</p>"},{"location":"api/utils/yield-batch-single/#parameters","title":"Parameters","text":"<ul> <li> <p>array (Union[List[str], str, List[Dict[str, Any]]])</p> </li> <li> <p>desc (str)</p> </li> <li> <p>tqdm_bar (bool) \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/utils/yield-batch/","title":"yield_batch","text":"<p>Yield successive n-sized chunks from array.</p>"},{"location":"api/utils/yield-batch/#parameters","title":"Parameters","text":"<ul> <li> <p>array (Union[List[str], str, List[Dict[str, Any]], numpy.ndarray])</p> </li> <li> <p>batch_size (int)</p> </li> <li> <p>desc (str)</p> </li> <li> <p>tqdm_bar (bool) \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"documents/documents/","title":"Documents","text":"<p>When using Cherche, we must define a document as a Python dictionary. A set of documents is simply a list of dictionaries. The name of the fields of the documents does not matter. We can choose the field(s) of your choice to perform neural search. However, it is mandatory to have a unique identifier for each document. Also, the name of this identifier does not matter. In the example below, the identifier is the <code>id</code> field.</p> <p>It can happen that not all documents have the same fields. Therefore, we do not need to standardize or fill all the fields (except the identifier).</p> <pre><code>[\n    {\n        \"id\": 0,\n        \"title\": \"Paris\",\n        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n        \"article\": \"Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).\",\n    },\n    {\n        \"id\": 1,\n        \"title\": \"Paris\",\n        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n        \"article\": \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\",\n    },\n    {\n        \"id\": 2,\n        \"title\": \"Paris\",\n        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.\",\n    },\n]\n</code></pre>"},{"location":"documents/towns/","title":"Towns","text":"<p>Cherche provides a dummy dataset made of sentences from Wikipedia that describes towns such as Toulouse, Paris, Bordeaux and Lyon. This dataset is intended to easily test Cherche. It contains ~200 documents.</p> <pre><code>&gt;&gt;&gt; from cherche import data\n&gt;&gt;&gt; documents = data.load_towns()\n&gt;&gt;&gt; documents[:3]\n</code></pre> <pre><code>[\n    {\n        \"id\": 0,\n        \"title\": \"Paris\",\n        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n        \"article\": \"Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).\",\n    },\n    {\n        \"id\": 1,\n        \"title\": \"Paris\",\n        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n        \"article\": \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\",\n    },\n    {\n        \"id\": 2,\n        \"title\": \"Paris\",\n        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.\",\n    },\n]\n</code></pre>"},{"location":"examples/encoder_retriever/","title":"Encoder as a retriever","text":"<p>In certain cases, particularly with small corpora, a user's query may not match any documents. This is where neural search proves to be incredibly useful, as the encoder can act as a backup to locate relevant documents in situations where traditional retrievers have failed to do so.</p> In\u00a0[1]: Copied! <pre>from cherche import retrieve, rank, data\nfrom sentence_transformers import SentenceTransformer\n</pre> from cherche import retrieve, rank, data from sentence_transformers import SentenceTransformer <p>Let's load a dummy dataset</p> In\u00a0[2]: Copied! <pre>documents = data.load_towns()\ndocuments[:2]\n</pre> documents = data.load_towns() documents[:2] Out[2]: <pre>[{'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).'},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\"}]</pre> <p>First, we will perform a search with a TfIdf to show that the model's ability to retrieve documents may be limited.</p> In\u00a0[3]: Copied! <pre>retriever = retrieve.TfIdf(key=\"id\", on=[\"article\", \"title\"], documents=documents)\nretriever\n</pre> retriever = retrieve.TfIdf(key=\"id\", on=[\"article\", \"title\"], documents=documents) retriever Out[3]: <pre>TfIdf retriever\n\tkey      : id\n\ton       : article, title\n\tdocuments: 105</pre> <p>There is a single document that match the query \"food\" using default TfIdf.</p> In\u00a0[4]: Copied! <pre>retriever(\"food\", k=10)\n</pre> retriever(\"food\", k=10) Out[4]: <pre>[{'id': 96, 'similarity': 0.057060669878117906},\n {'id': 20, 'similarity': 0.02514090300945658}]</pre> <p>We can now compare these results with the <code>retrieve.Encoder</code> using Sentence Bert. The <code>add</code> method takes time because the retriever will compute embeddings for every document.</p> In\u00a0[5]: Copied! <pre>retriever = retrieve.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n)\n\nretriever.add(documents=documents)\n</pre> retriever = retrieve.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode, )  retriever.add(documents=documents) <pre>Encoder index creation: 100%|\u2588| 2/2 [00:02&lt;00:00,  1.30s/it\n</pre> Out[5]: <pre>Encoder retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 105</pre> <p>As can be seen, the encoder recalls more documents, even if they do not systematically contain the word \"food\". These documents seem relevant.</p> In\u00a0[6]: Copied! <pre>retriever(\"food\", k=5)\n</pre> retriever(\"food\", k=5) Out[6]: <pre>[{'id': 48, 'similarity': 0.3757082873324092},\n {'id': 66, 'similarity': 0.3735201261683402},\n {'id': 96, 'similarity': 0.37012889770913526},\n {'id': 16, 'similarity': 0.3682042586662517},\n {'id': 49, 'similarity': 0.3594711511884871}]</pre> In\u00a0[7]: Copied! <pre>pipeline = retriever + documents\npipeline(\"food\", k=5)\n</pre> pipeline = retriever + documents pipeline(\"food\", k=5) Out[7]: <pre>[{'id': 48,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"The city is recognised for its cuisine and gastronomy, as well as historical and architectural landmarks; as such, the districts of Old Lyon, the Fourvi\u00e8re hill, the Presqu'\u00eele and the slopes of the Croix-Rousse are inscribed on the UNESCO World Heritage List.\",\n  'similarity': 0.3757082873324092},\n {'id': 66,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is also one of the centers of gastronomy and business tourism for the organization of international congresses.',\n  'similarity': 0.3735201261683402},\n {'id': 96,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': 'It remains an important centre of commerce, aerospace, transport, finance, pharmaceuticals, technology, design, education, art, culture, tourism, food, fashion, video game development, film, and world affairs.',\n  'similarity': 0.37012889770913526},\n {'id': 16,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris received 12.',\n  'similarity': 0.3682042586662517},\n {'id': 49,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon was historically an important area for the production and weaving of silk.',\n  'similarity': 0.3594711511884871}]</pre> <p>We can create a fancy neural search pipeline to benefit from TfIdf precision and Sentence Transformers recall using union operator <code>|</code>.</p> In\u00a0[8]: Copied! <pre>encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode\n</pre> encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode In\u00a0[9]: Copied! <pre># Precision pipeline\nprecision = retrieve.TfIdf(\n    key=\"id\", on=[\"article\", \"title\"], documents=documents\n) + rank.Encoder(key=\"id\", on=[\"title\", \"article\"], encoder=encoder)\n\n# Recall pipeline\nrecall = retrieve.Encoder(key=\"id\", on=[\"title\", \"article\"], encoder=encoder)\n\nsearch = precision | recall\n\nsearch.add(documents=documents)\n</pre> # Precision pipeline precision = retrieve.TfIdf(     key=\"id\", on=[\"article\", \"title\"], documents=documents ) + rank.Encoder(key=\"id\", on=[\"title\", \"article\"], encoder=encoder)  # Recall pipeline recall = retrieve.Encoder(key=\"id\", on=[\"title\", \"article\"], encoder=encoder)  search = precision | recall  search.add(documents=documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.37s/it]\nEncoder index creation: 100%|\u2588| 2/2 [00:02&lt;00:00,  1.31s/it\n</pre> Out[9]: <pre>Union Pipeline\n-----\nTfIdf retriever\n\tkey      : id\n\ton       : article, title\n\tdocuments: 105\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105\nEncoder retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 105\n-----</pre> <p>Our pipeline will first propose documents from the <code>precision</code> pipeline and then documents proposed by the <code>recall</code> pipeline.</p> In\u00a0[10]: Copied! <pre>search(\"food\", k=100)[:3]\n</pre> search(\"food\", k=100)[:3] <pre>TfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 740.78it/s]\nRanker scoring: 1it [00:00, 10407.70it/s]\nRanker sorting: 1it [00:00, 15196.75it/s]\nEncoder retriever: 100%|\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 19.30it/s]\n</pre> Out[10]: <pre>[{'id': 96, 'similarity': 2.4},\n {'id': 20, 'similarity': 1.0206185567010309},\n {'id': 48, 'similarity': 0.3333333333333333}]</pre> In\u00a0[11]: Copied! <pre># Map documents to the pipeline.\nsearch = search + documents\nsearch(\"food\", k=100)[:3]\n</pre> # Map documents to the pipeline. search = search + documents search(\"food\", k=100)[:3] <pre>TfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 898.33it/s]\nRanker scoring: 1it [00:00, 16644.06it/s]\nRanker sorting: 1it [00:00, 20460.02it/s]\nEncoder retriever: 100%|\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 18.98it/s]\n</pre> Out[11]: <pre>[{'id': 96,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': 'It remains an important centre of commerce, aerospace, transport, finance, pharmaceuticals, technology, design, education, art, culture, tourism, food, fashion, video game development, film, and world affairs.',\n  'similarity': 2.4},\n {'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 1.0206185567010309},\n {'id': 48,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"The city is recognised for its cuisine and gastronomy, as well as historical and architectural landmarks; as such, the districts of Old Lyon, the Fourvi\u00e8re hill, the Presqu'\u00eele and the slopes of the Croix-Rousse are inscribed on the UNESCO World Heritage List.\",\n  'similarity': 0.3333333333333333}]</pre>"},{"location":"examples/encoder_retriever/#encoder-as-a-retriever","title":"Encoder as a retriever\u00b6","text":""},{"location":"examples/eval_pipeline/","title":"Semanlink automatic tagging and evaluation","text":"<p>This notebook presents how to evaluate a neural search pipeline using pairs of queries and answers. We will automatically tag arXiv papers that Fran\u00e7ois-Paul Servant manually automated as part of the Semanlink Knowledge Graph.</p> In\u00a0[1]: Copied! <pre>from pprint import pprint as print\nfrom cherche import data, rank, retrieve, evaluate\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n</pre> from pprint import pprint as print from cherche import data, rank, retrieve, evaluate from sentence_transformers import SentenceTransformer, CrossEncoder from sklearn.feature_extraction.text import TfidfVectorizer In\u00a0[2]: Copied! <pre>documents, query_answers = data.arxiv_tags(\n    arxiv_title=True, arxiv_summary=False, comment=False\n)\n</pre> documents, query_answers = data.arxiv_tags(     arxiv_title=True, arxiv_summary=False, comment=False ) <p>The <code>documents</code> contain a list of tags. Each tag is represented as a dictionary and contains a set of attributes. We will try to automate the tagging of arXiv documents with a neural search pipeline that will retrieve tags based on their attributes using the title, abstract, and comments of the arXiv articles as a query. For each query, there is a list of relevant document identifiers.</p> In\u00a0[3]: Copied! <pre>print(query_answers[:2])\n</pre> print(query_answers[:2]) <pre>[(' Joint Embedding of Words and Labels for Text Classification',\n  [{'uri': 'http://www.semanlink.net/tag/deep_learning_attention'},\n   {'uri': 'http://www.semanlink.net/tag/arxiv_doc'},\n   {'uri': 'http://www.semanlink.net/tag/nlp_text_classification'},\n   {'uri': 'http://www.semanlink.net/tag/label_embedding'}]),\n (' A Survey on Recent Approaches for Natural Language Processing in '\n  'Low-Resource Scenarios',\n  [{'uri': 'http://www.semanlink.net/tag/bosch'},\n   {'uri': 'http://www.semanlink.net/tag/survey'},\n   {'uri': 'http://www.semanlink.net/tag/arxiv_doc'},\n   {'uri': 'http://www.semanlink.net/tag/nlp_low_resource_scenarios'},\n   {'uri': 'http://www.semanlink.net/tag/low_resource_languages'}])]\n</pre> <p>Here is the list of attributes each tag has:</p> In\u00a0[4]: Copied! <pre>documents[0]\n</pre> documents[0] Out[4]: <pre>{'prefLabel': ['Attention mechanism'],\n 'type': ['http://www.semanlink.net/2001/00/semanlink-schema#Tag'],\n 'broader': ['http://www.semanlink.net/tag/deep_learning'],\n 'creationTime': '2016-01-07T00:58:24Z',\n 'creationDate': '2016-01-07',\n 'comment': 'Good explanation is this [blog post by D. Britz](/doc/?uri=http%3A%2F%2Fwww.wildml.com%2F2016%2F01%2Fattention-and-memory-in-deep-learning-and-nlp%2F). (But the best explanation related to attention is to be found in this [post](/doc/2019/08/transformers_from_scratch_%7C_pet) about Self-Attention.) \\r\\n\\r\\nWhile simple Seq2Seq builds a single context vector out of the encoder\u2019s last hidden state, attention creates\\r\\nshortcuts between the context vector and the entire source input: the context vector has access to the entire input sequence.\\r\\nThe decoder can \u201cattend\u201d to different parts of the source sentence at each step of the output generation, and the model learns what to attend to based on the input sentence and what it has produced so far.\\r\\n\\r\\nPossible to interpret what the model is doing by looking at the Attention weight matrix\\r\\n\\r\\nCost: We need to calculate an attention value for each combination of input and output word (D. Britz: -&gt; \"attention is a bit of a misnomer: we look at everything in details before deciding what to focus on\")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n 'uri': 'http://www.semanlink.net/tag/deep_learning_attention',\n 'broader_prefLabel': ['Deep Learning'],\n 'broader_related': ['http://www.semanlink.net/tag/feature_learning',\n  'http://www.semanlink.net/tag/feature_extraction'],\n 'broader_prefLabel_text': 'Deep Learning',\n 'prefLabel_text': 'Attention mechanism'}</pre> <p>Let's evaluate a first piepline made of a single retriever</p> In\u00a0[5]: Copied! <pre>retriever = retrieve.TfIdf(\n    key=\"uri\",\n    on=[\"prefLabel_text\", \"altLabel_text\"],\n    documents=documents,\n    tfidf=TfidfVectorizer(\n        lowercase=True, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"\n    ),\n    k=30,\n)\n\nevaluate.evaluation(search=retriever, query_answers=query_answers, hits_k=range(6))\n</pre> retriever = retrieve.TfIdf(     key=\"uri\",     on=[\"prefLabel_text\", \"altLabel_text\"],     documents=documents,     tfidf=TfidfVectorizer(         lowercase=True, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"     ),     k=30, )  evaluate.evaluation(search=retriever, query_answers=query_answers, hits_k=range(6)) <pre>TfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 30.78it/s]\n</pre> Out[5]: <pre>{'Precision@1': '63.06%',\n 'Precision@2': '43.47%',\n 'Precision@3': '33.12%',\n 'Precision@4': '26.67%',\n 'Precision@5': '22.55%',\n 'Recall@1': '16.79%',\n 'Recall@2': '22.22%',\n 'Recall@3': '25.25%',\n 'Recall@4': '27.03%',\n 'Recall@5': '28.54%',\n 'F1@1': '26.52%',\n 'F1@2': '29.41%',\n 'F1@3': '28.65%',\n 'F1@4': '26.85%',\n 'F1@5': '25.19%',\n 'R-Precision': '26.95%'}</pre> <p>The results of Lunr are inferior to TfIdf on this dataset.</p> In\u00a0[6]: Copied! <pre>retriever = retrieve.Lunr(\n    key=\"uri\", on=[\"prefLabel_text\", \"altLabel_text\"], documents=documents, k=30\n)\n\nevaluate.evaluation(search=retriever, query_answers=query_answers, hits_k=range(6))\n</pre> retriever = retrieve.Lunr(     key=\"uri\", on=[\"prefLabel_text\", \"altLabel_text\"], documents=documents, k=30 )  evaluate.evaluation(search=retriever, query_answers=query_answers, hits_k=range(6)) <pre>Lunr retriever: 100%|\u2588\u2588| 314/314 [00:00&lt;00:00, 2258.93it/s]\n</pre> Out[6]: <pre>{'Precision@1': '60.38%',\n 'Precision@2': '45.35%',\n 'Precision@3': '36.92%',\n 'Precision@4': '31.01%',\n 'Precision@5': '26.00%',\n 'Recall@1': '16.22%',\n 'Recall@2': '23.62%',\n 'Recall@3': '28.22%',\n 'Recall@4': '31.23%',\n 'Recall@5': '32.30%',\n 'F1@1': '25.57%',\n 'F1@2': '31.06%',\n 'F1@3': '31.99%',\n 'F1@4': '31.12%',\n 'F1@5': '28.81%',\n 'R-Precision': '30.95%'}</pre> <p>You can find an explanation of the metrics here. The TfIdf retriever using caracters ngrams did well.</p> <p>Here is what tagging looks like using our retriever</p> In\u00a0[7]: Copied! <pre>retriever(\n    q=\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n)\n</pre> retriever(     q=\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\", ) Out[7]: <pre>[{'uri': 'http://www.semanlink.net/tag/information_retrieval',\n  'similarity': 4.147},\n {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval',\n  'similarity': 3.489},\n {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval',\n  'similarity': 3.489},\n {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir', 'similarity': 3.489},\n {'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm',\n  'similarity': 3.489},\n {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp',\n  'similarity': 3.489},\n {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking',\n  'similarity': 1.579},\n {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval',\n  'similarity': 1.479}]</pre> <p>Let's try to improve those results using a ranker.</p> In\u00a0[8]: Copied! <pre>retriever = retrieve.TfIdf(\n    key=\"uri\",\n    on=[\"prefLabel_text\", \"altLabel_text\"],\n    documents=documents,\n    tfidf=TfidfVectorizer(\n        lowercase=True, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"\n    ),\n    k=100,\n)\n\nranker = rank.Encoder(\n    key=\"uri\",\n    on=[\"prefLabel_text\", \"altLabel_text\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n    k=30,\n).add(documents)\n</pre> retriever = retrieve.TfIdf(     key=\"uri\",     on=[\"prefLabel_text\", \"altLabel_text\"],     documents=documents,     tfidf=TfidfVectorizer(         lowercase=True, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"     ),     k=100, )  ranker = rank.Encoder(     key=\"uri\",     on=[\"prefLabel_text\", \"altLabel_text\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,     k=30, ).add(documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02&lt;00:00,  2.35it/s]\n</pre> In\u00a0[9]: Copied! <pre>search = retriever + ranker\n</pre> search = retriever + ranker In\u00a0[10]: Copied! <pre>evaluate.evaluation(search=search, query_answers=query_answers, hits_k=range(6))\n</pre> evaluate.evaluation(search=search, query_answers=query_answers, hits_k=range(6)) <pre>TfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 26.88it/s]\n</pre> Out[10]: <pre>{'Precision@1': '62.42%',\n 'Precision@2': '41.88%',\n 'Precision@3': '32.27%',\n 'Precision@4': '26.19%',\n 'Precision@5': '22.42%',\n 'Recall@1': '16.87%',\n 'Recall@2': '22.20%',\n 'Recall@3': '25.41%',\n 'Recall@4': '26.88%',\n 'Recall@5': '28.53%',\n 'F1@1': '26.56%',\n 'F1@2': '29.02%',\n 'F1@3': '28.44%',\n 'F1@4': '26.53%',\n 'F1@5': '25.11%',\n 'R-Precision': '27.20%'}</pre> <p>The Bert Sentence classifier improved the results of the extractor a little. We managed to increase the F1@k score, precision and recall.</p> <p>Here are proposed tags for Bert using retriever ranker:</p> In\u00a0[11]: Copied! <pre>search(\n    \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\", k=5\n)\n</pre> search(     \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\", k=5 ) Out[11]: <pre>[{'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm',\n  'similarity': 0.54491174},\n {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval',\n  'similarity': 0.42808783},\n {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval',\n  'similarity': 0.42641872},\n {'uri': 'http://www.semanlink.net/tag/information_retrieval',\n  'similarity': 0.40513238},\n {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp',\n  'similarity': 0.32937095}]</pre> <p>Let's try to use using Flash as a retriever. Flash Text will retrieve tags labels inside the title.</p> In\u00a0[12]: Copied! <pre>retriever = retrieve.Flash(\n    key=\"uri\",\n    on=[\"prefLabel\", \"altLabel\"],\n)\n\nsearch = retriever + ranker\nsearch.add(documents)\n</pre> retriever = retrieve.Flash(     key=\"uri\",     on=[\"prefLabel\", \"altLabel\"], )  search = retriever + ranker search.add(documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:03&lt;00:00,  2.17it/s]\n</pre> Out[12]: <pre>Flash retriever\n\tkey      : uri\n\ton       : prefLabel, altLabel\n\tdocuments: 604\nEncoder ranker\n\tkey       : uri\n\ton        : prefLabel_text, altLabel_text\n\tnormalize : True\n\tembeddings: 433</pre> <p>FlashText as a retriever provides fewer candidates than TfIdf but has higher precision.</p> In\u00a0[13]: Copied! <pre>evaluate.evaluation(search=search, query_answers=query_answers, hits_k=range(6))\n</pre> evaluate.evaluation(search=search, query_answers=query_answers, hits_k=range(6)) <pre>Flash retriever: 100%|\u2588| 314/314 [00:00&lt;00:00, 110173.29it/\n</pre> Out[13]: <pre>{'Precision@1': '72.80%',\n 'Precision@2': '61.90%',\n 'Precision@3': '59.90%',\n 'Precision@4': '59.27%',\n 'Precision@5': '59.37%',\n 'Recall@1': '16.33%',\n 'Recall@2': '19.54%',\n 'Recall@3': '20.11%',\n 'Recall@4': '20.16%',\n 'Recall@5': '20.20%',\n 'F1@1': '26.67%',\n 'F1@2': '29.71%',\n 'F1@3': '30.11%',\n 'F1@4': '30.08%',\n 'F1@5': '30.15%',\n 'R-Precision': '20.20%'}</pre> In\u00a0[14]: Copied! <pre>search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")\n</pre> search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\") Out[14]: <pre>[]</pre> <p>We can get the best of both worlds by using pipeline union. It gets a bit complicated, but the union allows us to retrieve the best candidates from the first model then add the candidates from the second model without duplicates (no matter how many models are in the union). Our first retriever and ranker (Flash + Encoder) have low recall and high precision. The second retriever has a lower precision but higher recall. So we can mix things up and offer Flash and Ranker candidates first, then TfIdf and Ranker candidates seconds.</p> In\u00a0[15]: Copied! <pre>ranker = rank.Encoder(\n    key=\"uri\",\n    on=[\"prefLabel_text\", \"altLabel_text\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n    k=30,\n).add(documents)\n\nprecision = (\n    retrieve.Flash(\n        key=\"uri\",\n        on=[\"prefLabel\", \"altLabel\"],\n    ).add(documents)\n    + ranker\n)\n\nrecall = (\n    retrieve.TfIdf(\n        key=\"uri\",\n        on=[\"prefLabel_text\", \"altLabel_text\"],\n        documents=documents,\n        tfidf=TfidfVectorizer(lowercase=True, ngram_range=(3, 7), analyzer=\"char\"),\n        k=30,\n    )\n    + ranker\n)\n\nsearch = precision | recall\n</pre> ranker = rank.Encoder(     key=\"uri\",     on=[\"prefLabel_text\", \"altLabel_text\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,     k=30, ).add(documents)  precision = (     retrieve.Flash(         key=\"uri\",         on=[\"prefLabel\", \"altLabel\"],     ).add(documents)     + ranker )  recall = (     retrieve.TfIdf(         key=\"uri\",         on=[\"prefLabel_text\", \"altLabel_text\"],         documents=documents,         tfidf=TfidfVectorizer(lowercase=True, ngram_range=(3, 7), analyzer=\"char\"),         k=30,     )     + ranker )  search = precision | recall <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:03&lt;00:00,  2.25it/s]\n</pre> In\u00a0[16]: Copied! <pre>evaluate.evaluation(search=search, query_answers=query_answers, hits_k=range(6))\n</pre> evaluate.evaluation(search=search, query_answers=query_answers, hits_k=range(6)) <pre>Flash retriever: 100%|\u2588| 314/314 [00:00&lt;00:00, 108022.59it/\nTfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 33.39it/s]\n</pre> Out[16]: <pre>{'Precision@1': '69.11%',\n 'Precision@2': '49.84%',\n 'Precision@3': '39.07%',\n 'Precision@4': '31.13%',\n 'Precision@5': '25.92%',\n 'Recall@1': '18.74%',\n 'Recall@2': '25.89%',\n 'Recall@3': '30.10%',\n 'Recall@4': '31.58%',\n 'Recall@5': '32.57%',\n 'F1@1': '29.49%',\n 'F1@2': '34.08%',\n 'F1@3': '34.00%',\n 'F1@4': '31.35%',\n 'F1@5': '28.87%',\n 'R-Precision': '31.99%'}</pre> <p>We did improves F1 and recall scores using union of pipelines.</p> <p>We could also calculate a voting score between the precision and recall pipelines.</p> In\u00a0[17]: Copied! <pre>ranker = rank.Encoder(\n    key=\"uri\",\n    on=[\"prefLabel_text\", \"altLabel_text\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n    k=30,\n).add(documents)\n\nprecision = (\n    retrieve.Flash(\n        key=\"uri\",\n        on=[\"prefLabel\", \"altLabel\"],\n    ).add(documents)\n    + ranker\n)\n\nrecall = (\n    retrieve.TfIdf(\n        key=\"uri\",\n        on=[\"prefLabel_text\", \"altLabel_text\"],\n        documents=documents,\n        tfidf=TfidfVectorizer(lowercase=True, ngram_range=(3, 7), analyzer=\"char\"),\n        k=30,\n    )\n    + ranker\n)\n\n\n# Vote between precision and recall followed by precision and recall\nsearch = precision * recall\n</pre> ranker = rank.Encoder(     key=\"uri\",     on=[\"prefLabel_text\", \"altLabel_text\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,     k=30, ).add(documents)  precision = (     retrieve.Flash(         key=\"uri\",         on=[\"prefLabel\", \"altLabel\"],     ).add(documents)     + ranker )  recall = (     retrieve.TfIdf(         key=\"uri\",         on=[\"prefLabel_text\", \"altLabel_text\"],         documents=documents,         tfidf=TfidfVectorizer(lowercase=True, ngram_range=(3, 7), analyzer=\"char\"),         k=30,     )     + ranker )   # Vote between precision and recall followed by precision and recall search = precision * recall <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:03&lt;00:00,  2.15it/s]\n</pre> In\u00a0[18]: Copied! <pre>evaluate.evaluation(search=search, query_answers=query_answers, hits_k=range(6))\n</pre> evaluate.evaluation(search=search, query_answers=query_answers, hits_k=range(6)) <pre>Flash retriever: 100%|\u2588| 314/314 [00:00&lt;00:00, 104774.18it/\nTfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 27.62it/s]\n</pre> Out[18]: <pre>{'Precision@1': '69.43%',\n 'Precision@2': '49.84%',\n 'Precision@3': '39.07%',\n 'Precision@4': '31.13%',\n 'Precision@5': '25.92%',\n 'Recall@1': '18.81%',\n 'Recall@2': '25.89%',\n 'Recall@3': '30.10%',\n 'Recall@4': '31.58%',\n 'Recall@5': '32.57%',\n 'F1@1': '29.60%',\n 'F1@2': '34.08%',\n 'F1@3': '34.00%',\n 'F1@4': '31.35%',\n 'F1@5': '28.87%',\n 'R-Precision': '31.99%'}</pre> <p>Here are our tags for BERT's article with best of both worlds</p> In\u00a0[19]: Copied! <pre>search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")\n</pre> search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\") <pre>Flash retriever: 100%|\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 1801.68it/s]\nTfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 685.90it/s]\n</pre> Out[19]: <pre>[{'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm',\n  'similarity': 1.0},\n {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval',\n  'similarity': 0.5},\n {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir',\n  'similarity': 0.3333333333333333},\n {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval',\n  'similarity': 0.25},\n {'uri': 'http://www.semanlink.net/tag/information_retrieval',\n  'similarity': 0.2},\n {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking',\n  'similarity': 0.16666666666666666},\n {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval',\n  'similarity': 0.14285714285714285},\n {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp',\n  'similarity': 0.125},\n {'uri': 'http://www.semanlink.net/tag/active_learning',\n  'similarity': 0.1111111111111111},\n {'uri': 'http://www.semanlink.net/tag/cognitive_search', 'similarity': 0.1},\n {'uri': 'http://www.semanlink.net/tag/contrastive_learning',\n  'similarity': 0.09090909090909091},\n {'uri': 'http://www.semanlink.net/tag/intent_classification_and_slot_filling',\n  'similarity': 0.08333333333333333},\n {'uri': 'http://www.semanlink.net/tag/relational_inductive_biases',\n  'similarity': 0.07692307692307693},\n {'uri': 'http://www.semanlink.net/tag/knowledge_augmented_language_models',\n  'similarity': 0.07142857142857142},\n {'uri': 'http://www.semanlink.net/tag/thought_vector',\n  'similarity': 0.06666666666666667},\n {'uri': 'http://www.semanlink.net/tag/aspect_detection',\n  'similarity': 0.0625},\n {'uri': 'http://www.semanlink.net/tag/generative_adversarial_network',\n  'similarity': 0.058823529411764705},\n {'uri': 'http://www.semanlink.net/tag/bert',\n  'similarity': 0.05555555555555555},\n {'uri': 'http://www.semanlink.net/tag/information_extraction',\n  'similarity': 0.05263157894736842},\n {'uri': 'http://www.semanlink.net/tag/connectionist_vs_symbolic_debate',\n  'similarity': 0.05},\n {'uri': 'http://www.semanlink.net/tag/artificial_human_intelligence',\n  'similarity': 0.047619047619047616},\n {'uri': 'http://www.semanlink.net/tag/good_related_work_section',\n  'similarity': 0.045454545454545456},\n {'uri': 'http://www.semanlink.net/tag/artificial_general_intelligence',\n  'similarity': 0.043478260869565216},\n {'uri': 'http://www.semanlink.net/tag/conscience_artificielle',\n  'similarity': 0.041666666666666664},\n {'uri': 'http://www.semanlink.net/tag/neuroscience_and_ai',\n  'similarity': 0.04},\n {'uri': 'http://www.semanlink.net/tag/introduction',\n  'similarity': 0.038461538461538464},\n {'uri': 'http://www.semanlink.net/tag/constraint_satisfaction_problem',\n  'similarity': 0.037037037037037035},\n {'uri': 'http://www.semanlink.net/tag/out_of_distribution_detection',\n  'similarity': 0.03571428571428571},\n {'uri': 'http://www.semanlink.net/tag/rotate',\n  'similarity': 0.034482758620689655},\n {'uri': 'http://www.semanlink.net/tag/patent_landscaping',\n  'similarity': 0.03333333333333333}]</pre>"},{"location":"examples/eval_pipeline/#semanlink-automatic-tagging-and-evaluation","title":"Semanlink automatic tagging and evaluation\u00b6","text":""},{"location":"examples/retriever_ranker/","title":"Retriever and ranker","text":"<p>This notebook present a simple neural search pipeline composed of two retrievers and a ranker.</p> In\u00a0[2]: Copied! <pre>from cherche import data, rank, retrieve, utils\nfrom sentence_transformers import SentenceTransformer\n</pre> from cherche import data, rank, retrieve, utils from sentence_transformers import SentenceTransformer <p>The first step is to define the corpus on which we will perform the neural search. The towns dataset contains about a hundred documents. Each document has fours attributes, the <code>id</code>, the <code>title</code> of the article, the <code>url</code> and the content of the <code>article</code>.</p> In\u00a0[3]: Copied! <pre>documents = data.load_towns()\ndocuments[:4]\n</pre> documents = data.load_towns() documents[:4] Out[3]: <pre>[{'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).'},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\"},\n {'id': 2,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.'},\n {'id': 3,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Paris Region had a GDP of \u20ac709 billion ($808 billion) in 2017.'}]</pre> <p>We start by initiating a retriever whose mission will be to quickly filter the documents. This retriever will find documents based on the title and content of the article using the <code>on</code> parameter.</p> In\u00a0[4]: Copied! <pre>retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n</pre> retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents) <p>We then add a ranker to the pipeline to filter the results according to the semantic similarity between the query and the retrieved documents. similarity between the query and the retriever's output documents. The ranker will be based on the content of the article.</p> In\u00a0[5]: Copied! <pre>ranker = rank.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n)\n</pre> ranker = rank.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode, ) <p>We initialise the pipeline and ask the retrievers to index the documents and the ranker to pre-compute the document embeddings. This step can take some time if you have a lot of documents. It can be interesting to use a GPU to pre-calculate all the embeddings if you have many documents. The embeddings will be stored in the <code>encoder.pkl</code> file.</p> In\u00a0[6]: Copied! <pre>search = retriever + ranker\nsearch.add(documents)\n</pre> search = retriever + ranker search.add(documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.33s/it]\n</pre> Out[6]: <pre>TfIdf retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 105\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105</pre> <p>Let's call our model to retrieve documents related to football in Paris. The search pipeline provides a similarity score for each document. The documents are sorted in order of relevance, from most similar to least similar.</p> In\u00a0[7]: Copied! <pre>search(\"paris football\", k=30)\n</pre> search(\"paris football\", k=30) Out[7]: <pre>[{'id': 20, 'similarity': 0.7220986},\n {'id': 16, 'similarity': 0.48418275},\n {'id': 21, 'similarity': 0.47666836},\n {'id': 56, 'similarity': 0.47011483},\n {'id': 22, 'similarity': 0.45666158},\n {'id': 1, 'similarity': 0.44948608},\n {'id': 0, 'similarity': 0.44595104},\n {'id': 2, 'similarity': 0.4206621},\n {'id': 25, 'similarity': 0.4146704},\n {'id': 6, 'similarity': 0.41367412},\n {'id': 3, 'similarity': 0.4131328},\n {'id': 23, 'similarity': 0.41079015},\n {'id': 14, 'similarity': 0.37518078},\n {'id': 51, 'similarity': 0.37361926},\n {'id': 7, 'similarity': 0.37052304},\n {'id': 8, 'similarity': 0.36798736},\n {'id': 17, 'similarity': 0.35948235},\n {'id': 9, 'similarity': 0.34356856},\n {'id': 13, 'similarity': 0.33688956},\n {'id': 12, 'similarity': 0.31458178},\n {'id': 15, 'similarity': 0.3111611},\n {'id': 53, 'similarity': 0.30873594},\n {'id': 5, 'similarity': 0.30330563},\n {'id': 52, 'similarity': 0.30239156},\n {'id': 10, 'similarity': 0.2945645},\n {'id': 19, 'similarity': 0.2915255},\n {'id': 94, 'similarity': 0.28307498},\n {'id': 11, 'similarity': 0.27992725},\n {'id': 4, 'similarity': 0.276568},\n {'id': 18, 'similarity': 0.20204495}]</pre> <p>The retriever we use is a bit too basic, the word aerospace appears in the corpus but aero does not. We are therefore unable to retrieve relevant documents for the query aero.</p> In\u00a0[8]: Copied! <pre>search(\"aero\", k=30)  # Aerospace\n</pre> search(\"aero\", k=30)  # Aerospace Out[8]: <pre>[{'id': 67, 'similarity': 0.32282117},\n {'id': 29, 'similarity': 0.30668122},\n {'id': 31, 'similarity': 0.2690589},\n {'id': 96, 'similarity': 0.027692636}]</pre> <p>We can improve the retrieval by processing sub-units of words using the <code>ngram_range</code> parameter of the <code>TfidfVectorizer</code> model. This update to the retriever will reduce its precision but increase the recall.</p> In\u00a0[9]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\n\nretriever = retrieve.TfIdf(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    documents=documents,\n    tfidf=TfidfVectorizer(ngram_range=(4, 10), analyzer=\"char_wb\", max_df=0.3),\n)\n\nsearch = retriever + ranker\nsearch.add(documents)\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer  retriever = retrieve.TfIdf(     key=\"id\",     on=[\"title\", \"article\"],     documents=documents,     tfidf=TfidfVectorizer(ngram_range=(4, 10), analyzer=\"char_wb\", max_df=0.3), )  search = retriever + ranker search.add(documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.32s/it]\n</pre> Out[9]: <pre>TfIdf retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 105\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105</pre> In\u00a0[10]: Copied! <pre>search(\"paris football\", k=30)\n</pre> search(\"paris football\", k=30) Out[10]: <pre>[{'id': 20, 'similarity': 0.7220986},\n {'id': 24, 'similarity': 0.5216039},\n {'id': 16, 'similarity': 0.48418275},\n {'id': 21, 'similarity': 0.47666836},\n {'id': 56, 'similarity': 0.47011483},\n {'id': 22, 'similarity': 0.45666158},\n {'id': 1, 'similarity': 0.44948608},\n {'id': 0, 'similarity': 0.44595104},\n {'id': 2, 'similarity': 0.4206621},\n {'id': 25, 'similarity': 0.4146704},\n {'id': 6, 'similarity': 0.41367412},\n {'id': 3, 'similarity': 0.4131328},\n {'id': 23, 'similarity': 0.41079015},\n {'id': 14, 'similarity': 0.37518078},\n {'id': 7, 'similarity': 0.37052304},\n {'id': 8, 'similarity': 0.36798736},\n {'id': 17, 'similarity': 0.35948235},\n {'id': 9, 'similarity': 0.34356856},\n {'id': 13, 'similarity': 0.33688956},\n {'id': 12, 'similarity': 0.31458178},\n {'id': 15, 'similarity': 0.3111611},\n {'id': 5, 'similarity': 0.30330563},\n {'id': 10, 'similarity': 0.2945645},\n {'id': 19, 'similarity': 0.2915255},\n {'id': 11, 'similarity': 0.27992725},\n {'id': 4, 'similarity': 0.276568},\n {'id': 43, 'similarity': 0.2750644},\n {'id': 96, 'similarity': 0.21408883},\n {'id': 18, 'similarity': 0.20204495},\n {'id': 79, 'similarity': 0.09676781}]</pre> <p>By treating the characters we have built a retriever with a better recall.</p> In\u00a0[11]: Copied! <pre>search(\"aero\", k=30)  # Aerospace\n</pre> search(\"aero\", k=30)  # Aerospace Out[11]: <pre>[{'id': 67, 'similarity': 0.32282117},\n {'id': 29, 'similarity': 0.30668122},\n {'id': 31, 'similarity': 0.2690589},\n {'id': 96, 'similarity': 0.027692636}]</pre> <p>Let's map indexes to our documents.</p> In\u00a0[12]: Copied! <pre>search += documents\n</pre> search += documents In\u00a0[13]: Copied! <pre>search(\"paris football\", k=10)\n</pre> search(\"paris football\", k=10) Out[13]: <pre>[{'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 0.7220986},\n {'id': 16,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris received 12.',\n  'similarity': 0.48418275},\n {'id': 21,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The 80,000-seat Stade de France, built for the 1998 FIFA World Cup, is located just north of Paris in the neighbouring commune of Saint-Denis.',\n  'similarity': 0.47666836},\n {'id': 22,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris hosts the annual French Open Grand Slam tennis tournament on the red clay of Roland Garros.',\n  'similarity': 0.45666158},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\",\n  'similarity': 0.44948608},\n {'id': 2,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.',\n  'similarity': 0.4206621},\n {'id': 3,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Paris Region had a GDP of \u20ac709 billion ($808 billion) in 2017.',\n  'similarity': 0.4131328},\n {'id': 7,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Opened in 1900, the city's subway system, the Paris M\u00e9tro, serves 5.\",\n  'similarity': 0.37052304},\n {'id': 5,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Another source ranked Paris as most expensive, on par with Singapore and Hong Kong, in 2018.',\n  'similarity': 0.30330563},\n {'id': 18,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The number of foreign visitors declined by 80.',\n  'similarity': 0.20204495}]</pre> In\u00a0[14]: Copied! <pre>search(\"aero\", k=30)  # Aerospace\n</pre> search(\"aero\", k=30)  # Aerospace Out[14]: <pre>[{'id': 67,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'It is a central and strategic hub for the aeronautics, military and space sector, home to international companies such as Dassault Aviation, Ariane Group, Safran and Thal\u00e8s.',\n  'similarity': 0.32282117},\n {'id': 29,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Toulouse is the centre of the European aerospace industry, with the headquarters of Airbus (formerly EADS), the SPOT satellite system, ATR and the Aerospace Valley.',\n  'similarity': 0.30668122},\n {'id': 31,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Thales Alenia Space, ATR, SAFRAN, Liebherr-Aerospace and Airbus Defence and Space also have a significant presence in Toulouse.',\n  'similarity': 0.2690589},\n {'id': 96,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': 'It remains an important centre of commerce, aerospace, transport, finance, pharmaceuticals, technology, design, education, art, culture, tourism, food, fashion, video game development, film, and world affairs.',\n  'similarity': 0.027692636}]</pre>"},{"location":"examples/retriever_ranker/#retriever-and-ranker","title":"Retriever and ranker\u00b6","text":""},{"location":"examples/retriever_ranker_qa/","title":"Retriever, ranker, question answering","text":"<p>The purpose of this notebook is to create a straightforward pipeline for the extractive question answering task. The pipeline includes a retriever, a ranker, and a question-answering model, with the retriever serving as the initial filter in this architecture. Following that, the ranker filters the documents again based on the semantic similarity between the question and the documents. Finally, we'll utilize a question-answering model to extract the answer from the filtered documents.</p> <p>Due to the slow nature of extractive question answering, even with document filtering, utilizing a GPU for this type of pipeline that employs QA models would be advantageous.</p> In\u00a0[1]: Copied! <pre>from cherche import data, rank, retrieve, qa\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import pipeline\n</pre> from cherche import data, rank, retrieve, qa from sentence_transformers import SentenceTransformer from transformers import pipeline <p>We can use the <code>towns</code> corpus for this example: we can ask questions about the cities of Bordeaux, Toulouse, Paris and Lyon.</p> In\u00a0[2]: Copied! <pre>documents = data.load_towns()\ndocuments[:4]\n</pre> documents = data.load_towns() documents[:4] Out[2]: <pre>[{'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).'},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\"},\n {'id': 2,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.'},\n {'id': 3,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Paris Region had a GDP of \u20ac709 billion ($808 billion) in 2017.'}]</pre> <p>We start by creating a retriever whose mission will be to quickly filter the documents. This retriever will find documents based on the title and content of the article using <code>on</code> parameter.</p> In\u00a0[3]: Copied! <pre>retriever = retrieve.TfIdf(\n    key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100\n)\n</pre> retriever = retrieve.TfIdf(     key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100 ) <p>We then add a ranker to the pipeline to filter the results according to the semantic similarity between the query and the retrieved documents. similarity between the query and the retriever's output documents. The ranker will be based on the content of the article.</p> In\u00a0[4]: Copied! <pre>ranker = rank.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n    k=30,\n)\n</pre> ranker = rank.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,     k=30, ) In\u00a0[5]: Copied! <pre>question_answering = qa.QA(\n    model=pipeline(\n        \"question-answering\",\n        model=\"deepset/roberta-base-squad2\",\n        tokenizer=\"deepset/roberta-base-squad2\",\n    ),\n    on=\"article\",\n)\n</pre> question_answering = qa.QA(     model=pipeline(         \"question-answering\",         model=\"deepset/roberta-base-squad2\",         tokenizer=\"deepset/roberta-base-squad2\",     ),     on=\"article\", ) <p>We initialise the pipeline and ask the retrievers to index the documents and the ranker to pre-compute the document embeddings. This step can take some time if you have a lot of documents. A GPU could speed up the process. Also the question answering model needs the documents fields and not only ids. To map ids to documents, we add the <code>documents</code> to our pipeline.</p> In\u00a0[6]: Copied! <pre>search = retriever + ranker + documents + question_answering\nsearch.add(documents)\n</pre> search = retriever + ranker + documents + question_answering search.add(documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.34s/it]\n</pre> Out[6]: <pre>TfIdf retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 105\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105\nMapping to documents\nQuestion Answering\n\ton: article</pre> <p>Paris Saint Germain is the name of the biggest football team of Paris. The Question Answering Pipeline provides the ranking-related similarity score called <code>similarity</code> and the question answering task-related score <code>qa_score</code>. The higher the <code>qa_score</code> the more likely the answer is. The answers are sorted from the most likely to the least likely.</p> In\u00a0[7]: Copied! <pre>search(\"What is the name of the football club of Paris?\")\n</pre> search(\"What is the name of the football club of Paris?\") <pre>Question answering: 100%|\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.70s/it]\n</pre> Out[7]: <pre>[{'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 0.7104821,\n  'score': 0.9848365783691406,\n  'start': 18,\n  'end': 37,\n  'answer': 'Paris Saint-Germain',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 21,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The 80,000-seat Stade de France, built for the 1998 FIFA World Cup, is located just north of Paris in the neighbouring commune of Saint-Denis.',\n  'similarity': 0.46161494,\n  'score': 0.8121969103813171,\n  'start': 16,\n  'end': 31,\n  'answer': 'Stade de France',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 40,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The city\\'s unique architecture made of pinkish terracotta bricks has earned Toulouse the nickname La Ville Rose (\"The Pink City\").',\n  'similarity': 0.37491357,\n  'score': 0.16557253897190094,\n  'start': 76,\n  'end': 84,\n  'answer': 'Toulouse',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 41,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon or Lyons (UK: , US: , French: [lj\u0254\u0303] (listen); Arpitan: Liyon, pronounced [\u028ej\u0254\u0303]) is the third-largest city and second-largest urban area of France.',\n  'similarity': 0.43718058,\n  'score': 0.15580952167510986,\n  'start': 0,\n  'end': 4,\n  'answer': 'Lyon',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 51,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'It is also known for its light festival, the F\u00eate des Lumi\u00e8res, which begins every 8 December and lasts for four days, earning Lyon the title of \"Capital of Lights\".',\n  'similarity': 0.35435817,\n  'score': 0.08197779208421707,\n  'start': 127,\n  'end': 131,\n  'answer': 'Lyon',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 63,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The term \"Bordelais\" may also refer to the city and its surrounding region.',\n  'similarity': 0.38600546,\n  'score': 0.025884564965963364,\n  'start': 10,\n  'end': 19,\n  'answer': 'Bordelais',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 16,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris received 12.',\n  'similarity': 0.46774143,\n  'score': 0.015906406566500664,\n  'start': 15,\n  'end': 17,\n  'answer': '12',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 104,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': 'It is also home to ice hockey team Montreal Canadiens, the franchise with the most Stanley Cup wins.',\n  'similarity': 0.34670672,\n  'score': 0.0028171883895993233,\n  'start': 35,\n  'end': 53,\n  'answer': 'Montreal Canadiens',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 7,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Opened in 1900, the city's subway system, the Paris M\u00e9tro, serves 5.\",\n  'similarity': 0.40217242,\n  'score': 0.002542218891903758,\n  'start': 46,\n  'end': 51,\n  'answer': 'Paris',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 57,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux ( bor-DOH, French: [b\u0254\u0281do] (listen); Gascon Occitan: Bord\u00e8u [bu\u027e\u02c8\u00f0\u025bw]) is a port city on the river Garonne in the Gironde department, Southwestern France.',\n  'similarity': 0.3889441,\n  'score': 0.000859599094837904,\n  'start': 0,\n  'end': 8,\n  'answer': 'Bordeaux',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 6,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris is a major railway, highway, and air-transport hub served by two international airports: Paris\u2013Charles de Gaulle (the second-busiest airport in Europe) and Paris\u2013Orly.',\n  'similarity': 0.3775912,\n  'score': 0.0005627072532661259,\n  'start': 162,\n  'end': 172,\n  'answer': 'Paris\u2013Orly',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\",\n  'similarity': 0.39131272,\n  'score': 0.0002991863002534956,\n  'start': 24,\n  'end': 29,\n  'answer': 'Paris',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 55,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'According to the Globalization and World Rankings Research Institute, Lyon is considered a Beta city, as of 2018.',\n  'similarity': 0.34211817,\n  'score': 0.00024206833040807396,\n  'start': 70,\n  'end': 74,\n  'answer': 'Lyon',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 3,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Paris Region had a GDP of \u20ac709 billion ($808 billion) in 2017.',\n  'similarity': 0.38708982,\n  'score': 0.00012191522546345368,\n  'start': 4,\n  'end': 9,\n  'answer': 'Paris',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 13,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Mus\u00e9e Rodin and Mus\u00e9e Picasso exhibit the works of two noted Parisians.',\n  'similarity': 0.3348205,\n  'score': 0.00010052858851850033,\n  'start': 65,\n  'end': 75,\n  'answer': 'Parisians.',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 2,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.',\n  'similarity': 0.41702062,\n  'score': 9.009212226374075e-05,\n  'start': 4,\n  'end': 8,\n  'answer': 'City',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 39,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Sernin, the largest remaining Romanesque building in Europe, designated in 1998 along with the former hospital H\u00f4tel-Dieu Saint-Jacques because of their significance to the Santiago de Compostela pilgrimage route.',\n  'similarity': 0.33389583,\n  'score': 8.280130714410916e-05,\n  'start': 0,\n  'end': 6,\n  'answer': 'Sernin',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 24,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The 1938 and 1998 FIFA World Cups, the 2007 Rugby World Cup, as well as the 1960, 1984 and 2016 UEFA European Championships were also held in the city.',\n  'similarity': 0.42733788,\n  'score': 7.101883238647133e-05,\n  'start': 96,\n  'end': 151,\n  'answer': 'UEFA European Championships were also held in the city.',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 35,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': \"According to the rankings of L'Express and Challenges, Toulouse is the most dynamic French city.\",\n  'similarity': 0.3657912,\n  'score': 5.10705795022659e-05,\n  'start': 55,\n  'end': 63,\n  'answer': 'Toulouse',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 23,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The city hosted the Olympic Games in 1900, 1924 and will host the 2024 Summer Olympics.',\n  'similarity': 0.36762083,\n  'score': 3.203179585398175e-05,\n  'start': 66,\n  'end': 87,\n  'answer': '2024 Summer Olympics.',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).',\n  'similarity': 0.43164164,\n  'score': 2.7218264222028665e-05,\n  'start': 29,\n  'end': 35,\n  'answer': '\\u200b[pa\u0281i',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 14,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The historical district along the Seine in the city centre has been classified as a UNESCO World Heritage Site since 1991; popular landmarks there include the Cathedral of Notre Dame de Paris on the \u00cele de la Cit\u00e9, now closed for renovation after the 15 April 2019 fire.',\n  'similarity': 0.36198646,\n  'score': 2.5467967134318314e-05,\n  'start': 172,\n  'end': 191,\n  'answer': 'Notre Dame de Paris',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 33,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The air route between Toulouse\u2013Blagnac and the Parisian airports is the busiest in France, transporting 3.',\n  'similarity': 0.36112642,\n  'score': 1.3854595636075828e-05,\n  'start': 22,\n  'end': 38,\n  'answer': 'Toulouse\u2013Blagnac',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 67,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'It is a central and strategic hub for the aeronautics, military and space sector, home to international companies such as Dassault Aviation, Ariane Group, Safran and Thal\u00e8s.',\n  'similarity': 0.34512353,\n  'score': 1.3446222510538064e-05,\n  'start': 122,\n  'end': 172,\n  'answer': 'Dassault Aviation, Ariane Group, Safran and Thal\u00e8s',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 37,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'It is now the capital of the Occitanie region, the second largest region in Metropolitan France.',\n  'similarity': 0.33124608,\n  'score': 1.067540688381996e-05,\n  'start': 89,\n  'end': 95,\n  'answer': 'France',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 32,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The University of Toulouse is one of the oldest in Europe (founded in 1229) and, with more than 103,000 students, it is the fourth-largest university campus in France, after the universities of Paris, Lyon and Lille.',\n  'similarity': 0.36503303,\n  'score': 2.1875375750823878e-06,\n  'start': 201,\n  'end': 205,\n  'answer': 'Lyon',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 56,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"It ranked second in France and 40th globally in Mercer's 2019 liveability rankings.\",\n  'similarity': 0.43785292,\n  'score': 9.458052545596729e-07,\n  'start': 20,\n  'end': 26,\n  'answer': 'France',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 22,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris hosts the annual French Open Grand Slam tennis tournament on the red clay of Roland Garros.',\n  'similarity': 0.36557713,\n  'score': 6.266361083362426e-07,\n  'start': 83,\n  'end': 97,\n  'answer': 'Roland Garros.',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 8,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': '23 million passengers daily; it is the second-busiest metro system in Europe after the Moscow Metro.',\n  'similarity': 0.36415952,\n  'score': 1.0279434548010613e-07,\n  'start': 87,\n  'end': 100,\n  'answer': 'Moscow Metro.',\n  'question': 'What is the name of the football club of Paris?'},\n {'id': 25,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Every July, the Tour de France bicycle race finishes on the Avenue des Champs-\u00c9lys\u00e9es in Paris.',\n  'similarity': 0.38520125,\n  'score': 8.300106912884075e-08,\n  'start': 89,\n  'end': 95,\n  'answer': 'Paris.',\n  'question': 'What is the name of the football club of Paris?'}]</pre> <p>Toulouse in France is known as \"The Pink City\".</p> In\u00a0[8]: Copied! <pre>search(\"What is the color of Toulouse?\")\n</pre> search(\"What is the color of Toulouse?\") <pre>Question answering: 100%|\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.57s/it]\n</pre> Out[8]: <pre>[{'id': 40,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The city\\'s unique architecture made of pinkish terracotta bricks has earned Toulouse the nickname La Ville Rose (\"The Pink City\").',\n  'similarity': 0.640329,\n  'score': 0.5257010459899902,\n  'start': 39,\n  'end': 46,\n  'answer': 'pinkish',\n  'question': 'What is the color of Toulouse?'},\n {'id': 64,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Crossed by the Garonne River and bordering the Atlantic Coast, the metropolis, a perfect example of the Age of Enlightment, has been showcasing since the 18th century its blond and golden facades, its courtyards and monumental squares, as well as its lively streets accompanied by its French-style gardens.',\n  'similarity': 0.41179663,\n  'score': 0.36229389905929565,\n  'start': 171,\n  'end': 176,\n  'answer': 'blond',\n  'question': 'What is the color of Toulouse?'},\n {'id': 35,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': \"According to the rankings of L'Express and Challenges, Toulouse is the most dynamic French city.\",\n  'similarity': 0.43199244,\n  'score': 0.33106717467308044,\n  'start': 84,\n  'end': 90,\n  'answer': 'French',\n  'question': 'What is the color of Toulouse?'},\n {'id': 81,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': \"French is the city's official language and in 2016 was the only home language of 53.\",\n  'similarity': 0.3581546,\n  'score': 0.033816203474998474,\n  'start': 0,\n  'end': 6,\n  'answer': 'French',\n  'question': 'What is the color of Toulouse?'},\n {'id': 26,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Toulouse ( too-LOOZ, French: [tuluz] (listen); Occitan: Tolosa [tu\u02c8luz\u0254]; Latin: Tolosa [t\u0254\u02c8lo\u02d0sa]) is the prefecture of the French department of Haute-Garonne and of the larger region of Occitanie.',\n  'similarity': 0.57575536,\n  'score': 0.024280602112412453,\n  'start': 11,\n  'end': 35,\n  'answer': 'too-LOOZ, French: [tuluz',\n  'question': 'What is the color of Toulouse?'},\n {'id': 63,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The term \"Bordelais\" may also refer to the city and its surrounding region.',\n  'similarity': 0.43531418,\n  'score': 0.010926609858870506,\n  'start': 10,\n  'end': 19,\n  'answer': 'Bordelais',\n  'question': 'What is the color of Toulouse?'},\n {'id': 27,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The city is on the banks of the River Garonne, 150 kilometres (93 miles) from the Mediterranean Sea, 230 km (143 mi) from the Atlantic Ocean and 680 km (420 mi) from Paris.',\n  'similarity': 0.48132837,\n  'score': 0.001753090531565249,\n  'start': 32,\n  'end': 45,\n  'answer': 'River Garonne',\n  'question': 'What is the color of Toulouse?'},\n {'id': 41,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon or Lyons (UK: , US: , French: [lj\u0254\u0303] (listen); Arpitan: Liyon, pronounced [\u028ej\u0254\u0303]) is the third-largest city and second-largest urban area of France.',\n  'similarity': 0.39131057,\n  'score': 0.0004565822018776089,\n  'start': 0,\n  'end': 4,\n  'answer': 'Lyon',\n  'question': 'What is the color of Toulouse?'},\n {'id': 28,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'It is the fourth-largest commune in France, with 479,553 inhabitants within its municipal boundaries (as of January 2017), after Paris, Marseille and Lyon, ahead of Nice; it has a population of 1,360,829 within its wider metropolitan area (also as of January 2017).',\n  'similarity': 0.5390336,\n  'score': 0.0002655640128068626,\n  'start': 165,\n  'end': 169,\n  'answer': 'Nice',\n  'question': 'What is the color of Toulouse?'},\n {'id': 57,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux ( bor-DOH, French: [b\u0254\u0281do] (listen); Gascon Occitan: Bord\u00e8u [bu\u027e\u02c8\u00f0\u025bw]) is a port city on the river Garonne in the Gironde department, Southwestern France.',\n  'similarity': 0.40526178,\n  'score': 0.0001682575821178034,\n  'start': 0,\n  'end': 8,\n  'answer': 'Bordeaux',\n  'question': 'What is the color of Toulouse?'},\n {'id': 37,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'It is now the capital of the Occitanie region, the second largest region in Metropolitan France.',\n  'similarity': 0.54804677,\n  'score': 0.0001560609816806391,\n  'start': 29,\n  'end': 38,\n  'answer': 'Occitanie',\n  'question': 'What is the color of Toulouse?'},\n {'id': 48,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"The city is recognised for its cuisine and gastronomy, as well as historical and architectural landmarks; as such, the districts of Old Lyon, the Fourvi\u00e8re hill, the Presqu'\u00eele and the slopes of the Croix-Rousse are inscribed on the UNESCO World Heritage List.\",\n  'similarity': 0.35850886,\n  'score': 0.00015423276636283845,\n  'start': 136,\n  'end': 140,\n  'answer': 'Lyon',\n  'question': 'What is the color of Toulouse?'},\n {'id': 62,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Its inhabitants are called \"Bordelais\" (for men) or \"Bordelaises\" (women).',\n  'similarity': 0.41753897,\n  'score': 0.00013673467037733644,\n  'start': 28,\n  'end': 47,\n  'answer': 'Bordelais\" (for men',\n  'question': 'What is the color of Toulouse?'},\n {'id': 36,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Founded by the Romans, the city was the capital of the Visigothic Kingdom in the 5th century and the capital of the province of Languedoc in the Late Middle Ages and early modern period (provinces were abolished during the French Revolution), making it the unofficial capital of the cultural region of Occitania (Southern France).',\n  'similarity': 0.5121455,\n  'score': 0.00010824214405147359,\n  'start': 322,\n  'end': 328,\n  'answer': 'France',\n  'question': 'What is the color of Toulouse?'},\n {'id': 29,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Toulouse is the centre of the European aerospace industry, with the headquarters of Airbus (formerly EADS), the SPOT satellite system, ATR and the Aerospace Valley.',\n  'similarity': 0.39126396,\n  'score': 8.086419984465465e-05,\n  'start': 30,\n  'end': 38,\n  'answer': 'European',\n  'question': 'What is the color of Toulouse?'},\n {'id': 32,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The University of Toulouse is one of the oldest in Europe (founded in 1229) and, with more than 103,000 students, it is the fourth-largest university campus in France, after the universities of Paris, Lyon and Lille.',\n  'similarity': 0.44059187,\n  'score': 4.404589344630949e-05,\n  'start': 160,\n  'end': 166,\n  'answer': 'France',\n  'question': 'What is the color of Toulouse?'},\n {'id': 33,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The air route between Toulouse\u2013Blagnac and the Parisian airports is the busiest in France, transporting 3.',\n  'similarity': 0.44097584,\n  'score': 2.8188960641273297e-05,\n  'start': 22,\n  'end': 38,\n  'answer': 'Toulouse\u2013Blagnac',\n  'question': 'What is the color of Toulouse?'},\n {'id': 51,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'It is also known for its light festival, the F\u00eate des Lumi\u00e8res, which begins every 8 December and lasts for four days, earning Lyon the title of \"Capital of Lights\".',\n  'similarity': 0.38153768,\n  'score': 2.5527337129460648e-05,\n  'start': 25,\n  'end': 62,\n  'answer': 'light festival, the F\u00eate des Lumi\u00e8res',\n  'question': 'What is the color of Toulouse?'},\n {'id': 16,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris received 12.',\n  'similarity': 0.3566119,\n  'score': 3.2625996482238406e-06,\n  'start': 15,\n  'end': 18,\n  'answer': '12.',\n  'question': 'What is the color of Toulouse?'},\n {'id': 38,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Toulouse counts three UNESCO World Heritage Sites: the Canal du Midi (designated in 1996 and shared with other cities), and the Basilica of St.',\n  'similarity': 0.4401517,\n  'score': 1.537480898150534e-06,\n  'start': 0,\n  'end': 8,\n  'answer': 'Toulouse',\n  'question': 'What is the color of Toulouse?'},\n {'id': 73,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': \"In June 2007, the Port of the Moon in historic Bordeux were inscribed on the UNESCO World Heritage List, for its outstanding architecture and urban ensemble and in recognition of Bordeux's international ijmportance over the last 2000 years.\",\n  'similarity': 0.3526206,\n  'score': 1.3830921261615003e-06,\n  'start': 47,\n  'end': 54,\n  'answer': 'Bordeux',\n  'question': 'What is the color of Toulouse?'},\n {'id': 34,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': '2 million passengers in 2019.',\n  'similarity': 0.48204136,\n  'score': 1.3364382311920053e-06,\n  'start': 0,\n  'end': 29,\n  'answer': '2 million passengers in 2019.',\n  'question': 'What is the color of Toulouse?'},\n {'id': 70,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is an international tourist destination for its architectural and cultural heritage with more than 350 historic monuments, making it, after Paris, the city with the most listed or registered monuments in France.',\n  'similarity': 0.36267632,\n  'score': 1.1475751762191067e-06,\n  'start': 0,\n  'end': 8,\n  'answer': 'Bordeaux',\n  'question': 'What is the color of Toulouse?'},\n {'id': 93,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': '4% of the population able to speak both English and French.',\n  'similarity': 0.358346,\n  'score': 8.207165365092806e-07,\n  'start': 52,\n  'end': 59,\n  'answer': 'French.',\n  'question': 'What is the color of Toulouse?'},\n {'id': 31,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Thales Alenia Space, ATR, SAFRAN, Liebherr-Aerospace and Airbus Defence and Space also have a significant presence in Toulouse.',\n  'similarity': 0.39821905,\n  'score': 6.988089467085956e-07,\n  'start': 118,\n  'end': 126,\n  'answer': 'Toulouse',\n  'question': 'What is the color of Toulouse?'},\n {'id': 30,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': \"It also hosts the European headquarters of Intel and the CNES's Toulouse Space Centre (CST), the largest space centre in Europe.\",\n  'similarity': 0.40180072,\n  'score': 5.210671361055574e-07,\n  'start': 121,\n  'end': 127,\n  'answer': 'Europe',\n  'question': 'What is the color of Toulouse?'},\n {'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 0.3795408,\n  'score': 1.7200987656451616e-07,\n  'start': 91,\n  'end': 97,\n  'answer': 'Paris.',\n  'question': 'What is the color of Toulouse?'},\n {'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).',\n  'similarity': 0.35670274,\n  'score': 8.119992855881719e-08,\n  'start': 29,\n  'end': 35,\n  'answer': '\\u200b[pa\u0281i',\n  'question': 'What is the color of Toulouse?'},\n {'id': 39,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Sernin, the largest remaining Romanesque building in Europe, designated in 1998 along with the former hospital H\u00f4tel-Dieu Saint-Jacques because of their significance to the Santiago de Compostela pilgrimage route.',\n  'similarity': 0.40986514,\n  'score': 3.506784196360968e-08,\n  'start': 30,\n  'end': 40,\n  'answer': 'Romanesque',\n  'question': 'What is the color of Toulouse?'},\n {'id': 49,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon was historically an important area for the production and weaving of silk.',\n  'similarity': 0.38565272,\n  'score': 1.6559930315906968e-08,\n  'start': 0,\n  'end': 79,\n  'answer': 'Lyon was historically an important area for the production and weaving of silk.',\n  'question': 'What is the color of Toulouse?'}]</pre> <p>Bordeaux is known worldwide for its wine.</p> In\u00a0[9]: Copied! <pre>search(\"What is the speciality of Bordeaux ?\")\n</pre> search(\"What is the speciality of Bordeaux ?\") <pre>Question answering: 100%|\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.61s/it]\n</pre> Out[9]: <pre>[{'id': 65,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': \"Bordeaux is a world capital of wine, with its castles and vineyards of the Bordeaux region that stand on the hillsides of the Gironde and is home to the world's main wine fair, Vinexpo.\",\n  'similarity': 0.64157647,\n  'score': 0.7739061713218689,\n  'start': 31,\n  'end': 35,\n  'answer': 'wine',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 74,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is also ranked as a Sufficiency city by the Globalization and World Cities Research Network.',\n  'similarity': 0.5668111,\n  'score': 0.7677939534187317,\n  'start': 29,\n  'end': 40,\n  'answer': 'Sufficiency',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 68,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The link with aviation dates back to 1910, the year the first airplane flew over the city.',\n  'similarity': 0.45792317,\n  'score': 0.6874910593032837,\n  'start': 14,\n  'end': 22,\n  'answer': 'aviation',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 59,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is the centre of Bordeaux M\u00e9tropole that has a population of 796,273 (2019), the sixth-largest in France after Paris, Lyon, Marseille, Toulouse and Lille with its immediate suburbs and closest satellite towns.',\n  'similarity': 0.5395987,\n  'score': 0.4667576551437378,\n  'start': 26,\n  'end': 44,\n  'answer': 'Bordeaux M\u00e9tropole',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 66,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is also one of the centers of gastronomy and business tourism for the organization of international congresses.',\n  'similarity': 0.63723767,\n  'score': 0.43909284472465515,\n  'start': 39,\n  'end': 49,\n  'answer': 'gastronomy',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 72,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The metropolis has also received awards and rankings by international organizations such as in 1957, Bordeaux was awarded the Europe Prize for its efforts in transmitting the European ideal.',\n  'similarity': 0.4994722,\n  'score': 0.4337575435638428,\n  'start': 158,\n  'end': 189,\n  'answer': 'transmitting the European ideal',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 48,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"The city is recognised for its cuisine and gastronomy, as well as historical and architectural landmarks; as such, the districts of Old Lyon, the Fourvi\u00e8re hill, the Presqu'\u00eele and the slopes of the Croix-Rousse are inscribed on the UNESCO World Heritage List.\",\n  'similarity': 0.45109624,\n  'score': 0.4090783894062042,\n  'start': 31,\n  'end': 53,\n  'answer': 'cuisine and gastronomy',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 57,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux ( bor-DOH, French: [b\u0254\u0281do] (listen); Gascon Occitan: Bord\u00e8u [bu\u027e\u02c8\u00f0\u025bw]) is a port city on the river Garonne in the Gironde department, Southwestern France.',\n  'similarity': 0.6325322,\n  'score': 0.3996892273426056,\n  'start': 85,\n  'end': 89,\n  'answer': 'port',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 67,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'It is a central and strategic hub for the aeronautics, military and space sector, home to international companies such as Dassault Aviation, Ariane Group, Safran and Thal\u00e8s.',\n  'similarity': 0.5798279,\n  'score': 0.3809475898742676,\n  'start': 42,\n  'end': 80,\n  'answer': 'aeronautics, military and space sector',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 70,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is an international tourist destination for its architectural and cultural heritage with more than 350 historic monuments, making it, after Paris, the city with the most listed or registered monuments in France.',\n  'similarity': 0.5996214,\n  'score': 0.3765573799610138,\n  'start': 57,\n  'end': 92,\n  'answer': 'architectural and cultural heritage',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 40,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The city\\'s unique architecture made of pinkish terracotta bricks has earned Toulouse the nickname La Ville Rose (\"The Pink City\").',\n  'similarity': 0.38761497,\n  'score': 0.19654542207717896,\n  'start': 18,\n  'end': 64,\n  'answer': 'architecture made of pinkish terracotta bricks',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 64,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Crossed by the Garonne River and bordering the Atlantic Coast, the metropolis, a perfect example of the Age of Enlightment, has been showcasing since the 18th century its blond and golden facades, its courtyards and monumental squares, as well as its lively streets accompanied by its French-style gardens.',\n  'similarity': 0.52706313,\n  'score': 0.17074397206306458,\n  'start': 285,\n  'end': 305,\n  'answer': 'French-style gardens',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 73,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': \"In June 2007, the Port of the Moon in historic Bordeux were inscribed on the UNESCO World Heritage List, for its outstanding architecture and urban ensemble and in recognition of Bordeux's international ijmportance over the last 2000 years.\",\n  'similarity': 0.50383896,\n  'score': 0.15214470028877258,\n  'start': 189,\n  'end': 214,\n  'answer': 'international ijmportance',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 62,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Its inhabitants are called \"Bordelais\" (for men) or \"Bordelaises\" (women).',\n  'similarity': 0.52885556,\n  'score': 0.1171083152294159,\n  'start': 44,\n  'end': 47,\n  'answer': 'men',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 52,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Economically, Lyon is a major centre for banking, as well as for the chemical, pharmaceutical and biotech industries.',\n  'similarity': 0.40036455,\n  'score': 0.08858604729175568,\n  'start': 41,\n  'end': 48,\n  'answer': 'banking',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 69,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'A crossroads of knowledge through university research, it is home to one of the only two megajoule lasers in the world, as well as a university population of nearly 100,000 students within the Bordeaux metropolis.',\n  'similarity': 0.4822334,\n  'score': 0.052133411169052124,\n  'start': 16,\n  'end': 53,\n  'answer': 'knowledge through university research',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 58,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The municipality (commune) of Bordeaux proper has a population of 257,804 (2019).',\n  'similarity': 0.5038713,\n  'score': 0.04493545740842819,\n  'start': 18,\n  'end': 25,\n  'answer': 'commune',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 93,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': '4% of the population able to speak both English and French.',\n  'similarity': 0.35953513,\n  'score': 0.03135840594768524,\n  'start': 0,\n  'end': 58,\n  'answer': '4% of the population able to speak both English and French',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 49,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon was historically an important area for the production and weaving of silk.',\n  'similarity': 0.44937736,\n  'score': 0.027099115774035454,\n  'start': 74,\n  'end': 78,\n  'answer': 'silk',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 56,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"It ranked second in France and 40th globally in Mercer's 2019 liveability rankings.\",\n  'similarity': 0.4140333,\n  'score': 0.014323828741908073,\n  'start': 62,\n  'end': 73,\n  'answer': 'liveability',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 61,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'It is the capital of the Nouvelle-Aquitaine region, as well as the prefecture of the Gironde department.',\n  'similarity': 0.5252466,\n  'score': 0.012730337679386139,\n  'start': 0,\n  'end': 50,\n  'answer': 'It is the capital of the Nouvelle-Aquitaine region',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 63,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The term \"Bordelais\" may also refer to the city and its surrounding region.',\n  'similarity': 0.62718254,\n  'score': 0.012131016701459885,\n  'start': 10,\n  'end': 74,\n  'answer': 'Bordelais\" may also refer to the city and its surrounding region',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 26,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Toulouse ( too-LOOZ, French: [tuluz] (listen); Occitan: Tolosa [tu\u02c8luz\u0254]; Latin: Tolosa [t\u0254\u02c8lo\u02d0sa]) is the prefecture of the French department of Haute-Garonne and of the larger region of Occitanie.',\n  'similarity': 0.4185983,\n  'score': 0.00626229215413332,\n  'start': 0,\n  'end': 8,\n  'answer': 'Toulouse',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 37,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'It is now the capital of the Occitanie region, the second largest region in Metropolitan France.',\n  'similarity': 0.3949396,\n  'score': 0.005843970458954573,\n  'start': 29,\n  'end': 45,\n  'answer': 'Occitanie region',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 71,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The \"Pearl of Aquitaine\" has been voted European Destination of the year in a 2015 online poll.',\n  'similarity': 0.5632974,\n  'score': 0.0008770654676482081,\n  'start': 0,\n  'end': 23,\n  'answer': 'The \"Pearl of Aquitaine',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 60,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The larger metropolitan area has a population of 1,247,977 (2017).',\n  'similarity': 0.51093805,\n  'score': 0.0007982160313986242,\n  'start': 11,\n  'end': 23,\n  'answer': 'metropolitan',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 39,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Sernin, the largest remaining Romanesque building in Europe, designated in 1998 along with the former hospital H\u00f4tel-Dieu Saint-Jacques because of their significance to the Santiago de Compostela pilgrimage route.',\n  'similarity': 0.35952526,\n  'score': 0.0003476233105175197,\n  'start': 30,\n  'end': 40,\n  'answer': 'Romanesque',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 47,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon became a major economic hub during the Renaissance.',\n  'similarity': 0.3632738,\n  'score': 0.00029291369719430804,\n  'start': 20,\n  'end': 28,\n  'answer': 'economic',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 35,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': \"According to the rankings of L'Express and Challenges, Toulouse is the most dynamic French city.\",\n  'similarity': 0.39081496,\n  'score': 0.00011482322588562965,\n  'start': 76,\n  'end': 83,\n  'answer': 'dynamic',\n  'question': 'What is the speciality of Bordeaux ?'},\n {'id': 41,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon or Lyons (UK: , US: , French: [lj\u0254\u0303] (listen); Arpitan: Liyon, pronounced [\u028ej\u0254\u0303]) is the third-largest city and second-largest urban area of France.',\n  'similarity': 0.3792387,\n  'score': 2.2786307454225607e-05,\n  'start': 0,\n  'end': 4,\n  'answer': 'Lyon',\n  'question': 'What is the speciality of Bordeaux ?'}]</pre> <p>Every year there is a silk festival in Lyon.</p> In\u00a0[10]: Copied! <pre>search(\"What is the speciality of Lyon ?\")\n</pre> search(\"What is the speciality of Lyon ?\") <pre>Question answering: 100%|\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.58s/it]\n</pre> Out[10]: <pre>[{'id': 52,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Economically, Lyon is a major centre for banking, as well as for the chemical, pharmaceutical and biotech industries.',\n  'similarity': 0.69942987,\n  'score': 0.6367450952529907,\n  'start': 41,\n  'end': 48,\n  'answer': 'banking',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 53,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'The city contains a significant software industry with a particular focus on video games; in recent years it has fostered a growing local start-up sector.',\n  'similarity': 0.5620864,\n  'score': 0.5953128933906555,\n  'start': 77,\n  'end': 88,\n  'answer': 'video games',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 48,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"The city is recognised for its cuisine and gastronomy, as well as historical and architectural landmarks; as such, the districts of Old Lyon, the Fourvi\u00e8re hill, the Presqu'\u00eele and the slopes of the Croix-Rousse are inscribed on the UNESCO World Heritage List.\",\n  'similarity': 0.60598767,\n  'score': 0.4346011281013489,\n  'start': 31,\n  'end': 53,\n  'answer': 'cuisine and gastronomy',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 55,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'According to the Globalization and World Rankings Research Institute, Lyon is considered a Beta city, as of 2018.',\n  'similarity': 0.5154138,\n  'score': 0.3933768570423126,\n  'start': 91,\n  'end': 100,\n  'answer': 'Beta city',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 50,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon played a significant role in the history of cinema: it is where Auguste and Louis Lumi\u00e8re invented the cinematograph.',\n  'similarity': 0.5905135,\n  'score': 0.37524232268333435,\n  'start': 49,\n  'end': 55,\n  'answer': 'cinema',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 67,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'It is a central and strategic hub for the aeronautics, military and space sector, home to international companies such as Dassault Aviation, Ariane Group, Safran and Thal\u00e8s.',\n  'similarity': 0.44267184,\n  'score': 0.37140029668807983,\n  'start': 42,\n  'end': 80,\n  'answer': 'aeronautics, military and space sector',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 49,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon was historically an important area for the production and weaving of silk.',\n  'similarity': 0.6723434,\n  'score': 0.34435904026031494,\n  'start': 74,\n  'end': 78,\n  'answer': 'silk',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 46,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Former capital of the Gauls at the time of the Roman Empire, Lyon is the seat of an archbishopric whose holder bears the title of Primate of the Gauls.',\n  'similarity': 0.4933558,\n  'score': 0.3046538829803467,\n  'start': 84,\n  'end': 97,\n  'answer': 'archbishopric',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 51,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'It is also known for its light festival, the F\u00eate des Lumi\u00e8res, which begins every 8 December and lasts for four days, earning Lyon the title of \"Capital of Lights\".',\n  'similarity': 0.52886266,\n  'score': 0.2726683020591736,\n  'start': 25,\n  'end': 39,\n  'answer': 'light festival',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 43,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'The City of Lyon proper had a population of 516,092 in 2017 within its small municipal territory of 48 km2 (19 sq mi), but together with its suburbs and exurbs the Lyon metropolitan area had a population of 2,323,221 that same year, the second-most populated in France.',\n  'similarity': 0.5234837,\n  'score': 0.22288426756858826,\n  'start': 237,\n  'end': 268,\n  'answer': 'second-most populated in France',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 47,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon became a major economic hub during the Renaissance.',\n  'similarity': 0.6418806,\n  'score': 0.16321411728858948,\n  'start': 20,\n  'end': 28,\n  'answer': 'economic',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 69,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'A crossroads of knowledge through university research, it is home to one of the only two megajoule lasers in the world, as well as a university population of nearly 100,000 students within the Bordeaux metropolis.',\n  'similarity': 0.4157374,\n  'score': 0.11405184864997864,\n  'start': 16,\n  'end': 53,\n  'answer': 'knowledge through university research',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 41,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon or Lyons (UK: , US: , French: [lj\u0254\u0303] (listen); Arpitan: Liyon, pronounced [\u028ej\u0254\u0303]) is the third-largest city and second-largest urban area of France.',\n  'similarity': 0.6103668,\n  'score': 0.09764698892831802,\n  'start': 132,\n  'end': 142,\n  'answer': 'urban area',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 63,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'The term \"Bordelais\" may also refer to the city and its surrounding region.',\n  'similarity': 0.4286503,\n  'score': 0.09355312585830688,\n  'start': 10,\n  'end': 19,\n  'answer': 'Bordelais',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 29,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Toulouse is the centre of the European aerospace industry, with the headquarters of Airbus (formerly EADS), the SPOT satellite system, ATR and the Aerospace Valley.',\n  'similarity': 0.38397413,\n  'score': 0.06821610033512115,\n  'start': 39,\n  'end': 57,\n  'answer': 'aerospace industry',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 44,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon and 58 suburban municipalities have formed since 2015 the Metropolis of Lyon, a directly elected metropolitan authority now in charge of most urban issues, with a population of 1,385,927 in 2017.',\n  'similarity': 0.5153016,\n  'score': 0.06548945605754852,\n  'start': 147,\n  'end': 159,\n  'answer': 'urban issues',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 93,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': '4% of the population able to speak both English and French.',\n  'similarity': 0.38949788,\n  'score': 0.04390397667884827,\n  'start': 0,\n  'end': 58,\n  'answer': '4% of the population able to speak both English and French',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 54,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon hosts the international headquarters of Interpol, the International Agency for Research on Cancer, as well as Euronews.',\n  'similarity': 0.56943977,\n  'score': 0.04046850651502609,\n  'start': 115,\n  'end': 123,\n  'answer': 'Euronews',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 45,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon is the prefecture of the Auvergne-Rh\u00f4ne-Alpes region and seat of the Departmental Council of Rh\u00f4ne (whose jurisdiction, however, no longer extends over the Metropolis of Lyon since 2015).',\n  'similarity': 0.55201995,\n  'score': 0.016977420076727867,\n  'start': 74,\n  'end': 103,\n  'answer': 'Departmental Council of Rh\u00f4ne',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 26,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Toulouse ( too-LOOZ, French: [tuluz] (listen); Occitan: Tolosa [tu\u02c8luz\u0254]; Latin: Tolosa [t\u0254\u02c8lo\u02d0sa]) is the prefecture of the French department of Haute-Garonne and of the larger region of Occitanie.',\n  'similarity': 0.3841616,\n  'score': 0.01222841814160347,\n  'start': 0,\n  'end': 8,\n  'answer': 'Toulouse',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 42,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'It is located at the confluence of the rivers Rh\u00f4ne and Sa\u00f4ne, about 470 km (292 mi) southeast of Paris, 320 km (199 mi) north of Marseille and 56 km (35 mi) northeast of Saint-\u00c9tienne.',\n  'similarity': 0.5372427,\n  'score': 0.011921494267880917,\n  'start': 0,\n  'end': 45,\n  'answer': 'It is located at the confluence of the rivers',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 35,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': \"According to the rankings of L'Express and Challenges, Toulouse is the most dynamic French city.\",\n  'similarity': 0.3902865,\n  'score': 0.010680731385946274,\n  'start': 76,\n  'end': 83,\n  'answer': 'dynamic',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\",\n  'similarity': 0.437737,\n  'score': 0.0035781192127615213,\n  'start': 111,\n  'end': 121,\n  'answer': 'gastronomy',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 30,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': \"It also hosts the European headquarters of Intel and the CNES's Toulouse Space Centre (CST), the largest space centre in Europe.\",\n  'similarity': 0.3905908,\n  'score': 0.00046677718637511134,\n  'start': 105,\n  'end': 128,\n  'answer': 'space centre in Europe.',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 66,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is also one of the centers of gastronomy and business tourism for the organization of international congresses.',\n  'similarity': 0.4227606,\n  'score': 0.00044891092693433166,\n  'start': 39,\n  'end': 49,\n  'answer': 'gastronomy',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 56,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"It ranked second in France and 40th globally in Mercer's 2019 liveability rankings.\",\n  'similarity': 0.6205231,\n  'score': 0.00029419001657515764,\n  'start': 62,\n  'end': 73,\n  'answer': 'liveability',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 74,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is also ranked as a Sufficiency city by the Globalization and World Cities Research Network.',\n  'similarity': 0.413479,\n  'score': 0.00025896544684655964,\n  'start': 29,\n  'end': 40,\n  'answer': 'Sufficiency',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 0.38333356,\n  'score': 6.305298302322626e-05,\n  'start': 46,\n  'end': 57,\n  'answer': 'rugby union',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 32,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The University of Toulouse is one of the oldest in Europe (founded in 1229) and, with more than 103,000 students, it is the fourth-largest university campus in France, after the universities of Paris, Lyon and Lille.',\n  'similarity': 0.3976636,\n  'score': 2.2592739696847275e-05,\n  'start': 178,\n  'end': 190,\n  'answer': 'universities',\n  'question': 'What is the speciality of Lyon ?'},\n {'id': 31,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'Thales Alenia Space, ATR, SAFRAN, Liebherr-Aerospace and Airbus Defence and Space also have a significant presence in Toulouse.',\n  'similarity': 0.406978,\n  'score': 6.35207470622845e-06,\n  'start': 0,\n  'end': 19,\n  'answer': 'Thales Alenia Space',\n  'question': 'What is the speciality of Lyon ?'}]</pre>"},{"location":"examples/retriever_ranker_qa/#retriever-ranker-question-answering","title":"Retriever, ranker, question answering\u00b6","text":""},{"location":"examples/union_intersection_rankers/","title":"Union and intersection of rankers","text":"<p>Let's build a pipeline using union <code>|</code> and intersection <code>&amp;</code> operators.</p> In\u00a0[1]: Copied! <pre>from cherche import data, rank, retrieve\nfrom sentence_transformers import SentenceTransformer\n</pre> from cherche import data, rank, retrieve from sentence_transformers import SentenceTransformer <p>The first step is to define the corpus on which we will perform the neural search. The towns dataset contains about a hundred documents, all of which have four attributes, an <code>id</code>, the <code>title</code> of the article, the <code>url</code> and the content of the <code>article</code>.</p> In\u00a0[2]: Copied! <pre>documents = data.load_towns()\ndocuments[:4]\n</pre> documents = data.load_towns() documents[:4] Out[2]: <pre>[{'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).'},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\"},\n {'id': 2,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.'},\n {'id': 3,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Paris Region had a GDP of \u20ac709 billion ($808 billion) in 2017.'}]</pre> <p>Let's create the union of two pipelines. The first with high precision and low recall and the second with better recall.</p> In\u00a0[3]: Copied! <pre># Low recall, high precision\nprecision = retrieve.Flash(key=\"id\", on=[\"title\", \"article\"], k=30) + rank.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n)\n\n# High recall\nrecall = retrieve.TfIdf(\n    key=\"id\", on=[\"title\", \"article\"], documents=documents, k=30\n) + rank.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n)\n</pre> # Low recall, high precision precision = retrieve.Flash(key=\"id\", on=[\"title\", \"article\"], k=30) + rank.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode, )  # High recall recall = retrieve.TfIdf(     key=\"id\", on=[\"title\", \"article\"], documents=documents, k=30 ) + rank.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode, ) In\u00a0[4]: Copied! <pre># Union: precision | recall\nsearch = precision | recall\nsearch.add(documents)\n</pre> # Union: precision | recall search = precision | recall search.add(documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.35s/it]\nEncoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.32s/it]\n</pre> Out[4]: <pre>Union Pipeline\n-----\nFlash retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 110\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105\nTfIdf retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 105\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105\n-----</pre> In\u00a0[5]: Copied! <pre>search(\"Paris football\", k=30)\n</pre> search(\"Paris football\", k=30) <pre>Flash retriever: 100%|\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 8473.34it/s]\nTfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 240.38it/s]\n</pre> Out[5]: <pre>[{'id': 20, 'similarity': 2.074074074074074},\n {'id': 24, 'similarity': 0.5},\n {'id': 16, 'similarity': 0.738095238095238},\n {'id': 21, 'similarity': 0.5689655172413793},\n {'id': 22, 'similarity': 0.4645161290322581},\n {'id': 1, 'similarity': 0.3958333333333333},\n {'id': 0, 'similarity': 0.3463203463203463},\n {'id': 2, 'similarity': 0.3088235294117647},\n {'id': 25, 'similarity': 0.27936507936507937},\n {'id': 6, 'similarity': 0.25555555555555554},\n {'id': 3, 'similarity': 0.23587223587223588},\n {'id': 23, 'similarity': 0.21929824561403508},\n {'id': 14, 'similarity': 0.20512820512820512},\n {'id': 7, 'similarity': 0.19163763066202089},\n {'id': 8, 'similarity': 0.18095238095238095},\n {'id': 17, 'similarity': 0.17151162790697674},\n {'id': 9, 'similarity': 0.16310160427807485},\n {'id': 13, 'similarity': 0.15555555555555556},\n {'id': 12, 'similarity': 0.14874141876430205},\n {'id': 15, 'similarity': 0.1425531914893617},\n {'id': 5, 'similarity': 0.13605442176870747},\n {'id': 10, 'similarity': 0.13012477718360071},\n {'id': 19, 'similarity': 0.1254180602006689},\n {'id': 11, 'similarity': 0.12037037037037036},\n {'id': 4, 'similarity': 0.11636363636363636},\n {'id': 18, 'similarity': 0.11263736263736264},\n {'id': 56, 'similarity': 0.03333333333333333},\n {'id': 51, 'similarity': 0.025},\n {'id': 53, 'similarity': 0.020833333333333332},\n {'id': 52, 'similarity': 0.02},\n {'id': 94, 'similarity': 0.018867924528301886}]</pre> In\u00a0[6]: Copied! <pre>search(\"speciality Lyon\", k=10)\n</pre> search(\"speciality Lyon\", k=10) <pre>Flash retriever: 100%|\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 5377.31it/s]\nTfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 609.37it/s]\n</pre> Out[6]: <pre>[{'id': 49, 'similarity': 2.1818181818181817},\n {'id': 45, 'similarity': 1.1538461538461537},\n {'id': 48, 'similarity': 0.3333333333333333},\n {'id': 41, 'similarity': 0.6428571428571428},\n {'id': 47, 'similarity': 0.5333333333333333},\n {'id': 50, 'similarity': 0.16666666666666666},\n {'id': 42, 'similarity': 0.14285714285714285},\n {'id': 46, 'similarity': 0.125},\n {'id': 44, 'similarity': 0.33986928104575165},\n {'id': 43, 'similarity': 0.3111111111111111},\n {'id': 56, 'similarity': 0.08333333333333333},\n {'id': 55, 'similarity': 0.0625},\n {'id': 10, 'similarity': 0.05263157894736842},\n {'id': 58, 'similarity': 0.05}]</pre> <p>We can automatically map document identifiers to their content.</p> In\u00a0[7]: Copied! <pre>search += documents\n</pre> search += documents In\u00a0[8]: Copied! <pre>search(\"Paris football\", k=30)[:5]\n</pre> search(\"Paris football\", k=30)[:5] <pre>Flash retriever: 100%|\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 10866.07it/s]\nTfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 1649.35it/s]\n</pre> Out[8]: <pre>[{'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 2.074074074074074},\n {'id': 24,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The 1938 and 1998 FIFA World Cups, the 2007 Rugby World Cup, as well as the 1960, 1984 and 2016 UEFA European Championships were also held in the city.',\n  'similarity': 0.5},\n {'id': 16,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris received 12.',\n  'similarity': 0.738095238095238},\n {'id': 21,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The 80,000-seat Stade de France, built for the 1998 FIFA World Cup, is located just north of Paris in the neighbouring commune of Saint-Denis.',\n  'similarity': 0.5689655172413793},\n {'id': 22,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris hosts the annual French Open Grand Slam tennis tournament on the red clay of Roland Garros.',\n  'similarity': 0.4645161290322581}]</pre> In\u00a0[9]: Copied! <pre>search(\"speciality Lyon\", k=30)[:5]\n</pre> search(\"speciality Lyon\", k=30)[:5] <pre>Flash retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 529.05it/s]\nTfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 958.04it/s]\n</pre> Out[9]: <pre>[{'id': 52,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Economically, Lyon is a major centre for banking, as well as for the chemical, pharmaceutical and biotech industries.',\n  'similarity': 2.1176470588235294},\n {'id': 49,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon was historically an important area for the production and weaving of silk.',\n  'similarity': 1.1111111111111112},\n {'id': 56,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"It ranked second in France and 40th globally in Mercer's 2019 liveability rankings.\",\n  'similarity': 0.7719298245614035},\n {'id': 45,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon is the prefecture of the Auvergne-Rh\u00f4ne-Alpes region and seat of the Departmental Council of Rh\u00f4ne (whose jurisdiction, however, no longer extends over the Metropolis of Lyon since 2015).',\n  'similarity': 0.6},\n {'id': 48,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"The city is recognised for its cuisine and gastronomy, as well as historical and architectural landmarks; as such, the districts of Old Lyon, the Fourvi\u00e8re hill, the Presqu'\u00eele and the slopes of the Croix-Rousse are inscribed on the UNESCO World Heritage List.\",\n  'similarity': 0.49523809523809526}]</pre> In\u00a0[10]: Copied! <pre>retriever = retrieve.Lunr(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n</pre> retriever = retrieve.Lunr(key=\"id\", on=[\"title\", \"article\"], documents=documents) <p>We will build a set of rankers consisting of two different pre-trained models with the intersection operator <code>&amp;</code>. The pipeline will only offer the documents returned by the union of the two retrievers and the intersection of the rankers.</p> In\u00a0[11]: Copied! <pre>ranker = rank.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n) &amp; rank.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\n        \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n    ).encode,\n)\n</pre> ranker = rank.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode, ) &amp; rank.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(         \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"     ).encode, ) In\u00a0[12]: Copied! <pre>search = retriever + ranker\nsearch.add(documents)\n</pre> search = retriever + ranker search.add(documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.43s/it]\nEncoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.40s/it]\n</pre> Out[12]: <pre>Lunr retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 105\nIntersection\n-----\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105\n-----</pre> In\u00a0[13]: Copied! <pre>search(\"Paris football\")\n</pre> search(\"Paris football\") Out[13]: <pre>[{'id': 20, 'similarity': 2.0588235294117645},\n {'id': 24, 'similarity': 1.0571428571428572},\n {'id': 16, 'similarity': 0.7207207207207207},\n {'id': 21, 'similarity': 0.5555555555555556},\n {'id': 22, 'similarity': 0.45263157894736844},\n {'id': 1, 'similarity': 0.3833333333333333},\n {'id': 0, 'similarity': 0.33699633699633696},\n {'id': 2, 'similarity': 0.2965116279069767},\n {'id': 25, 'similarity': 0.261437908496732},\n {'id': 6, 'similarity': 0.24878048780487805},\n {'id': 3, 'similarity': 0.22529644268774704},\n {'id': 23, 'similarity': 0.2074829931972789},\n {'id': 14, 'similarity': 0.1982905982905983},\n {'id': 7, 'similarity': 0.18541033434650456},\n {'id': 8, 'similarity': 0.18095238095238095},\n {'id': 42, 'similarity': 0.16346153846153846},\n {'id': 32, 'similarity': 0.15931372549019607},\n {'id': 17, 'similarity': 0.14747474747474748},\n {'id': 9, 'similarity': 0.1429990069513406},\n {'id': 27, 'similarity': 0.13703703703703704},\n {'id': 13, 'similarity': 0.13523809523809524},\n {'id': 12, 'similarity': 0.12599681020733652},\n {'id': 15, 'similarity': 0.12143928035982009},\n {'id': 5, 'similarity': 0.11666666666666667},\n {'id': 70, 'similarity': 0.11174603174603175},\n {'id': 10, 'similarity': 0.1076923076923077},\n {'id': 19, 'similarity': 0.11952861952861953},\n {'id': 94, 'similarity': 0.10267857142857142},\n {'id': 11, 'similarity': 0.10122358175750834},\n {'id': 4, 'similarity': 0.09696969696969697},\n {'id': 59, 'similarity': 0.09730301427815971},\n {'id': 28, 'similarity': 0.09639830508474576},\n {'id': 18, 'similarity': 0.09632034632034632}]</pre> In\u00a0[14]: Copied! <pre>search(\"speciality Lyon\")\n</pre> search(\"speciality Lyon\") Out[14]: <pre>[{'id': 52, 'similarity': 2.1},\n {'id': 49, 'similarity': 1.0909090909090908},\n {'id': 56, 'similarity': 0.7619047619047619},\n {'id': 45, 'similarity': 0.58},\n {'id': 48, 'similarity': 0.48695652173913045},\n {'id': 41, 'similarity': 0.41025641025641024},\n {'id': 54, 'similarity': 0.3482142857142857},\n {'id': 47, 'similarity': 0.32407407407407407},\n {'id': 50, 'similarity': 0.28888888888888886},\n {'id': 53, 'similarity': 0.2689655172413793},\n {'id': 42, 'similarity': 0.26515151515151514},\n {'id': 51, 'similarity': 0.2238095238095238},\n {'id': 46, 'similarity': 0.21266968325791857},\n {'id': 55, 'similarity': 0.20346320346320346},\n {'id': 44, 'similarity': 0.1978494623655914},\n {'id': 43, 'similarity': 0.19642857142857142},\n {'id': 32, 'similarity': 0.17027863777089783},\n {'id': 28, 'similarity': 0.16666666666666666},\n {'id': 59, 'similarity': 0.1593172119487909}]</pre> <p>We can automatically map document identifiers to their content.</p> In\u00a0[15]: Copied! <pre>search += documents\n</pre> search += documents In\u00a0[16]: Copied! <pre>search(\"Paris football\")\n</pre> search(\"Paris football\") Out[16]: <pre>[{'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 2.0588235294117645},\n {'id': 24,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The 1938 and 1998 FIFA World Cups, the 2007 Rugby World Cup, as well as the 1960, 1984 and 2016 UEFA European Championships were also held in the city.',\n  'similarity': 1.0571428571428572},\n {'id': 16,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris received 12.',\n  'similarity': 0.7207207207207207},\n {'id': 21,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The 80,000-seat Stade de France, built for the 1998 FIFA World Cup, is located just north of Paris in the neighbouring commune of Saint-Denis.',\n  'similarity': 0.5555555555555556},\n {'id': 22,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris hosts the annual French Open Grand Slam tennis tournament on the red clay of Roland Garros.',\n  'similarity': 0.45263157894736844},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\",\n  'similarity': 0.3833333333333333},\n {'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).',\n  'similarity': 0.33699633699633696},\n {'id': 2,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.',\n  'similarity': 0.2965116279069767},\n {'id': 25,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Every July, the Tour de France bicycle race finishes on the Avenue des Champs-\u00c9lys\u00e9es in Paris.',\n  'similarity': 0.261437908496732},\n {'id': 6,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris is a major railway, highway, and air-transport hub served by two international airports: Paris\u2013Charles de Gaulle (the second-busiest airport in Europe) and Paris\u2013Orly.',\n  'similarity': 0.24878048780487805},\n {'id': 3,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Paris Region had a GDP of \u20ac709 billion ($808 billion) in 2017.',\n  'similarity': 0.22529644268774704},\n {'id': 23,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The city hosted the Olympic Games in 1900, 1924 and will host the 2024 Summer Olympics.',\n  'similarity': 0.2074829931972789},\n {'id': 14,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The historical district along the Seine in the city centre has been classified as a UNESCO World Heritage Site since 1991; popular landmarks there include the Cathedral of Notre Dame de Paris on the \u00cele de la Cit\u00e9, now closed for renovation after the 15 April 2019 fire.',\n  'similarity': 0.1982905982905983},\n {'id': 7,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Opened in 1900, the city's subway system, the Paris M\u00e9tro, serves 5.\",\n  'similarity': 0.18541033434650456},\n {'id': 8,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': '23 million passengers daily; it is the second-busiest metro system in Europe after the Moscow Metro.',\n  'similarity': 0.18095238095238095},\n {'id': 42,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'It is located at the confluence of the rivers Rh\u00f4ne and Sa\u00f4ne, about 470 km (292 mi) southeast of Paris, 320 km (199 mi) north of Marseille and 56 km (35 mi) northeast of Saint-\u00c9tienne.',\n  'similarity': 0.16346153846153846},\n {'id': 32,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The University of Toulouse is one of the oldest in Europe (founded in 1229) and, with more than 103,000 students, it is the fourth-largest university campus in France, after the universities of Paris, Lyon and Lille.',\n  'similarity': 0.15931372549019607},\n {'id': 17,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': '6 million visitors in 2020, measured by hotel stays, a drop of 73 percent from 2019, due to the COVID-19 virus.',\n  'similarity': 0.14747474747474748},\n {'id': 9,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Gare du Nord is the 24th-busiest railway station in the world, but the busiest located outside Japan, with 262 million passengers in 2015.',\n  'similarity': 0.1429990069513406},\n {'id': 27,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The city is on the banks of the River Garonne, 150 kilometres (93 miles) from the Mediterranean Sea, 230 km (143 mi) from the Atlantic Ocean and 680 km (420 mi) from Paris.',\n  'similarity': 0.13703703703703704},\n {'id': 13,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Mus\u00e9e Rodin and Mus\u00e9e Picasso exhibit the works of two noted Parisians.',\n  'similarity': 0.13523809523809524},\n {'id': 12,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"The Pompidou Centre Mus\u00e9e National d'Art Moderne has the largest collection of modern and contemporary art in Europe.\",\n  'similarity': 0.12599681020733652},\n {'id': 15,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Other popular tourist sites include the Gothic royal chapel of Sainte-Chapelle, also on the \u00cele de la Cit\u00e9; the Eiffel Tower, constructed for the Paris Universal Exposition of 1889; the Grand Palais and Petit Palais, built for the Paris Universal Exposition of 1900; the Arc de Triomphe on the Champs-\u00c9lys\u00e9es, and the hill of Montmartre with its artistic history and its Basilica of Sacr\u00e9-Coeur.',\n  'similarity': 0.12143928035982009},\n {'id': 5,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Another source ranked Paris as most expensive, on par with Singapore and Hong Kong, in 2018.',\n  'similarity': 0.11666666666666667},\n {'id': 70,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is an international tourist destination for its architectural and cultural heritage with more than 350 historic monuments, making it, after Paris, the city with the most listed or registered monuments in France.',\n  'similarity': 0.11174603174603175},\n {'id': 10,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris is especially known for its museums and architectural landmarks: the Louvre remained the most-visited museum in the world with  2,677,504 visitors in 2020, despite the long museum closings caused by the COVID-19 virus.',\n  'similarity': 0.1076923076923077},\n {'id': 19,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Museums re-opened in 2021, with limitations on the number of visitors at a time and a requirement that visitors wear masks.',\n  'similarity': 0.11952861952861953},\n {'id': 94,\n  'title': 'Montreal',\n  'url': 'https://en.wikipedia.org/wiki/Montreal',\n  'article': 'Montreal is the second-largest primarily French-speaking city in the developed world, after Paris.',\n  'similarity': 0.10267857142857142},\n {'id': 11,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"The Mus\u00e9e d'Orsay, Mus\u00e9e Marmottan Monet and Mus\u00e9e de l'Orangerie are noted for their collections of French Impressionist art.\",\n  'similarity': 0.10122358175750834},\n {'id': 4,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'According to the Economist Intelligence Unit Worldwide Cost of Living Survey in 2018, Paris was the second most expensive city in the world, after Singapore and ahead of Z\u00fcrich, Hong Kong, Oslo, and Geneva.',\n  'similarity': 0.09696969696969697},\n {'id': 59,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is the centre of Bordeaux M\u00e9tropole that has a population of 796,273 (2019), the sixth-largest in France after Paris, Lyon, Marseille, Toulouse and Lille with its immediate suburbs and closest satellite towns.',\n  'similarity': 0.09730301427815971},\n {'id': 28,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'It is the fourth-largest commune in France, with 479,553 inhabitants within its municipal boundaries (as of January 2017), after Paris, Marseille and Lyon, ahead of Nice; it has a population of 1,360,829 within its wider metropolitan area (also as of January 2017).',\n  'similarity': 0.09639830508474576},\n {'id': 18,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The number of foreign visitors declined by 80.',\n  'similarity': 0.09632034632034632}]</pre> In\u00a0[17]: Copied! <pre>search(\"speciality Lyon\")\n</pre> search(\"speciality Lyon\") Out[17]: <pre>[{'id': 52,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Economically, Lyon is a major centre for banking, as well as for the chemical, pharmaceutical and biotech industries.',\n  'similarity': 2.1},\n {'id': 49,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon was historically an important area for the production and weaving of silk.',\n  'similarity': 1.0909090909090908},\n {'id': 56,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"It ranked second in France and 40th globally in Mercer's 2019 liveability rankings.\",\n  'similarity': 0.7619047619047619},\n {'id': 45,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon is the prefecture of the Auvergne-Rh\u00f4ne-Alpes region and seat of the Departmental Council of Rh\u00f4ne (whose jurisdiction, however, no longer extends over the Metropolis of Lyon since 2015).',\n  'similarity': 0.58},\n {'id': 48,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"The city is recognised for its cuisine and gastronomy, as well as historical and architectural landmarks; as such, the districts of Old Lyon, the Fourvi\u00e8re hill, the Presqu'\u00eele and the slopes of the Croix-Rousse are inscribed on the UNESCO World Heritage List.\",\n  'similarity': 0.48695652173913045},\n {'id': 41,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon or Lyons (UK: , US: , French: [lj\u0254\u0303] (listen); Arpitan: Liyon, pronounced [\u028ej\u0254\u0303]) is the third-largest city and second-largest urban area of France.',\n  'similarity': 0.41025641025641024},\n {'id': 54,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon hosts the international headquarters of Interpol, the International Agency for Research on Cancer, as well as Euronews.',\n  'similarity': 0.3482142857142857},\n {'id': 47,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon became a major economic hub during the Renaissance.',\n  'similarity': 0.32407407407407407},\n {'id': 50,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon played a significant role in the history of cinema: it is where Auguste and Louis Lumi\u00e8re invented the cinematograph.',\n  'similarity': 0.28888888888888886},\n {'id': 53,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'The city contains a significant software industry with a particular focus on video games; in recent years it has fostered a growing local start-up sector.',\n  'similarity': 0.2689655172413793},\n {'id': 42,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'It is located at the confluence of the rivers Rh\u00f4ne and Sa\u00f4ne, about 470 km (292 mi) southeast of Paris, 320 km (199 mi) north of Marseille and 56 km (35 mi) northeast of Saint-\u00c9tienne.',\n  'similarity': 0.26515151515151514},\n {'id': 51,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'It is also known for its light festival, the F\u00eate des Lumi\u00e8res, which begins every 8 December and lasts for four days, earning Lyon the title of \"Capital of Lights\".',\n  'similarity': 0.2238095238095238},\n {'id': 46,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Former capital of the Gauls at the time of the Roman Empire, Lyon is the seat of an archbishopric whose holder bears the title of Primate of the Gauls.',\n  'similarity': 0.21266968325791857},\n {'id': 55,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'According to the Globalization and World Rankings Research Institute, Lyon is considered a Beta city, as of 2018.',\n  'similarity': 0.20346320346320346},\n {'id': 44,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon and 58 suburban municipalities have formed since 2015 the Metropolis of Lyon, a directly elected metropolitan authority now in charge of most urban issues, with a population of 1,385,927 in 2017.',\n  'similarity': 0.1978494623655914},\n {'id': 43,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'The City of Lyon proper had a population of 516,092 in 2017 within its small municipal territory of 48 km2 (19 sq mi), but together with its suburbs and exurbs the Lyon metropolitan area had a population of 2,323,221 that same year, the second-most populated in France.',\n  'similarity': 0.19642857142857142},\n {'id': 32,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'The University of Toulouse is one of the oldest in Europe (founded in 1229) and, with more than 103,000 students, it is the fourth-largest university campus in France, after the universities of Paris, Lyon and Lille.',\n  'similarity': 0.17027863777089783},\n {'id': 28,\n  'title': 'Toulouse',\n  'url': 'https://en.wikipedia.org/wiki/Toulouse',\n  'article': 'It is the fourth-largest commune in France, with 479,553 inhabitants within its municipal boundaries (as of January 2017), after Paris, Marseille and Lyon, ahead of Nice; it has a population of 1,360,829 within its wider metropolitan area (also as of January 2017).',\n  'similarity': 0.16666666666666666},\n {'id': 59,\n  'title': 'Bordeaux',\n  'url': 'https://en.wikipedia.org/wiki/Bordeaux',\n  'article': 'Bordeaux is the centre of Bordeaux M\u00e9tropole that has a population of 796,273 (2019), the sixth-largest in France after Paris, Lyon, Marseille, Toulouse and Lille with its immediate suburbs and closest satellite towns.',\n  'similarity': 0.1593172119487909}]</pre>"},{"location":"examples/union_intersection_rankers/#union-and-intersection-of-rankers","title":"Union and intersection of rankers\u00b6","text":""},{"location":"examples/union_intersection_rankers/#union","title":"Union\u00b6","text":""},{"location":"examples/union_intersection_rankers/#intersection","title":"Intersection\u00b6","text":""},{"location":"examples/voting/","title":"Voting operator for retrievers and rankers","text":"<p>Let's build a pipeline using voting <code>*</code> and union <code>|</code> operators.</p> In\u00a0[1]: Copied! <pre>from cherche import data, rank, retrieve\nfrom sentence_transformers import SentenceTransformer\n</pre> from cherche import data, rank, retrieve from sentence_transformers import SentenceTransformer <p>The first step is to define the corpus on which we will perform the neural search. The towns dataset contains about a hundred documents, all of which have four attributes, an <code>id</code>, the <code>title</code> of the article, the <code>url</code> and the content of the <code>article</code>.</p> In\u00a0[2]: Copied! <pre>documents = data.load_towns()\ndocuments[:4]\n</pre> documents = data.load_towns() documents[:4] Out[2]: <pre>[{'id': 0,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris (French pronunciation: \\u200b[pa\u0281i] (listen)) is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles).'},\n {'id': 1,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Since the 17th century, Paris has been one of Europe's major centres of finance, diplomacy, commerce, fashion, gastronomy, science, and arts.\"},\n {'id': 2,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.'},\n {'id': 3,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The Paris Region had a GDP of \u20ac709 billion ($808 billion) in 2017.'}]</pre> <p>We start by creating a retriever whose mission will be to quickly filter the documents. This retriever will match the query with the documents using the title and content of the article with <code>on</code> parameter.</p> In\u00a0[3]: Copied! <pre>retriever = retrieve.TfIdf(\n    key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100\n)\n</pre> retriever = retrieve.TfIdf(     key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100 ) <p>We will use two pre-trained models as rankers composed of the voting operator.</p> In\u00a0[4]: Copied! <pre>ranker = rank.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n    k=30,\n) * rank.Encoder(\n    key=\"id\",\n    on=[\"title\", \"article\"],\n    encoder=SentenceTransformer(\n        \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n    ).encode,\n    k=30,\n)\n</pre> ranker = rank.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,     k=30, ) * rank.Encoder(     key=\"id\",     on=[\"title\", \"article\"],     encoder=SentenceTransformer(         \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"     ).encode,     k=30, ) In\u00a0[5]: Copied! <pre>search = retriever + ranker\nsearch.add(documents)\n</pre> search = retriever + ranker search.add(documents) <pre>Encoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.29s/it]\nEncoder ranker: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.26s/it]\n</pre> Out[5]: <pre>TfIdf retriever\n\tkey      : id\n\ton       : title, article\n\tdocuments: 105\nVote\n-----\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105\nEncoder ranker\n\tkey       : id\n\ton        : title, article\n\tnormalize : True\n\tembeddings: 105\n-----</pre> <p>The output similarity score of the pipeline is composed of the average of the similarity scores of the models. The scores have been normalized for each model.</p> In\u00a0[6]: Copied! <pre>search(\"Paris football\")\n</pre> search(\"Paris football\") Out[6]: <pre>[{'id': 20, 'similarity': 2.064516129032258},\n {'id': 24, 'similarity': 1.0625},\n {'id': 16, 'similarity': 0.7254901960784313},\n {'id': 21, 'similarity': 0.5606060606060606},\n {'id': 56, 'similarity': 0.4540540540540541},\n {'id': 22, 'similarity': 0.3904761904761905},\n {'id': 1, 'similarity': 0.33699633699633696},\n {'id': 0, 'similarity': 0.3055555555555556},\n {'id': 41, 'similarity': 0.27485380116959063},\n {'id': 2, 'similarity': 0.24761904761904763},\n {'id': 25, 'similarity': 0.2202797202797203},\n {'id': 6, 'similarity': 0.21666666666666667},\n {'id': 3, 'similarity': 0.19732441471571907},\n {'id': 23, 'similarity': 0.18285714285714286},\n {'id': 35, 'similarity': 0.17588652482269504},\n {'id': 14, 'similarity': 0.15555555555555556},\n {'id': 33, 'similarity': 0.1507177033492823},\n {'id': 8, 'similarity': 0.13968957871396898},\n {'id': 7, 'similarity': 0.1369047619047619},\n {'id': 42, 'similarity': 0.1246923707957342},\n {'id': 32, 'similarity': 0.12414965986394558},\n {'id': 17, 'similarity': 0.11201079622132254},\n {'id': 54, 'similarity': 0.10846560846560846},\n {'id': 9, 'similarity': 0.10532915360501567},\n {'id': 27, 'similarity': 0.10238095238095238},\n {'id': 55, 'similarity': 0.0625},\n {'id': 57, 'similarity': 0.058823529411764705},\n {'id': 51, 'similarity': 0.05},\n {'id': 46, 'similarity': 0.04},\n {'id': 45, 'similarity': 0.037037037037037035},\n {'id': 19, 'similarity': 0.023255813953488372},\n {'id': 13, 'similarity': 0.0196078431372549},\n {'id': 18, 'similarity': 0.017241379310344827},\n {'id': 39, 'similarity': 0.01694915254237288},\n {'id': 37, 'similarity': 0.016666666666666666}]</pre> In\u00a0[7]: Copied! <pre>search(\"speciality Lyon\")\n</pre> search(\"speciality Lyon\") Out[7]: <pre>[{'id': 52, 'similarity': 2.064516129032258},\n {'id': 49, 'similarity': 1.0606060606060606},\n {'id': 56, 'similarity': 0.7291666666666666},\n {'id': 45, 'similarity': 0.5555555555555556},\n {'id': 48, 'similarity': 0.45882352941176474},\n {'id': 41, 'similarity': 0.38738738738738737},\n {'id': 54, 'similarity': 0.3322259136212624},\n {'id': 47, 'similarity': 0.3026315789473684},\n {'id': 50, 'similarity': 0.27100271002710025},\n {'id': 53, 'similarity': 0.25},\n {'id': 42, 'similarity': 0.23896103896103896},\n {'id': 51, 'similarity': 0.21014492753623187},\n {'id': 46, 'similarity': 0.1982905982905983},\n {'id': 55, 'similarity': 0.18831168831168832},\n {'id': 44, 'similarity': 0.18095238095238095},\n {'id': 43, 'similarity': 0.1689291101055807},\n {'id': 67, 'similarity': 0.1675531914893617},\n {'id': 63, 'similarity': 0.15192743764172334},\n {'id': 69, 'similarity': 0.1437246963562753},\n {'id': 29, 'similarity': 0.13773584905660377},\n {'id': 74, 'similarity': 0.1286231884057971},\n {'id': 35, 'similarity': 0.12727272727272726},\n {'id': 37, 'similarity': 0.1172316384180791},\n {'id': 57, 'similarity': 0.11703703703703704},\n {'id': 70, 'similarity': 0.11407407407407408},\n {'id': 28, 'similarity': 0.10714285714285714},\n {'id': 93, 'similarity': 0.10114942528735632},\n {'id': 32, 'similarity': 0.047619047619047616},\n {'id': 40, 'similarity': 0.038461538461538464},\n {'id': 36, 'similarity': 0.034482758620689655},\n {'id': 90, 'similarity': 0.0196078431372549},\n {'id': 81, 'similarity': 0.017543859649122806},\n {'id': 68, 'similarity': 0.016666666666666666}]</pre> <p>We can automatically map document identifiers to their content.</p> In\u00a0[8]: Copied! <pre>search += documents\n</pre> search += documents In\u00a0[9]: Copied! <pre>search(\"Paris football\")[:3]\n</pre> search(\"Paris football\")[:3] Out[9]: <pre>[{'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 2.064516129032258},\n {'id': 24,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The 1938 and 1998 FIFA World Cups, the 2007 Rugby World Cup, as well as the 1960, 1984 and 2016 UEFA European Championships were also held in the city.',\n  'similarity': 1.0625},\n {'id': 16,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris received 12.',\n  'similarity': 0.7254901960784313}]</pre> In\u00a0[10]: Copied! <pre>search(\"speciality Lyon\")[:3]\n</pre> search(\"speciality Lyon\")[:3] Out[10]: <pre>[{'id': 52,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Economically, Lyon is a major centre for banking, as well as for the chemical, pharmaceutical and biotech industries.',\n  'similarity': 2.064516129032258},\n {'id': 49,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon was historically an important area for the production and weaving of silk.',\n  'similarity': 1.0606060606060606},\n {'id': 56,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': \"It ranked second in France and 40th globally in Mercer's 2019 liveability rankings.\",\n  'similarity': 0.7291666666666666}]</pre> In\u00a0[11]: Copied! <pre>retriever = retrieve.TfIdf(\n    key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100\n) * retrieve.Lunr(key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100)\n</pre> retriever = retrieve.TfIdf(     key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100 ) * retrieve.Lunr(key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100) In\u00a0[12]: Copied! <pre>search = retriever + documents\n</pre> search = retriever + documents In\u00a0[13]: Copied! <pre>search(\"Paris football\")[:3]\n</pre> search(\"Paris football\")[:3] <pre>TfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 362.83it/s]\nLunr retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 545.57it/s]\n</pre> Out[13]: <pre>[{'id': 20,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n  'similarity': 2.0238095238095237},\n {'id': 16,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris received 12.',\n  'similarity': 1.0235294117647058},\n {'id': 7,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': \"Opened in 1900, the city's subway system, the Paris M\u00e9tro, serves 5.\",\n  'similarity': 0.6893939393939393}]</pre> In\u00a0[14]: Copied! <pre>search(\"speciality Lyon\")[:3]\n</pre> search(\"speciality Lyon\")[:3] <pre>TfIdf retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 279.47it/s]\nLunr retriever: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 820.16it/s]\n</pre> Out[14]: <pre>[{'id': 10,\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'article': 'Paris is especially known for its museums and architectural landmarks: the Louvre remained the most-visited museum in the world with  2,677,504 visitors in 2020, despite the long museum closings caused by the COVID-19 virus.',\n  'similarity': 1.0},\n {'id': 44,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon and 58 suburban municipalities have formed since 2015 the Metropolis of Lyon, a directly elected metropolitan authority now in charge of most urban issues, with a population of 1,385,927 in 2017.',\n  'similarity': 0.6974358974358974},\n {'id': 41,\n  'title': 'Lyon',\n  'url': 'https://en.wikipedia.org/wiki/Lyon',\n  'article': 'Lyon or Lyons (UK: , US: , French: [lj\u0254\u0303] (listen); Arpitan: Liyon, pronounced [\u028ej\u0254\u0303]) is the third-largest city and second-largest urban area of France.',\n  'similarity': 0.5303030303030303}]</pre>"},{"location":"examples/voting/#voting-operator-for-retrievers-and-rankers","title":"Voting operator for retrievers and rankers\u00b6","text":""},{"location":"examples/voting/#voting","title":"Voting\u00b6","text":""},{"location":"examples/voting/#voting-is-also-compatible-with-retrievers","title":"Voting is also compatible with retrievers\u00b6","text":""},{"location":"pipeline/pipeline/","title":"Pipeline","text":"<p>Cherche is a tool that provides operators for building pipelines efficiently. The operators it replaces are <code>+</code> (pipeline), <code>|</code> (union), <code>&amp;</code> (intersection), and <code>*</code> (voting).</p>"},{"location":"pipeline/pipeline/#pipeline_1","title":"Pipeline <code>+</code>","text":"<p>The <code>+</code> operator is used to create pipelines. Here's an example of a pipeline created with a retriever and a ranker:</p> <pre><code>&gt;&gt;&gt; search = retriever + ranker\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>The pipeline allows you to map output indexes to their content, as shown here:</p> <pre><code>&gt;&gt;&gt; search = retriever + ranker + documents\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>When building a pipeline for question answering, mapping ids to documents is mandatory. Here's an example:</p> <pre><code>&gt;&gt;&gt; search_qa = retriever + ranker + documents + question_answering\n&gt;&gt;&gt; search.add(documents)\n</code></pre>"},{"location":"pipeline/pipeline/#union","title":"Union <code>|</code>","text":"<p>The <code>|</code> operator improves neural search recall by gathering documents retrieved by multiple models. The union operator will avoid duplicate documents and keep the first one. The first documents out of the union will be from the first model, and the subsequent ones will be from the second model. This strategy allows prioritizing one model or pipeline over another. It may make sense to create a union between two separate pipelines, with the first one having the highest precision and the second one having better recall, like a spare tire.</p> <p>Here are some examples of unions:</p> <p>Union of two retrievers:</p> <pre><code>&gt;&gt;&gt; search = retriever_a | retriever_b\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Union of two retrievers followed by a ranker:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a | retriever_b) + ranker\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Union of two rankers:</p> <pre><code>&gt;&gt;&gt; search = retriever + (ranker_a | ranker_b)\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Union of two pipelines:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a + ranker_a) | (retriever_b + ranker_b)\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Union of three pipelines:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a + ranker_a) | (retriever_b + ranker_b) | retriever_c\n&gt;&gt;&gt; search.add(documents)\n</code></pre>"},{"location":"pipeline/pipeline/#intersection","title":"Intersection <code>&amp;</code>","text":"<p>The <code>&amp;</code> operator improves the precision of the model by filtering documents on the intersection of proposed candidates of retrievers and rankers.</p> <p>Here are some examples of intersections:</p> <p>Intersection of two retrievers:</p> <pre><code>&gt;&gt;&gt; search = retriever_a &amp; retriever_b\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Intersection of two retrievers followed by a ranker:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a &amp; retriever_b) + ranker\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Intersection of two rankers:</p> <pre><code>&gt;&gt;&gt; search = retriever + (ranker_a &amp; ranker_b)\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Intersection of two pipelines:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a + ranker_a) &amp; (retriever_b + ranker_b)\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Intersection of three pipelines:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a + ranker_a) &amp; (retriever_b + ranker_b) &amp; retriever_c\n&gt;&gt;&gt; search.add(documents)\n</code></pre>"},{"location":"pipeline/pipeline/#voting","title":"Voting <code>*</code>","text":"<p>The <code>*</code> operator improves both the precision and recall of the model by computing the average normalized similarity between the documents.</p> <p>Here are some examples of voting:</p> <p>Vote of two retrievers:</p> <pre><code>&gt;&gt;&gt; search = retriever_a * retriever_b\n&gt;&gt;&gt; search.add\n</code></pre> <p>Vote of two retrievers followed by a ranker:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a * retriever_b) + ranker\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Vote of two rankers:</p> <pre><code>&gt;&gt;&gt; search = retriever + (ranker_a * ranker_b)\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Vote of two pipelines:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a + ranker_a) * (retriever_b + ranker_b)\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>Vote of three pipelines:</p> <pre><code>&gt;&gt;&gt; search = (retriever_a + ranker_a) * (retriever_b + ranker_b) * retriever_c\n&gt;&gt;&gt; search.add(documents)\n</code></pre>"},{"location":"pipeline/pipeline/#lets-create-a-pipeline","title":"Let's create a pipeline","text":"<p>Here we create a pipeline from the union of two distinct pipelines. The first part of the union improves precision, and the second improves recall. We can use the Semanlink dataset to feed our neural search pipeline.</p> <pre><code>&gt;&gt;&gt; search = (retriever_a + ranker_a) | (retriever_b + ranker_b)\n&gt;&gt;&gt; search.add(documents)\n</code></pre> <p>And here is the code:</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve, rank, data\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; from lenlp import sparse\n\n&gt;&gt;&gt; documents, _ = data.arxiv_tags(arxiv_title=True, arxiv_summary=False, comment=False)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    key = \"uri\",\n...    on = [\"prefLabel_text\", \"altLabel_text\"],\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    k = 10,\n... )\n\n&gt;&gt;&gt; ranker.add(documents)\n\n&gt;&gt;&gt; precision = retrieve.Flash(\n...    key = \"uri\",\n...    on = [\"prefLabel\", \"altLabel\"],\n...    k = 100,\n... ).add(documents) + ranker\n\n&gt;&gt;&gt; recall = retrieve.TfIdf(\n...    key = \"uri\",\n...    on = [\"prefLabel_text\", \"altLabel_text\"],\n...    documents = documents,\n...    tfidf = sparse.TfidfVectorizer(normalize=True, min_df=1, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"),\n...    k = 100,\n... ) + ranker\n\n&gt;&gt;&gt; search = precision | recall\n\n&gt;&gt;&gt; search(\"Knowledge Base Embedding By Cooperative Knowledge Distillation\")\n[{'uri': 'http://www.semanlink.net/tag/knowledge_base',\n  'similarity': 2.1666666666666665},\n {'uri': 'http://www.semanlink.net/tag/knowledge_distillation',\n  'similarity': 0.5},\n {'uri': 'http://www.semanlink.net/tag/embeddings',\n  'similarity': 0.3333333333333333},\n {'uri': 'http://www.semanlink.net/tag/knowledge_graph_embeddings',\n  'similarity': 0.25},\n {'uri': 'http://www.semanlink.net/tag/knowledge_driven_embeddings',\n  'similarity': 0.2},\n {'uri': 'http://www.semanlink.net/tag/hierarchy_aware_knowledge_graph_embeddings',\n  'similarity': 0.16666666666666666},\n {'uri': 'http://www.semanlink.net/tag/entity_embeddings',\n  'similarity': 0.14285714285714285},\n {'uri': 'http://www.semanlink.net/tag/text_kg_and_embeddings',\n  'similarity': 0.125},\n {'uri': 'http://www.semanlink.net/tag/text_aware_kg_embedding',\n  'similarity': 0.1111111111111111},\n {'uri': 'http://www.semanlink.net/tag/knowledge_graph_completion',\n  'similarity': 0.1},\n {'uri': 'http://www.semanlink.net/tag/knowledge_graph_deep_learning',\n  'similarity': 0.09090909090909091},\n {'uri': 'http://www.semanlink.net/tag/combining_knowledge_graphs',\n  'similarity': 0.07692307692307693}]\n</code></pre>"},{"location":"qa/qa/","title":"Question Answering","text":"<p>The <code>qa.QA</code> module is a crucial component of our neural search pipeline, integrating an extractive question answering model that is compatible with Hugging Face. This model efficiently extracts the most likely answer spans from a list of documents in response to user queries. To further expedite the search process, our neural search pipeline filters the entire corpus and narrows down the search to a few relevant documents, resulting in faster response times for top answers. However, it's worth noting that even with corpus filtering, question answering models can be slow when using a CPU and typically require a GPU to achieve optimal performance.</p>"},{"location":"qa/qa/#documents","title":"Documents","text":"<p>The pipeline must provide the documents and not only the identifiers to the question answering model such as:</p> <pre><code>search = pipeline + documents + question_answering\n</code></pre>"},{"location":"qa/qa/#tutorial","title":"Tutorial","text":"<pre><code>&gt;&gt;&gt; from cherche import data, rank, retrieve, qa\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; from transformers import pipeline\n\n&gt;&gt;&gt; documents = data.load_towns()\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    key = \"id\",\n...    on = \"article\",\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    k = 30,\n... )\n\n&gt;&gt;&gt; question_answering = qa.QA(\n...    model = pipeline(\"question-answering\",\n...         model = \"deepset/roberta-base-squad2\",\n...         tokenizer = \"deepset/roberta-base-squad2\"\n...    ),\n...    on = \"article\",\n... )\n\n&gt;&gt;&gt; search = retriever + ranker + documents + question_answering\n&gt;&gt;&gt; search.add(documents)\n&gt;&gt;&gt; answers = search(\n...   q=[\n...     \"What is the name of the football club of Paris?\",\n...     \"What is the speciality of Lyon?\"\n...   ]\n... )\n\n# The answer is Paris Saint-Germain\n&gt;&gt;&gt; answers[0][0]\n{'id': 20,\n 'title': 'Paris',\n 'url': 'https://en.wikipedia.org/wiki/Paris',\n 'article': 'The football club Paris Saint-Germain and the rugby union club Stade Fran\u00e7ais are based in Paris.',\n 'similarity': 0.6905894,\n 'score': 0.9848365783691406,\n 'start': 18,\n 'end': 37,\n 'answer': 'Paris Saint-Germain',\n 'question': 'What is the name of the football club of Paris?'}\n\n\n&gt;&gt;&gt; answers[1][0]\n{'id': 52,\n'title': 'Lyon',\n'url': 'https://en.wikipedia.org/wiki/Lyon',\n'article': 'Economically, Lyon is a major centre for banking, as well as for the chemical, pharmaceutical and biotech industries.',\n'similarity': 0.64728546,\n'score': 0.6952874660491943,\n'start': 41,\n'end': 48,\n'answer': 'banking',\n'question': 'What is the speciality of Lyon?'}\n</code></pre>"},{"location":"rank/crossencoder/","title":"rank.CrossEncoder","text":"<p>The <code>rank.CrossEncoder</code> model re-ranks documents in ouput of the retriever using a pre-trained CrossEncoder. The cross coder takes as input both the query and the document and produces a score accordingly. The <code>rank.Encoder</code> can't pre-compute embeddings to speed up search. A GPU will significantly speed up the cross-encoder.</p>"},{"location":"rank/crossencoder/#requirements","title":"Requirements","text":"<p>To use the CrossEncoder ranker we will need to install \"cherche[cpu]\"</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>or on GPU:</p> <pre><code>pip install \"cherche[gpu]\"\n</code></pre>"},{"location":"rank/crossencoder/#documents","title":"Documents","text":"<p>The cross-encoder must take as input the content of the document and not only the keys.</p> <pre><code>search = retriever + documents + cross_encoder\n</code></pre>"},{"location":"rank/crossencoder/#tutorial","title":"Tutorial","text":"<pre><code>&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import CrossEncoder\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n# Cross-Encoder needs documents contents. So we map documents to the output of retriever.\n&gt;&gt;&gt; retriever += documents\n\n&gt;&gt;&gt; ranker = rank.CrossEncoder(\n...     on = [\"title\", \"article\"],\n...     encoder = CrossEncoder(\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\").predict,\n... )\n\n&gt;&gt;&gt; match = retriever([\"paris\", \"art\", \"fashion\"], k=100)\n\n# Re-rank output of retriever\n&gt;&gt;&gt; ranker([\"paris\", \"art\", \"fashion\"], documents=match, k=30)\n[[{'id': 0, # Query 1\n   'article': 'Paris is the capital and most populous city of France',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 6.915566},\n  {'id': 2,\n   'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 6.651541},\n  {'id': 1,\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 4.3157015}],\n [{'id': 1, # Query 2\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': -2.9981978}],\n [{'id': 2, # Query 3\n   'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': -4.356096},\n  {'id': 1,\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': -4.7529125}]]\n</code></pre>"},{"location":"rank/crossencoder/#ranker-in-pipeline","title":"Ranker in pipeline","text":"<pre><code>&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import CrossEncoder\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100)\n\n&gt;&gt;&gt; ranker = rank.CrossEncoder(\n...     on = [\"title\", \"article\"],\n...     encoder = CrossEncoder(\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\").predict,\n... )\n\n# Cross-Encoder needs documents contents. So we map documents to the output of retriever.\n&gt;&gt;&gt; search = retriever + documents + ranker\n&gt;&gt;&gt; search(q=[\"paris\", \"arts\", \"fashion\"])\n[[{'id': 0, # Query 1\n   'article': 'Paris is the capital and most populous city of France',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 6.915566},\n  {'id': 2,\n   'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 6.651541},\n  {'id': 1,\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 4.3157015}],\n [{'id': 1, # Query 2\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': -2.9981978}],\n [{'id': 2, # Query 3\n   'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': -4.356096},\n  {'id': 1,\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': -4.7529125}]]\n</code></pre>"},{"location":"rank/crossencoder/#pre-trained-cross-encoders","title":"Pre-trained Cross-Encoders","text":"<p>Pre-trained Cross-Encoder are available on Hugging Face hub.</p>"},{"location":"rank/dpr/","title":"rank.DPR","text":"<p>The <code>rank.DPR</code> model re-ranks documents in ouput of the retriever. <code>rank.DPR</code> is dedicated to the Dense Passage Retrieval models which aims to use two distinct neural networks, one that encodes the query and the other one that encodes the documents.</p> <p>The <code>rank.DPR</code> can pre-compute the set of document embeddings to speed up search and avoiding computing embeddings twice using method <code>.add</code>. A GPU will significantly reduce pre-computing time dedicated to document embeddings.</p>"},{"location":"rank/dpr/#tutorial","title":"Tutorial","text":"<p>To use the DPR ranker we will need to install \"cherche[cpu]\"</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>or on GPU:</p> <pre><code>pip install \"cherche[gpu]\"\n</code></pre>"},{"location":"rank/dpr/#tutorial_1","title":"Tutorial","text":"<pre><code>&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.DPR(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base').encode,\n...    query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base').encode,\n...    normalize=True,\n... )\n\n&gt;&gt;&gt; ranker.add(documents, batch_size=64)\n\n&gt;&gt;&gt; match = retriever([\"paris\", \"art\", \"fashion\"], k=100)\n\n# Re-rank output of retriever\n&gt;&gt;&gt; ranker([\"paris\", \"art\", \"fashion\"], documents=match, k=30)\n[[{'id': 0, 'similarity': 8.163156}, # Query 1\n  {'id': 1, 'similarity': 8.021494},\n  {'id': 2, 'similarity': 7.8683443}],\n [{'id': 1, 'similarity': 5.4577255}], # Query 2\n [{'id': 1, 'similarity': 6.8593264}, {'id': 2, 'similarity': 6.1895266}]] # Query 3\n</code></pre>"},{"location":"rank/dpr/#ranker-in-pipeline","title":"Ranker in pipeline","text":"<pre><code>&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100)\n\n&gt;&gt;&gt; ranker = rank.DPR(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base').encode,\n...    query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base').encode,\n...    k = 30,\n... )\n\n&gt;&gt;&gt; search = retriever + ranker\n&gt;&gt;&gt; search.add(documents, batch_size=64)\n&gt;&gt;&gt; search(q=[\"paris\", \"arts\", \"fashion\"])\n[[{'id': 0, 'similarity': 8.163156}, # Query 1\n  {'id': 1, 'similarity': 8.021494},\n  {'id': 2, 'similarity': 7.8683443}],\n [{'id': 1, 'similarity': 5.4577255}], # Query 2\n [{'id': 1, 'similarity': 6.8593264}, {'id': 2, 'similarity': 6.1895266}]] # Query 3\n</code></pre>"},{"location":"rank/dpr/#map-index-to-documents","title":"Map index to documents","text":"<p>We can map the documents to the ids retrieved by the pipeline.</p> <pre><code>&gt;&gt;&gt; search += documents\n&gt;&gt;&gt; search(q=\"arts\")\n[{'id': 1,\n  'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.21684971}]\n</code></pre>"},{"location":"rank/dpr/#pre-trained-models","title":"Pre-trained models","text":"<p>Here is the list of models provided by SentenceTransformers. This list of models is not exhaustive; there is a wide range of models available with Hugging Face and in many languages.</p>"},{"location":"rank/embedding/","title":"rank.Embedding","text":"<p>The <code>rank.Embedding</code> model utilizes pre-computed embeddings to re-rank documents within the output of the retriever. If you have a custom model that produces its own embeddings and want to re-rank documents accordingly, <code>rank.Embedding</code> is the ideal tool for the job.</p>"},{"location":"rank/embedding/#tutorial","title":"Tutorial","text":"<pre><code>&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n# Let's use a custom encoder and create our documents embeddings of shape (n_documents, dim_embeddings)\n&gt;&gt;&gt; encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n&gt;&gt;&gt; embeddings_documents = encoder.encode([\n...    document[\"article\"] for document in documents\n... ])\n\n&gt;&gt;&gt; queries = [\"paris\", \"art\", \"fashion\"]\n\n# Queries embeddings of shape (n_queries, dim_embeddings)\n&gt;&gt;&gt; embeddings_queries = encoder.encode(queries)\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.Embedding(\n...    key = \"id\",\n...    normalize = True,\n... )\n\n&gt;&gt;&gt; ranker = ranker.add(\n...    documents=documents,\n...    embeddings_documents=embeddings_documents,\n... )\n\n&gt;&gt;&gt; match = retriever(queries, k=100)\n\n# Re-rank output of retriever\n&gt;&gt;&gt; ranker(q=embeddings_queries, documents=match, k=30)\n[[{'id': 0, 'similarity': 0.6560695}, # Query 1\n  {'id': 1, 'similarity': 0.58203197},\n  {'id': 2, 'similarity': 0.5283624}],\n [{'id': 1, 'similarity': 0.1115652}], # Query 2\n [{'id': 1, 'similarity': 0.2555524}, {'id': 2, 'similarity': 0.06398084}]] # Query 3\n</code></pre>"},{"location":"rank/embedding/#map-index-to-documents","title":"Map index to documents","text":"<p>We can map the documents to the ids retrieved by the pipeline.</p> <pre><code>&gt;&gt;&gt; ranker += documents\n&gt;&gt;&gt; match = retriever(queries, k=100)\n&gt;&gt;&gt; ranker(q=embeddings_queries, documents=match, k=30)\n[[{'id': 0,\n   'article': 'Paris is the capital and most populous city of France',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 0.6560695},\n  {'id': 1,\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 0.58203197},\n  {'id': 2,\n   'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 0.5283624}],\n [{'id': 1,\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 0.1115652}],\n [{'id': 1,\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 0.2555524},\n  {'id': 2,\n   'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 0.06398084}]]\n</code></pre>"},{"location":"rank/encoder/","title":"rank.Encoder","text":"<p>The <code>rank.Encoder</code> model re-ranks documents in ouput of the retriever using a pre-trained SentenceTransformers.</p> <p>The <code>rank.Encoder</code> can pre-compute the set of document embeddings to speed up search and avoiding computing embeddings twice using method <code>.add</code>. A GPU will significantly reduce pre-computing time dedicated to document embeddings.</p>"},{"location":"rank/encoder/#requirements","title":"Requirements","text":"<p>To use the Encoder ranker we will need to install \"cherche[cpu]\"</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>or on GPU:</p> <pre><code>pip install \"cherche[gpu]\"\n</code></pre>"},{"location":"rank/encoder/#tutorial","title":"Tutorial","text":"<pre><code>&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer(f\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    normalize=True,\n... )\n\n&gt;&gt;&gt; ranker.add(documents, batch_size=64)\n\n&gt;&gt;&gt; match = retriever([\"paris\", \"art\", \"fashion\"], k=100)\n\n# Re-rank output of retriever\n&gt;&gt;&gt; ranker([\"paris\", \"art\", \"fashion\"], documents=match, k=30)\n[[{'id': 0, 'similarity': 0.6638489}, # Query 1\n  {'id': 2, 'similarity': 0.602515},\n  {'id': 1, 'similarity': 0.60133684}],\n [{'id': 1, 'similarity': 0.10321068}], # Query 2\n [{'id': 1, 'similarity': 0.26405674}, {'id': 2, 'similarity': 0.096046045}]] # Query 3\n</code></pre>"},{"location":"rank/encoder/#ranker-in-pipeline","title":"Ranker in pipeline","text":"<pre><code>&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents, k=100)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer(f\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    k = 30,\n...    normalize=True,\n... )\n\n&gt;&gt;&gt; search = retriever + ranker\n&gt;&gt;&gt; search.add(documents)\n&gt;&gt;&gt; search(q=[\"paris\", \"arts\", \"fashion\"])\n[[{'id': 0, 'similarity': 0.6638489}, # Query 1\n  {'id': 2, 'similarity': 0.602515},\n  {'id': 1, 'similarity': 0.60133684}],\n [{'id': 1, 'similarity': 0.21684976}], # Query 2\n [{'id': 1, 'similarity': 0.26405674}, {'id': 2, 'similarity': 0.096046045}]] # Query 3\n</code></pre>"},{"location":"rank/encoder/#map-index-to-documents","title":"Map index to documents","text":"<p>We can map the documents to the ids retrieved by the pipeline.</p> <pre><code>&gt;&gt;&gt; search += documents\n&gt;&gt;&gt; search(q=\"arts\")\n[{'id': 1,\n  'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.21684971}]\n</code></pre>"},{"location":"rank/encoder/#pre-trained-encoders","title":"Pre-trained encoders","text":"<p>Here is the list of models provided by SentenceTransformers. This list of models is not exhaustive; there is a wide range of models available with Hugging Face and in many languages.</p>"},{"location":"rank/rank/","title":"Rank","text":"<p>Rankers are models that measure the semantic similarity between a document and a query. Rankers filter out documents based on the semantic similarity between the query and the documents. Rankers are compatible with all the retrievers.</p> Ranker Precomputing GPU ranker.Encoder \u2705 Highly recommended when precomputing embeddings if the corpus is large. Not needed anymore after precomputing ranker.DPR \u2705 Highly recommended when precomputing embeddings if the corpus is large. Not needed anymore after precomputing ranker.CrossEncoder \u274c Highly recommended since ranker.ZeroShot cannot precompute embeddings ranker.Embedding \u274c Not needed <p>The <code>rank.Encoder</code> and <code>rank.DPR</code> rankers pre-compute the document embeddings once for all with the <code>add</code> method. This step can be time-consuming if we do not have a GPU. The embeddings are pre-computed so that the model can then rank the retriever documents at lightning speed.</p>"},{"location":"rank/rank/#requirements","title":"Requirements","text":"<p>To use the Encoder ranker we will need to install \"cherche[cpu]\"</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>or on GPU:</p> <pre><code>pip install \"cherche[gpu]\"\n</code></pre>"},{"location":"rank/rank/#tutorial","title":"Tutorial","text":"<pre><code>&gt;&gt;&gt; from cherche import retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer(f\"sentence-transformers/all-mpnet-base-v2\").encode,\n... )\n\n# Pre-compute embeddings\n&gt;&gt;&gt; ranker.add(documents, batch_size=64)\n\n&gt;&gt;&gt; match = retriever([\"paris\", \"art\", \"fashion\"], k=100)\n\n# Re-rank output of retriever\n&gt;&gt;&gt; ranker([\"paris\", \"art\", \"fashion\"], documents=match, k=30)\n[[{'id': 0, 'similarity': 0.6638489},\n  {'id': 2, 'similarity': 0.602515},\n  {'id': 1, 'similarity': 0.60133684}],\n [{'id': 1, 'similarity': 0.10321068}],\n [{'id': 1, 'similarity': 0.26405674}, {'id': 2, 'similarity': 0.096046045}]]\n</code></pre>"},{"location":"retrieve/bm25/","title":"BM25","text":"<p>Our BM25 retriever relies on the sparse.BM25Vectorizer of LeNLP.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.BM25(key=\"id\", on=[\"title\", \"article\"], documents=documents, k=30)\n\n&gt;&gt;&gt; retriever(\"france\")\n[{'id': 0, 'similarity': 0.1236413097778466},\n {'id': 2, 'similarity': 0.08907655343363269},\n {'id': 1, 'similarity': 0.0031730868527342104}]\n</code></pre> <p>We can also initialize the retriever with a custom sparse.BM25Vectorizer.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from lenlp import sparse\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; count_vectorizer = sparse.BM25Vectorizer(\n...  normalize=True, ngram_range=(3, 7), analyzer=\"char_wb\")\n\n&gt;&gt;&gt; retriever = retrieve.BM25Vectorizer(\n...  key=\"id\", on=[\"title\", \"article\"], documents=documents, count_vectorizer=count_vectorizer)\n\n&gt;&gt;&gt; retriever(\"fra\", k=3)\n[{'id': 0, 'similarity': 0.15055477454160002},\n {'id': 2, 'similarity': 0.022883459495904895}]\n</code></pre>"},{"location":"retrieve/bm25/#batch-retrieval","title":"Batch retrieval","text":"<p>If we have several queries for which we want to retrieve the top k documents then we can pass a list of queries to the retriever. This is much faster for multiple queries. In batch-mode, retriever returns a list of list of documents instead of a list of documents.</p> <pre><code>&gt;&gt;&gt; retriever([\"fra\", \"arts\", \"capital\"], k=3)\n[[{'id': 0, 'similarity': 0.051000705070125066}, # Match query 1\n  {'id': 2, 'similarity': 0.03415513704304113}],\n [{'id': 1, 'similarity': 0.07021399356970497}], # Match query 2\n [{'id': 0, 'similarity': 0.25972148184421534}]] # Match query 3\n</code></pre>"},{"location":"retrieve/bm25/#map-keys-to-documents","title":"Map keys to documents","text":"<p>We can map documents to retrieved keys.</p> <pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever(\"fra\")\n[{'id': 0,\n  'article': 'Paris is the capital and most populous city of France',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.15055477454160002},\n {'id': 2,\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.022883459495904895}]\n</code></pre>"},{"location":"retrieve/dpr/","title":"DPR","text":"<p>The <code>retriever.DPR</code> model uses DPR-based models that encode queries and documents with two distinct models. It is compatible with the SentenceTransformers and Hugging Face similarity DPR models.</p> <p>To use the DPR retriever we will need to install \"cherche[cpu]\"</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>The <code>retriever.DPR</code> uses Faiss to store pre-computed document embeddings in an index. This index allows for fast nearest neighbor search.</p> <p>By default, <code>retriever.DPR</code> uses the <code>IndexFlatL2</code> index which is stored in memory and called via the CPU. However, Faiss offers various algorithms suitable for different corpus sizes and speed constraints. To choose the most suitable index for your use case, you can refer to Faiss guidelines.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.DPR(\n...    encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base').encode,\n...    query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base').encode,\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    normalize = True,\n... )\n\n&gt;&gt;&gt; retriever = retriever.add(documents=documents)\n\n&gt;&gt;&gt; retriever([\"paris\", \"arts and science\"], k=3, batch_size=64)\n[[{'id': 1, 'similarity': 0.6063913848352276},\n  {'id': 0, 'similarity': 0.6021773868199615},\n  {'id': 2, 'similarity': 0.5844722795720981}],\n [{'id': 1, 'similarity': 0.5060106120613739},\n  {'id': 0, 'similarity': 0.4877345511626579},\n  {'id': 2, 'similarity': 0.4864927436178843}]]\n</code></pre>"},{"location":"retrieve/dpr/#run-dpr-on-gpu","title":"Run DPR on GPU","text":"<p>Let's create a faiss index stored in memory that runs on GPU with an encoder that also run on GPU. The retriever will run much faster.</p> <p>First we need to uninstall vanilla cherche and install \"cherche[gpu]\":</p> <pre><code>pip uninstall cherche\n</code></pre> <pre><code>pip install \"cherche[gpu]\"\n</code></pre> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; import faiss\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\")\n\n&gt;&gt;&gt; d = encoder.encode(\"Embeddings size.\").shape[0] # embeddings_dim\n&gt;&gt;&gt; index = faiss.IndexFlatL2(d)\n# # 0 is the id of the GPU\n&gt;&gt;&gt; index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, index)\n\n&gt;&gt;&gt; retriever = retrieve.DPR(\n...    encoder = encoder.encode,\n...    query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base', device=\"cuda\").encode,\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    normalize = True,\n... )\n\n&gt;&gt;&gt; retriever.add(documents)\n\n&gt;&gt;&gt; retriever(\"paris\")\n[{'id': 0,\n  'similarity': 0.9025790931437582},\n {'id': 2,\n  'similarity': 0.8160134832855334}]\n</code></pre>"},{"location":"retrieve/dpr/#map-keys-to-documents","title":"Map keys to documents","text":"<pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever([\"france\", \"arts\"], k=1, batch_size=64)\n[[{'id': 0,\n   'article': 'Paris is the capital and most populous city of France',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 0.5816614494510223}],\n [{'id': 1,\n   'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n   'title': 'Paris',\n   'url': 'https://en.wikipedia.org/wiki/Paris',\n   'similarity': 0.5130107727511707}]]\n</code></pre>"},{"location":"retrieve/embedding/","title":"Embedding","text":""},{"location":"retrieve/embedding/#embedding","title":"Embedding","text":"<p>The <code>retriever.Embedding</code> is a powerful tool when you need to retrieve documents based on a custom model or embeddings. The embeddings of documents must be of shape (n_documents, dim_embeddings). We can add embeddings in a streaming fashion way (no need to add all documents embeddings at once).</p> <p>To use the Embedding retriever we will need to install \"cherche[cpu]\"</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>The <code>retriever.Embedding</code> uses Faiss to store pre-computed document embeddings in an index. This index allows for fast nearest neighbor search.</p> <p>By default, <code>retriever.Embedding</code> uses the <code>IndexFlatL2</code> index which is stored in memory and called via the CPU. However, Faiss offers various algorithms suitable for different corpus sizes and speed constraints. To choose the most suitable index for your use case, you can refer to Faiss guidelines.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"content\": \"Paris is the capital and most populous city of France\",\n...    },\n...    {\n...        \"id\": 1,\n...        \"content\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...    },\n...    {\n...        \"id\": 2,\n...        \"content\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...    }\n... ]\n\n# Let's use a custom encoder and create our documents embeddings of shape (n_documents, dim_embeddings)\n&gt;&gt;&gt; encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n&gt;&gt;&gt; embeddings_documents = encoder.encode([\n...    document[\"content\"] for document in documents\n... ])\n\n&gt;&gt;&gt; retriever = retrieve.Embedding(\n...    key = \"id\",\n...    normalize = True,\n... )\n\n&gt;&gt;&gt; retriever = retriever.add(\n...    documents=documents,\n...    embeddings_documents=embeddings_documents,\n... )\n\n&gt;&gt;&gt; queries = [\n...    \"Paris\",\n...    \"Madrid\",\n...    \"Montreal\"\n... ]\n\n# Queries embeddings of shape (n_queries, dim_embeddings)\n&gt;&gt;&gt; embeddings_queries = encoder.encode(queries)\n&gt;&gt;&gt; retriever(embeddings_queries, k=2, batch_size=256)\n[[{'id': 0, 'similarity': 0.5924658608170578},\n  {'id': 1, 'similarity': 0.5446812754643415}],\n [{'id': 0, 'similarity': 0.40650240404418286},\n  {'id': 1, 'similarity': 0.39610636961156953}],\n [{'id': 0, 'similarity': 0.42386890080792006},\n  {'id': 2, 'similarity': 0.41253893705647166}]]\n</code></pre>"},{"location":"retrieve/embedding/#batch-computation","title":"Batch computation","text":"<p>If we have several queries for which we want to retrieve the top k documents then we can pass a list of queries to the retriever. This is much faster for multiple queries. In batch-mode, retriever returns a list of list of documents instead of a list of documents.</p> <pre><code>&gt;&gt;&gt; retriever([\"paris\", \"arts\"], k=30, batch_size=256)\n[[{'id': 0, 'similarity': 0.5979780283951969}, # Match query 1\n  {'id': 2, 'similarity': 0.5571123641024619},\n  {'id': 1, 'similarity': 0.5563819725806741}],\n [{'id': 1, 'similarity': 0.38966597854511925}, # Match query 2\n  {'id': 0, 'similarity': 0.36300965990952766},\n  {'id': 2, 'similarity': 0.356841141737425}]]\n</code></pre>"},{"location":"retrieve/embedding/#run-embedding-on-gpu","title":"Run embedding on GPU","text":"<p>Let's create a faiss index stored in memory that runs on GPU with the sentence transformer as an encoder that also runs on GPU. The retriever will run much faster.</p> <p>First we need to uninstall vanilla cherche and install \"cherche[gpu]\":</p> <pre><code>pip uninstall cherche\n</code></pre> <pre><code>pip install \"cherche[gpu]\"\n</code></pre> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; import faiss\n\n&gt;&gt;&gt; encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\")\n&gt;&gt;&gt; d = encoder.encode(\"Embeddings size.\").shape[0] # dim_embeddings\n&gt;&gt;&gt; index = faiss.IndexFlatL2(d)\n&gt;&gt;&gt; index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, index) # 0 is the id of the GPU\n</code></pre> <pre><code>&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Embedding(\n...    key = \"id\",\n...    index = index, # Our index running on GPU.\n... )\n\n# Documents embeddings of shape (n_documents, dim_embeddings)\n&gt;&gt;&gt; embeddings_documents = encoder.encode([\n...    document[\"content\"] for document in documents\n... ])\n\n&gt;&gt;&gt; retriever = retriever.add(\n...    documents=documents,\n...    embeddings_documents=embeddings_documents,\n... )\n\n&gt;&gt;&gt; queries = [\n...    \"Paris\",\n...    \"Madrid\",\n...    \"Montreal\"\n... ]\n\n# Queries embeddings of shape (n_queries, dim_embeddings)\n&gt;&gt;&gt; embeddings_queries = encoder.encode(queries)\n&gt;&gt;&gt; retriever(embeddings_queries, k=2, batch_size=256)\n[[{'id': 0, 'similarity': 0.5924658608170578},\n  {'id': 1, 'similarity': 0.5446812754643415}],\n [{'id': 0, 'similarity': 0.40650240404418286},\n  {'id': 1, 'similarity': 0.39610636961156953}],\n [{'id': 0, 'similarity': 0.42386890080792006},\n  {'id': 2, 'similarity': 0.41253893705647166}]]\n</code></pre>"},{"location":"retrieve/embedding/#map-keys-to-documents","title":"Map keys to documents","text":"<pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever(\"france\")\n[{'id': 0,\n  'article': 'Paris is the capital and most populous city of France',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.9025790931437582},\n {'id': 2,\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.8160134832855334}]\n</code></pre>"},{"location":"retrieve/encoder/","title":"Encoder","text":""},{"location":"retrieve/encoder/#encoder","title":"Encoder","text":"<p>To use the Encoder retriever we will need to install \"cherche[cpu]\"</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>The <code>retriever.Encoder</code> is a powerful tool when you need to retrieve documents based on semantic similarity of the query. It encodes both queries and documents within a single model. The <code>retriever.Encoder</code> model is compatible with the SentenceTransformers and Hugging Face similarity models.</p> <p>Using the <code>retriever.Encoder</code>, you can index your documents, and update them in mini-batches using the <code>add</code> method. However, note that the <code>add</code> method can take some time since the encoder pre-computes document embeddings and stores them in the index.</p> <p>The <code>retriever.Encoder</code> uses Faiss to store pre-computed document embeddings in an index. This index allows for fast nearest neighbor search.</p> <p>By default, <code>retriever.Encoder</code> uses the <code>IndexFlatL2</code> index which is stored in memory and called via the CPU. However, Faiss offers various algorithms suitable for different corpus sizes and speed constraints. To choose the most suitable index for your use case, you can refer to Faiss guidelines.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Encoder(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n...    normalize = True,\n... )\n\n&gt;&gt;&gt; retriever = retriever.add(documents=documents)\n\n&gt;&gt;&gt; retriever(\"paris\", k=30)\n[{'id': 0, 'similarity': 0.5979780497084908},\n {'id': 2, 'similarity': 0.5571123271029782},\n {'id': 1, 'similarity': 0.5563819541294073}]\n</code></pre>"},{"location":"retrieve/encoder/#batch-computation","title":"Batch computation","text":"<p>If we have several queries for which we want to retrieve the top k documents then we can pass a list of queries to the retriever. This is much faster for multiple queries. In batch-mode, retriever returns a list of list of documents instead of a list of documents.</p> <pre><code>&gt;&gt;&gt; retriever([\"paris\", \"arts\"], k=30, batch_size=64)\n[[{'id': 0, 'similarity': 0.5979780283951969}, # Match query 1\n  {'id': 2, 'similarity': 0.5571123641024619},\n  {'id': 1, 'similarity': 0.5563819725806741}],\n [{'id': 1, 'similarity': 0.38966597854511925}, # Match query 2\n  {'id': 0, 'similarity': 0.36300965990952766},\n  {'id': 2, 'similarity': 0.356841141737425}]]\n</code></pre>"},{"location":"retrieve/encoder/#run-encoder-on-gpu","title":"Run encoder on GPU","text":"<p>Let's create a faiss index stored in memory that runs on GPU with the sentence transformer as an encoder that also runs on GPU.</p> <p>First we need to uninstall vanilla cherche and install \"cherche[gpu]\":</p> <pre><code>pip uninstall cherche\n</code></pre> <pre><code>pip install \"cherche[gpu]\"\n</code></pre> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; import faiss\n\n&gt;&gt;&gt; encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\")\n&gt;&gt;&gt; d = encoder.encode(\"Embeddings size.\").shape[0]\n&gt;&gt;&gt; index = faiss.IndexFlatL2(d)\n&gt;&gt;&gt; index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, index) # 0 is the id of the GPU\n</code></pre> <pre><code>&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Encoder(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = encoder.encode,\n...    index = index, # Our index running on GPU.\n...    normalize = True,\n... )\n\n&gt;&gt;&gt; retriever.add(documents)\n\n&gt;&gt;&gt; retriever([\"paris\", \"arts\"], k=30, batch_size=64)\n[[{'id': 0, 'similarity': 0.5979780283951969}, # Match query 1\n  {'id': 2, 'similarity': 0.5571123641024619},\n  {'id': 1, 'similarity': 0.5563819725806741}],\n [{'id': 1, 'similarity': 0.38966597854511925}, # Match query 2\n  {'id': 0, 'similarity': 0.36300965990952766},\n  {'id': 2, 'similarity': 0.356841141737425}]]\n</code></pre>"},{"location":"retrieve/encoder/#map-keys-to-documents","title":"Map keys to documents","text":"<pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever(\"france\")\n[{'id': 0,\n  'article': 'Paris is the capital and most populous city of France',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.9025790931437582},\n {'id': 2,\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.8160134832855334}]\n</code></pre>"},{"location":"retrieve/flash/","title":"Flash","text":"<p>Flash is a wrapper of FlashText. This great algorithm can retrieve keywords in documents faster than anything else. We can find more information about Flash in Replace or Retrieve Keywords In Documents At Scale.</p> <p>We can use Flash to find documents from a field that contains a keyword or a list of keywords. Flash will find documents that contain the keyword or keywords specified in the query.</p> <p>We can update the Flash retriever with new documents using mini-batch via the <code>add</code> method.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n...        \"tags\": [\"paris\", \"france\", \"capital\"]\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n...        \"tags\": [\"paris\", \"france\", \"capital\", \"fashion\"]\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n...        \"tags\": \"paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Flash(key=\"id\", on=\"tags\")\n\n&gt;&gt;&gt; retriever.add(documents=documents)\n\n&gt;&gt;&gt; retriever(\"fashion\")\n[{'id': 1}]\n</code></pre>"},{"location":"retrieve/flash/#map-keys-to-documents","title":"Map keys to documents","text":"<pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever(\"fashion\")\n[{'id': 1,\n  'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'tags': ['paris', 'france', 'capital', 'fashion']}]\n</code></pre>"},{"location":"retrieve/fuzz/","title":"Fuzz","text":"<p><code>retrieve.Fuzz</code> is a wrapper of RapidFuzz. It is a blazing fast library dedicated to fuzzy string matching. Documents can be indexed online with this retriever using the <code>add</code> method.</p> <p>RapidFuzz provides more scoring functions for the fuzzy string matching task. We can select the most suitable method for our dataset with the <code>fuzzer</code> parameter. The default scoring function is <code>fuzz.partial_ratio</code>.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from rapidfuzz import fuzz\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n# List of available scoring function\n&gt;&gt;&gt; scoring = [\n...     fuzz.ratio,\n...     fuzz.partial_ratio,\n...     fuzz.token_set_ratio,\n...     fuzz.partial_token_set_ratio,\n...     fuzz.token_sort_ratio,\n...     fuzz.partial_token_sort_ratio,\n...     fuzz.token_ratio,\n...     fuzz.partial_token_ratio,\n...     fuzz.WRatio,\n...     fuzz.QRatio,\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Fuzz(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    fuzzer = fuzz.partial_ratio, # Choose the scoring function.\n... )\n\n# Index documents\n&gt;&gt;&gt; retriever.add(documents)\n\n&gt;&gt;&gt; retriever(\"fashion\", k=2)\n[{'id': 1, 'similarity': 100.0}, {'id': 0, 'similarity': 46.15384615384615}]\n</code></pre>"},{"location":"retrieve/fuzz/#batch-retrieval","title":"Batch retrieval","text":"<p>If we have several queries for which we want to retrieve the top k documents then we can pass a list of queries to the retriever. In batch-mode, retriever returns a list of list of documents instead of a list of documents.</p> <pre><code>&gt;&gt;&gt; retriever([\"france\", \"arts\", \"capital\"], k=30)\n[[{'id': 0, 'similarity': 100.0}, # Match query 1\n  {'id': 2, 'similarity': 100.0},\n  {'id': 1, 'similarity': 66.66666666666667}],\n [{'id': 1, 'similarity': 100.0}, # Match query 2\n  {'id': 0, 'similarity': 75.0},\n  {'id': 2, 'similarity': 75.0}],\n [{'id': 0, 'similarity': 100.0}, # Match query 3\n  {'id': 1, 'similarity': 44.44444444444444},\n  {'id': 2, 'similarity': 44.44444444444444}]]\n</code></pre>"},{"location":"retrieve/fuzz/#map-keys-to-documents","title":"Map keys to documents","text":"<p>We can map documents to retrieved keys.</p> <pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever(\"fashion\", k=30)\n[{'id': 1,\n  'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 100.0},\n {'id': 0,\n  'article': 'Paris is the capital and most populous city of France',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 46.15384615384615},\n {'id': 2,\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 46.15384615384615}]\n</code></pre>"},{"location":"retrieve/lunr/","title":"Lunr","text":"<p><code>retrieve.Lunr</code> is a wrapper of Lunr.py. It is a powerful and practical solution for searching inside a corpus of documents without using a retriever such as Elasticsearch when it is not needed. Lunr stores an inverted index in memory.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.Lunr(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n&gt;&gt;&gt; retriever(\"france\", k=30)\n[{'id': 0, 'similarity': 0.605}, {'id': 2, 'similarity': 0.47}]\n</code></pre>"},{"location":"retrieve/lunr/#batch-retrieval","title":"Batch retrieval","text":"<p>If we have several queries for which we want to retrieve the top k documents then we can pass a list of queries to the retriever. In batch-mode, retriever returns a list of list of documents instead of a list of documents.</p> <pre><code>&gt;&gt;&gt; retriever([\"france\", \"arts\", \"capital\"], k=30)\n[[{'id': 0, 'similarity': 0.605}, {'id': 2, 'similarity': 0.47}], # Match query 1\n [{'id': 1, 'similarity': 0.802}], # Match query 2\n [{'id': 0, 'similarity': 1.263}]] # Match query 3\n</code></pre>"},{"location":"retrieve/lunr/#map-keys-to-documents","title":"Map keys to documents","text":"<pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever(\"arts\")\n[{'id': 1,\n  'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.802}]\n</code></pre>"},{"location":"retrieve/retrieve/","title":"Retrieve","text":"<p>Retrievers speed up the neural search pipeline by filtering out the majority of documents that are not relevant. Rankers (slower) will then pull up the most relevant documents based on semantic similarity.</p> <p><code>retrieve.Encoder</code>, <code>retrieve.DPR</code> and <code>retrieve.Embedding</code> retrievers rely on semantic similarity, unlike the other retrievers, which match exact words.</p>"},{"location":"retrieve/retrieve/#retrievers","title":"Retrievers","text":"<p>Here is the list of available retrievers using Cherche:</p> <ul> <li><code>retrieve.TfIdf</code></li> <li><code>retrieve.Lunr</code></li> <li><code>retrieve.Flash</code></li> <li><code>retrieve.Fuzz</code></li> <li><code>retrieve.Encoder</code></li> <li><code>retrieve.DPR</code></li> <li><code>retrieve.Embedding</code></li> </ul> <p>To use <code>retrieve.Encoder</code>, <code>retrieve.DPR</code> or <code>retrieve.Embedding</code> we will need to install cherche using:</p> <pre><code>pip install \"cherche[cpu]\"\n</code></pre> <p>If we want to run semantic retrievers on GPU:</p> <pre><code>pip install \"cherche[gpu]\"\n</code></pre>"},{"location":"retrieve/retrieve/#tutorial","title":"Tutorial","text":"<p>The main parameter of retrievers is <code>on</code>; it is the field(s) on which the retriever will perform the search. If multiple fields are specified, the retriever will concatenate all fields in the order provided. The <code>key</code> parameter is the name of the field that contain an unique identifier for the document.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...    },\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n</code></pre> <p>Calling a retriever with a single query output a list of documents. <pre><code>retriever(\"paris\", k=30)\n[{'id': 0, 'similarity': 0.21638007903488998},\n {'id': 1, 'similarity': 0.13897776006154242}]\n</code></pre></p> <p>Calling a retriever with a list of queries output a list of list of documents. If we want to call retriever on multiples queries, we should opt for the following code:</p> <pre><code>retriever([\"paris\", \"art\", \"finance\"], k=30)\n[[{'id': 0, 'similarity': 0.21638007903488998},  # Query 1\n  {'id': 1, 'similarity': 0.13897776006154242}],\n [{'id': 1, 'similarity': 0.03987124117278171}], # Query 2\n [{'id': 1, 'similarity': 0.15208763286878763},  # Query 3\n  {'id': 0, 'similarity': 0.02564158475123616}]]\n</code></pre>"},{"location":"retrieve/retrieve/#parameters","title":"Parameters","text":"Retriever Add Semantic Batch optmized retrieve.Encoder \u2705 \u2705 \u2705 retrieve.DPR \u2705 \u2705 \u2705 retrieve.Embedding \u2705 \u2705 \u2705 retrieve.Flash \u2705 \u274c \u274c retrieve.Fuzz \u2705 \u274c \u274c retrieve.TfIdf \u274c \u274c \u2705 retrieve.Lunr \u274c \u274c \u274c <ul> <li>Add: Retriever has a <code>.add(documents)</code> method to index new documents along the way.</li> <li>Semantic: The Retriever is powered by a language model, enabling semantic similarity-based document retrieval.</li> <li>Batch-Optimized: The Retriever is optimized for batch processing, with a batch_size parameter that can be adjusted to handle multiple queries efficiently.</li> </ul> <p>We can call retrievers with a k-parameter, which enables the selection of the number of documents to be retrieved. By default, the value of k is set to None, meaning that the retrievers will retrieve all documents that match the query. However, if a specific value for k is chosen, the retriever will only retrieve the top k documents that are most likely to match the query.</p> <pre><code>&gt;&gt;&gt; retriever([\"paris\", \"art\"], k=3)\n[[{'id': 0, 'similarity': 0.21638007903488998},\n  {'id': 1, 'similarity': 0.13897776006154242}],\n [{'id': 1, 'similarity': 0.03987124117278171}]]\n</code></pre>"},{"location":"retrieve/retrieve/#matching-indexes-to-documents","title":"Matching indexes to documents","text":"<p>It is possible to directly retrieve the content of the documents using the <code>+</code> operator between retriever and documents. Documents mapping is helpful if we want to plug our retriever on a <code>rank.CrossEncoder</code>.</p> <pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever(\"Paris\")\n[{'id': 0,\n  'article': 'Paris is the capital and most populous city of France',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris'},\n {'id': 1,\n  'article': 'Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris'},\n {'id': 2,\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris'}]\n</code></pre>"},{"location":"retrieve/tfidf/","title":"TfIdf","text":"<p>Our TF-IDF retriever relies on the sparse.TfidfVectorizer of Sklearn. It computes the dot product between the query TF-IDF vector and the documents TF-IDF matrix and retrieves the highest match. TfIdf retriever stores a sparse matrix and an index that links the rows of the matrix to document identifiers.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents, k=30)\n\n&gt;&gt;&gt; retriever(\"france\")\n[{'id': 0, 'similarity': 0.15137222675009282},\n {'id': 2, 'similarity': 0.10831402366399025},\n {'id': 1, 'similarity': 0.02505818772920329}]\n</code></pre> <p>We can also initialize the retriever with a custom sparse.TfidfVectorizer.</p> <pre><code>&gt;&gt;&gt; from cherche import retrieve\n&gt;&gt;&gt; from lenlp import sparse\n\n&gt;&gt;&gt; documents = [\n...    {\n...        \"id\": 0,\n...        \"article\": \"Paris is the capital and most populous city of France\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 1,\n...        \"article\": \"Paris has been one of Europe major centres of finance, diplomacy , commerce , fashion , gastronomy , science , and arts.\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    },\n...    {\n...        \"id\": 2,\n...        \"article\": \"The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .\",\n...        \"title\": \"Paris\",\n...        \"url\": \"https://en.wikipedia.org/wiki/Paris\"\n...    }\n... ]\n\n&gt;&gt;&gt; tfidf = sparse.TfidfVectorizer(\n...  normalize=True, ngram_range=(3, 7), analyzer=\"char_wb\")\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(\n...  key=\"id\", on=[\"title\", \"article\"], documents=documents, tfidf=tfidf)\n\n&gt;&gt;&gt; retriever(\"fra\", k=3)\n[{'id': 0, 'similarity': 0.15055477454160002},\n {'id': 2, 'similarity': 0.022883459495904895}]\n</code></pre>"},{"location":"retrieve/tfidf/#batch-retrieval","title":"Batch retrieval","text":"<p>If we have several queries for which we want to retrieve the top k documents then we can pass a list of queries to the retriever. This is much faster for multiple queries. In batch-mode, retriever returns a list of list of documents instead of a list of documents.</p> <pre><code>&gt;&gt;&gt; retriever([\"fra\", \"arts\", \"capital\"], k=3)\n[[{'id': 0, 'similarity': 0.051000705070125066}, # Match query 1\n  {'id': 2, 'similarity': 0.03415513704304113}],\n [{'id': 1, 'similarity': 0.07021399356970497}], # Match query 2\n [{'id': 0, 'similarity': 0.25972148184421534}]] # Match query 3\n</code></pre>"},{"location":"retrieve/tfidf/#map-keys-to-documents","title":"Map keys to documents","text":"<p>We can map documents to retrieved keys.</p> <pre><code>&gt;&gt;&gt; retriever += documents\n&gt;&gt;&gt; retriever(\"fra\")\n[{'id': 0,\n  'article': 'Paris is the capital and most populous city of France',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.15055477454160002},\n {'id': 2,\n  'article': 'The City of Paris is the centre and seat of government of the region and province of \u00cele-de-France .',\n  'title': 'Paris',\n  'url': 'https://en.wikipedia.org/wiki/Paris',\n  'similarity': 0.022883459495904895}]\n</code></pre>"},{"location":"scripts/","title":"Index","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"This script is responsible for building the API reference. The API reference is located in\ndocs/api. The script scans through all the modules, classes, and functions. It processes\nthe __doc__ of each object and formats it so that MkDocs can process it in turn.\n\"\"\"\nimport functools\nimport importlib\nimport inspect\nimport os\nimport pathlib\nimport re\nimport shutil\n</pre> \"\"\"This script is responsible for building the API reference. The API reference is located in docs/api. The script scans through all the modules, classes, and functions. It processes the __doc__ of each object and formats it so that MkDocs can process it in turn. \"\"\" import functools import importlib import inspect import os import pathlib import re import shutil In\u00a0[\u00a0]: Copied! <pre>from numpydoc.docscrape import ClassDoc, FunctionDoc\n</pre> from numpydoc.docscrape import ClassDoc, FunctionDoc In\u00a0[\u00a0]: Copied! <pre>package = \"cherche\"\n</pre> package = \"cherche\" <p>shutil.copy(\"README.md\", \"docs/index.md\")</p> In\u00a0[\u00a0]: Copied! <pre>def paragraph(text):\n    return f\"{text}\\n\"\n</pre> def paragraph(text):     return f\"{text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def h1(text):\n    return paragraph(f\"# {text}\")\n</pre> def h1(text):     return paragraph(f\"# {text}\") In\u00a0[\u00a0]: Copied! <pre>def h2(text):\n    return paragraph(f\"## {text}\")\n</pre> def h2(text):     return paragraph(f\"## {text}\") In\u00a0[\u00a0]: Copied! <pre>def h3(text):\n    return paragraph(f\"### {text}\")\n</pre> def h3(text):     return paragraph(f\"### {text}\") In\u00a0[\u00a0]: Copied! <pre>def h4(text):\n    return paragraph(f\"#### {text}\")\n</pre> def h4(text):     return paragraph(f\"#### {text}\") In\u00a0[\u00a0]: Copied! <pre>def link(caption, href):\n    return f\"[{caption}]({href})\"\n</pre> def link(caption, href):     return f\"[{caption}]({href})\" In\u00a0[\u00a0]: Copied! <pre>def code(text):\n    return f\"`{text}`\"\n</pre> def code(text):     return f\"`{text}`\" In\u00a0[\u00a0]: Copied! <pre>def li(text):\n    return f\"- {text}\\n\"\n</pre> def li(text):     return f\"- {text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def snake_to_kebab(text):\n    return text.replace(\"_\", \"-\")\n</pre> def snake_to_kebab(text):     return text.replace(\"_\", \"-\") In\u00a0[\u00a0]: Copied! <pre>def inherit_docstring(c, meth):\n\"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class\n    if a class has none. However this doesn't seem to work for Cython classes.\n    \"\"\"\n\n    doc = None\n\n    for ancestor in inspect.getmro(c):\n        try:\n            ancestor_meth = getattr(ancestor, meth)\n        except AttributeError:\n            break\n        doc = inspect.getdoc(ancestor_meth)\n        if doc:\n            break\n\n    return doc\n</pre> def inherit_docstring(c, meth):     \"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class     if a class has none. However this doesn't seem to work for Cython classes.     \"\"\"      doc = None      for ancestor in inspect.getmro(c):         try:             ancestor_meth = getattr(ancestor, meth)         except AttributeError:             break         doc = inspect.getdoc(ancestor_meth)         if doc:             break      return doc In\u00a0[\u00a0]: Copied! <pre>def inherit_signature(c, method_name):\n    m = getattr(c, method_name)\n    sig = inspect.signature(m)\n\n    params = []\n\n    for param in sig.parameters.values():\n        if param.name == \"self\" or param.annotation is not param.empty:\n            params.append(param)\n            continue\n\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            try:\n                ancestor_param = ancestor_meth.parameters[param.name]\n            except KeyError:\n                break\n            if ancestor_param.annotation is not param.empty:\n                param = param.replace(annotation=ancestor_param.annotation)\n                break\n\n        params.append(param)\n\n    return_annotation = sig.return_annotation\n    if return_annotation is inspect._empty:\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            if ancestor_meth.return_annotation is not inspect._empty:\n                return_annotation = ancestor_meth.return_annotation\n                break\n\n    return sig.replace(parameters=params, return_annotation=return_annotation)\n</pre> def inherit_signature(c, method_name):     m = getattr(c, method_name)     sig = inspect.signature(m)      params = []      for param in sig.parameters.values():         if param.name == \"self\" or param.annotation is not param.empty:             params.append(param)             continue          for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             try:                 ancestor_param = ancestor_meth.parameters[param.name]             except KeyError:                 break             if ancestor_param.annotation is not param.empty:                 param = param.replace(annotation=ancestor_param.annotation)                 break          params.append(param)      return_annotation = sig.return_annotation     if return_annotation is inspect._empty:         for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             if ancestor_meth.return_annotation is not inspect._empty:                 return_annotation = ancestor_meth.return_annotation                 break      return sig.replace(parameters=params, return_annotation=return_annotation) In\u00a0[\u00a0]: Copied! <pre>def snake_to_kebab(snake: str) -&gt; str:\n    return snake.replace(\"_\", \"-\")\n</pre> def snake_to_kebab(snake: str) -&gt; str:     return snake.replace(\"_\", \"-\") In\u00a0[\u00a0]: Copied! <pre>def pascal_to_kebab(string):\n    string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)\n    string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower()\n</pre> def pascal_to_kebab(string):     string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)     string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)     return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower() In\u00a0[\u00a0]: Copied! <pre>class Linkifier:\n    def __init__(self):\n        path_index = {}\n        name_index = {}\n\n        modules = {\n            module: importlib.import_module(f\"{package}.{module}\")\n            for module in importlib.import_module(f\"{package}\").__all__\n        }\n\n        def index_module(mod_name, mod, path):\n            path = os.path.join(path, mod_name)\n            dotted_path = path.replace(\"/\", \".\")\n\n            for func_name, func in inspect.getmembers(mod, inspect.isfunction):\n                for e in (\n                    f\"{mod_name}.{func_name}\",\n                    f\"{dotted_path}.{func_name}\",\n                    f\"{func.__module__}.{func_name}\",\n                ):\n                    path_index[e] = os.path.join(path, snake_to_kebab(func_name))\n                    name_index[e] = f\"{dotted_path}.{func_name}\"\n\n            for klass_name, klass in inspect.getmembers(mod, inspect.isclass):\n                for e in (\n                    f\"{mod_name}.{klass_name}\",\n                    f\"{dotted_path}.{klass_name}\",\n                    f\"{klass.__module__}.{klass_name}\",\n                ):\n                    path_index[e] = os.path.join(path, klass_name)\n                    name_index[e] = f\"{dotted_path}.{klass_name}\"\n\n            for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):\n                if submod_name not in mod.__all__ or submod_name == \"typing\":\n                    continue\n                for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):\n                    path_index[e] = os.path.join(path, snake_to_kebab(submod_name))\n\n                # Recurse\n                index_module(submod_name, submod, path=path)\n\n        for mod_name, mod in modules.items():\n            index_module(mod_name, mod, path=\"\")\n\n        # Prepend {package} to each index entry\n        for k in list(path_index.keys()):\n            path_index[f\"{package}.{k}\"] = path_index[k]\n        for k in list(name_index.keys()):\n            name_index[f\"{package}.{k}\"] = name_index[k]\n\n        self.path_index = path_index\n        self.name_index = name_index\n\n    def linkify(self, text, use_fences, depth):\n        path = self.path_index.get(text)\n        name = self.name_index.get(text)\n        if path and name:\n            backwards = \"../\" * (depth + 1)\n            if use_fences:\n                return f\"[`{name}`]({backwards}{path})\"\n            return f\"[{name}]({backwards}{path})\"\n        return None\n\n    def linkify_fences(self, text, depth):\n        between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")\n        return between_fences.sub(\n            lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text\n        )\n\n    def linkify_dotted(self, text, depth):\n        dotted = re.compile(\"\\w+\\.[\\.\\w]+\")\n        return dotted.sub(\n            lambda x: self.linkify(x.group(), False, depth) or x.group(), text\n        )\n</pre> class Linkifier:     def __init__(self):         path_index = {}         name_index = {}          modules = {             module: importlib.import_module(f\"{package}.{module}\")             for module in importlib.import_module(f\"{package}\").__all__         }          def index_module(mod_name, mod, path):             path = os.path.join(path, mod_name)             dotted_path = path.replace(\"/\", \".\")              for func_name, func in inspect.getmembers(mod, inspect.isfunction):                 for e in (                     f\"{mod_name}.{func_name}\",                     f\"{dotted_path}.{func_name}\",                     f\"{func.__module__}.{func_name}\",                 ):                     path_index[e] = os.path.join(path, snake_to_kebab(func_name))                     name_index[e] = f\"{dotted_path}.{func_name}\"              for klass_name, klass in inspect.getmembers(mod, inspect.isclass):                 for e in (                     f\"{mod_name}.{klass_name}\",                     f\"{dotted_path}.{klass_name}\",                     f\"{klass.__module__}.{klass_name}\",                 ):                     path_index[e] = os.path.join(path, klass_name)                     name_index[e] = f\"{dotted_path}.{klass_name}\"              for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):                 if submod_name not in mod.__all__ or submod_name == \"typing\":                     continue                 for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):                     path_index[e] = os.path.join(path, snake_to_kebab(submod_name))                  # Recurse                 index_module(submod_name, submod, path=path)          for mod_name, mod in modules.items():             index_module(mod_name, mod, path=\"\")          # Prepend {package} to each index entry         for k in list(path_index.keys()):             path_index[f\"{package}.{k}\"] = path_index[k]         for k in list(name_index.keys()):             name_index[f\"{package}.{k}\"] = name_index[k]          self.path_index = path_index         self.name_index = name_index      def linkify(self, text, use_fences, depth):         path = self.path_index.get(text)         name = self.name_index.get(text)         if path and name:             backwards = \"../\" * (depth + 1)             if use_fences:                 return f\"[`{name}`]({backwards}{path})\"             return f\"[{name}]({backwards}{path})\"         return None      def linkify_fences(self, text, depth):         between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")         return between_fences.sub(             lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text         )      def linkify_dotted(self, text, depth):         dotted = re.compile(\"\\w+\\.[\\.\\w]+\")         return dotted.sub(             lambda x: self.linkify(x.group(), False, depth) or x.group(), text         ) In\u00a0[\u00a0]: Copied! <pre>def concat_lines(lines):\n    return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines))\n</pre> def concat_lines(lines):     return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines)) In\u00a0[\u00a0]: Copied! <pre>def print_docstring(obj, file, depth):\n\"\"\"Prints a classes's docstring to a file.\"\"\"\n\n    doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)\n\n    printf = functools.partial(print, file=file)\n\n    printf(h1(obj.__name__))\n    printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))\n    printf(\n        linkifier.linkify_fences(\n            paragraph(concat_lines(doc[\"Extended Summary\"])), depth\n        )\n    )\n\n    # We infer the type annotations from the signatures, and therefore rely on the signature\n    # instead of the docstring for documenting parameters\n    try:\n        signature = inspect.signature(obj)\n    except ValueError:\n        signature = (\n            inspect.Signature()\n        )  # TODO: this is necessary for Cython classes, but it's not correct\n    params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}\n\n    # Parameters\n    if signature.parameters:\n        printf(h2(\"Parameters\"))\n    for param in signature.parameters.values():\n        # Name\n        printf(f\"- **{param.name}**\", end=\"\")\n        # Type annotation\n        if param.annotation is not param.empty:\n            anno = inspect.formatannotation(param.annotation)\n            anno = linkifier.linkify_dotted(anno, depth)\n            printf(f\" (*{anno}*)\", end=\"\")\n        # Default value\n        if param.default is not param.empty:\n            printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        if param.name in params_desc:\n            desc = params_desc[param.name]\n            if desc:\n                printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Attributes\n    if doc[\"Attributes\"]:\n        printf(h2(\"Attributes\"))\n    for attr in doc[\"Attributes\"]:\n        # Name\n        printf(f\"- **{attr.name}**\", end=\"\")\n        # Type annotation\n        if attr.type:\n            printf(f\" (*{attr.type}*)\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        desc = \" \".join(attr.desc)\n        if desc:\n            printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Examples\n    if doc[\"Examples\"]:\n        printf(h2(\"Examples\"))\n\n        in_code = False\n        after_space = False\n\n        for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():\n            if (\n                in_code\n                and after_space\n                and line\n                and not line.startswith(\"&gt;&gt;&gt;\")\n                and not line.startswith(\"...\")\n            ):\n                printf(\"```\\n\")\n                in_code = False\n                after_space = False\n\n            if not in_code and line.startswith(\"&gt;&gt;&gt;\"):\n                printf(\"```python\")\n                in_code = True\n\n            after_space = False\n            if not line:\n                after_space = True\n\n            printf(line)\n\n        if in_code:\n            printf(\"```\")\n    printf(\"\")\n\n    # Methods\n    if inspect.isclass(obj) and doc[\"Methods\"]:\n        printf(h2(\"Methods\"))\n        printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)\n\n        for meth in doc[\"Methods\"]:\n            printf(paragraph(f'???- note \"{meth.name}\"'))\n\n            # Parse method docstring\n            docstring = inherit_docstring(c=obj, meth=meth.name)\n            if not docstring:\n                continue\n            meth_doc = FunctionDoc(func=None, doc=docstring)\n\n            printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))\n            if meth_doc[\"Extended Summary\"]:\n                printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))\n\n            # We infer the type annotations from the signatures, and therefore rely on the signature\n            # instead of the docstring for documenting parameters\n            signature = inherit_signature(obj, meth.name)\n            params_desc = {\n                param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]\n            }\n\n            # Parameters\n            if (\n                len(signature.parameters) &gt; 1\n            ):  # signature is never empty, but self doesn't count\n                printf_indent(\"**Parameters**\\n\")\n            for param in signature.parameters.values():\n                if param.name == \"self\":\n                    continue\n                # Name\n                printf_indent(f\"- **{param.name}**\", end=\"\")\n                # Type annotation\n                if param.annotation is not param.empty:\n                    printf_indent(\n                        f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"\n                    )\n                # Default value\n                if param.default is not param.empty:\n                    printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n                printf_indent(\"\", file=file)\n                # Description\n                desc = params_desc.get(param.name)\n                if desc:\n                    printf_indent(f\"    {desc}\")\n            printf_indent(\"\")\n\n            # Returns\n            if meth_doc[\"Returns\"]:\n                printf_indent(\"**Returns**\\n\")\n                return_val = meth_doc[\"Returns\"][0]\n                if signature.return_annotation is not inspect._empty:\n                    if inspect.isclass(signature.return_annotation):\n                        printf_indent(\n                            f\"*{signature.return_annotation.__name__}*: \", end=\"\"\n                        )\n                    else:\n                        printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")\n                printf_indent(return_val.type)\n                printf_indent(\"\")\n\n    # Notes\n    if doc[\"Notes\"]:\n        printf(h2(\"Notes\"))\n        printf(paragraph(\"\\n\".join(doc[\"Notes\"])))\n\n    # References\n    if doc[\"References\"]:\n        printf(h2(\"References\"))\n        printf(paragraph(\"\\n\".join(doc[\"References\"])))\n</pre> def print_docstring(obj, file, depth):     \"\"\"Prints a classes's docstring to a file.\"\"\"      doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)      printf = functools.partial(print, file=file)      printf(h1(obj.__name__))     printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))     printf(         linkifier.linkify_fences(             paragraph(concat_lines(doc[\"Extended Summary\"])), depth         )     )      # We infer the type annotations from the signatures, and therefore rely on the signature     # instead of the docstring for documenting parameters     try:         signature = inspect.signature(obj)     except ValueError:         signature = (             inspect.Signature()         )  # TODO: this is necessary for Cython classes, but it's not correct     params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}      # Parameters     if signature.parameters:         printf(h2(\"Parameters\"))     for param in signature.parameters.values():         # Name         printf(f\"- **{param.name}**\", end=\"\")         # Type annotation         if param.annotation is not param.empty:             anno = inspect.formatannotation(param.annotation)             anno = linkifier.linkify_dotted(anno, depth)             printf(f\" (*{anno}*)\", end=\"\")         # Default value         if param.default is not param.empty:             printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")         printf(\"\\n\", file=file)         # Description         if param.name in params_desc:             desc = params_desc[param.name]             if desc:                 printf(f\"    {desc}\\n\")     printf(\"\")      # Attributes     if doc[\"Attributes\"]:         printf(h2(\"Attributes\"))     for attr in doc[\"Attributes\"]:         # Name         printf(f\"- **{attr.name}**\", end=\"\")         # Type annotation         if attr.type:             printf(f\" (*{attr.type}*)\", end=\"\")         printf(\"\\n\", file=file)         # Description         desc = \" \".join(attr.desc)         if desc:             printf(f\"    {desc}\\n\")     printf(\"\")      # Examples     if doc[\"Examples\"]:         printf(h2(\"Examples\"))          in_code = False         after_space = False          for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():             if (                 in_code                 and after_space                 and line                 and not line.startswith(\"&gt;&gt;&gt;\")                 and not line.startswith(\"...\")             ):                 printf(\"```\\n\")                 in_code = False                 after_space = False              if not in_code and line.startswith(\"&gt;&gt;&gt;\"):                 printf(\"```python\")                 in_code = True              after_space = False             if not line:                 after_space = True              printf(line)          if in_code:             printf(\"```\")     printf(\"\")      # Methods     if inspect.isclass(obj) and doc[\"Methods\"]:         printf(h2(\"Methods\"))         printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)          for meth in doc[\"Methods\"]:             printf(paragraph(f'???- note \"{meth.name}\"'))              # Parse method docstring             docstring = inherit_docstring(c=obj, meth=meth.name)             if not docstring:                 continue             meth_doc = FunctionDoc(func=None, doc=docstring)              printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))             if meth_doc[\"Extended Summary\"]:                 printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))              # We infer the type annotations from the signatures, and therefore rely on the signature             # instead of the docstring for documenting parameters             signature = inherit_signature(obj, meth.name)             params_desc = {                 param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]             }              # Parameters             if (                 len(signature.parameters) &gt; 1             ):  # signature is never empty, but self doesn't count                 printf_indent(\"**Parameters**\\n\")             for param in signature.parameters.values():                 if param.name == \"self\":                     continue                 # Name                 printf_indent(f\"- **{param.name}**\", end=\"\")                 # Type annotation                 if param.annotation is not param.empty:                     printf_indent(                         f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"                     )                 # Default value                 if param.default is not param.empty:                     printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")                 printf_indent(\"\", file=file)                 # Description                 desc = params_desc.get(param.name)                 if desc:                     printf_indent(f\"    {desc}\")             printf_indent(\"\")              # Returns             if meth_doc[\"Returns\"]:                 printf_indent(\"**Returns**\\n\")                 return_val = meth_doc[\"Returns\"][0]                 if signature.return_annotation is not inspect._empty:                     if inspect.isclass(signature.return_annotation):                         printf_indent(                             f\"*{signature.return_annotation.__name__}*: \", end=\"\"                         )                     else:                         printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")                 printf_indent(return_val.type)                 printf_indent(\"\")      # Notes     if doc[\"Notes\"]:         printf(h2(\"Notes\"))         printf(paragraph(\"\\n\".join(doc[\"Notes\"])))      # References     if doc[\"References\"]:         printf(h2(\"References\"))         printf(paragraph(\"\\n\".join(doc[\"References\"]))) In\u00a0[\u00a0]: Copied! <pre>def print_module(mod, path, overview, is_submodule=False):\n    mod_name = mod.__name__.split(\".\")[-1]\n\n    # Create a directory for the module\n    mod_slug = snake_to_kebab(mod_name)\n    mod_path = path.joinpath(mod_slug)\n    mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")\n    os.makedirs(mod_path, exist_ok=True)\n    with open(mod_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(f\"title: {mod_name}\")\n\n    # Add the module to the overview\n    if is_submodule:\n        print(h3(mod_name), file=overview)\n    else:\n        print(h2(mod_name), file=overview)\n    if mod.__doc__:\n        print(paragraph(mod.__doc__), file=overview)\n\n    # Extract all public classes and functions\n    ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")\n    classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))\n    funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))\n\n    # Classes\n\n    if classes and funcs:\n        print(\"\\n**Classes**\\n\", file=overview)\n\n    for _, c in classes:\n        print(f\"{mod_name}.{c.__name__}\")\n\n        # Add the class to the overview\n        slug = snake_to_kebab(c.__name__)\n        print(\n            li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the class' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)\n\n    # Functions\n\n    if classes and funcs:\n        print(\"\\n**Functions**\\n\", file=overview)\n\n    for _, f in funcs:\n        print(f\"{mod_name}.{f.__name__}\")\n\n        # Add the function to the overview\n        slug = snake_to_kebab(f.__name__)\n        print(\n            li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the function' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)\n\n    # Sub-modules\n    for name, submod in inspect.getmembers(mod, inspect.ismodule):\n        # We only want to go through the public submodules, such as optim.schedulers\n        if (\n            name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")\n            or name not in mod.__all__\n            or name.startswith(\"_\")\n        ):\n            continue\n        print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)\n\n    print(\"\", file=overview)\n</pre> def print_module(mod, path, overview, is_submodule=False):     mod_name = mod.__name__.split(\".\")[-1]      # Create a directory for the module     mod_slug = snake_to_kebab(mod_name)     mod_path = path.joinpath(mod_slug)     mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")     os.makedirs(mod_path, exist_ok=True)     with open(mod_path.joinpath(\".pages\"), \"w\") as f:         f.write(f\"title: {mod_name}\")      # Add the module to the overview     if is_submodule:         print(h3(mod_name), file=overview)     else:         print(h2(mod_name), file=overview)     if mod.__doc__:         print(paragraph(mod.__doc__), file=overview)      # Extract all public classes and functions     ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")     classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))     funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))      # Classes      if classes and funcs:         print(\"\\n**Classes**\\n\", file=overview)      for _, c in classes:         print(f\"{mod_name}.{c.__name__}\")          # Add the class to the overview         slug = snake_to_kebab(c.__name__)         print(             li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the class' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)      # Functions      if classes and funcs:         print(\"\\n**Functions**\\n\", file=overview)      for _, f in funcs:         print(f\"{mod_name}.{f.__name__}\")          # Add the function to the overview         slug = snake_to_kebab(f.__name__)         print(             li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the function' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)      # Sub-modules     for name, submod in inspect.getmembers(mod, inspect.ismodule):         # We only want to go through the public submodules, such as optim.schedulers         if (             name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")             or name not in mod.__all__             or name.startswith(\"_\")         ):             continue         print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)      print(\"\", file=overview) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    api_path = pathlib.Path(\"docs/api\")\n\n    # Create a directory for the API reference\n    shutil.rmtree(api_path, ignore_errors=True)\n    os.makedirs(api_path, exist_ok=True)\n    with open(api_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")\n\n    overview = open(api_path.joinpath(\"overview.md\"), \"w\")\n    print(h1(\"Overview\"), file=overview)\n\n    linkifier = Linkifier()\n\n    for mod_name, mod in inspect.getmembers(\n        importlib.import_module(f\"{package}\"), inspect.ismodule\n    ):\n        if mod_name.startswith(\"_\"):\n            continue\n        print(mod_name)\n        print_module(mod, path=api_path, overview=overview)\n</pre> if __name__ == \"__main__\":     api_path = pathlib.Path(\"docs/api\")      # Create a directory for the API reference     shutil.rmtree(api_path, ignore_errors=True)     os.makedirs(api_path, exist_ok=True)     with open(api_path.joinpath(\".pages\"), \"w\") as f:         f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")      overview = open(api_path.joinpath(\"overview.md\"), \"w\")     print(h1(\"Overview\"), file=overview)      linkifier = Linkifier()      for mod_name, mod in inspect.getmembers(         importlib.import_module(f\"{package}\"), inspect.ismodule     ):         if mod_name.startswith(\"_\"):             continue         print(mod_name)         print_module(mod, path=api_path, overview=overview)"},{"location":"serialize/serialize/","title":"Save &amp; Load","text":"<p>Serialization in Python saves an object on the disk to reload it during a new session. Using Cherche, we could prototype a neural search pipeline in a notebook before deploying it on an API. We can also save a neural search pipeline to avoid recomputing embeddings of the ranker.</p> <p>We must ensure that the package versions are identical in both environments (dumping and loading).</p>"},{"location":"serialize/serialize/#saving-and-loading-on-same-environment","title":"Saving and loading on same environment","text":""},{"location":"serialize/serialize/#saving","title":"Saving","text":"<p>We will initialize and save our pipeline in a <code>search.pkl</code> file</p> <pre><code>&gt;&gt;&gt; from cherche import data, retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; import pickle\n\n&gt;&gt;&gt; documents = data.load_towns()\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n... )\n\n&gt;&gt;&gt; search = retriever + ranker\n# Pre-compute embeddings of the ranker\n&gt;&gt;&gt; search.add(documents=documents)\n\n# Dump our pipeline using pickle.\n# The file search.pkl contains our pipeline\n&gt;&gt;&gt; with open(\"search.pkl\", \"wb\") as search_file:\n...    pickle.dump(search, search_file)\n</code></pre>"},{"location":"serialize/serialize/#loading","title":"Loading","text":"<p>After saving our pipeline in the file <code>search.pkl</code>, we can reload it using Pickle.</p> <pre><code>&gt;&gt;&gt; import pickle\n\n&gt;&gt;&gt; with open(\"search.pkl\", \"rb\") as search_file:\n...    search = pickle.load(search_file)\n\n&gt;&gt;&gt; search(\"bordeaux\", k=10)\n[{'id': 57, 'similarity': 0.69513476},\n {'id': 63, 'similarity': 0.6214991},\n {'id': 65, 'similarity': 0.61809057},\n {'id': 59, 'similarity': 0.61285114},\n {'id': 71, 'similarity': 0.5893674},\n {'id': 67, 'similarity': 0.5893066},\n {'id': 74, 'similarity': 0.58757037},\n {'id': 61, 'similarity': 0.58593774},\n {'id': 70, 'similarity': 0.5854107},\n {'id': 66, 'similarity': 0.56525207}]\n</code></pre>"},{"location":"serialize/serialize/#saving-on-gpu-loading-on-cpu","title":"Saving on GPU, loading on CPU","text":"<p>Typically, we could pre-compute the document integration on google collab with a GPU before deploying our neural search pipeline on a CPU-based instance.</p> <p>When transferring the pipeline that runs on the GPU to a machine that will run it on the CPU, it will be necessary to avoid serializing the <code>retrieve.Encoder</code>, <code>retrieve.DPR</code>, <code>rank.DPR</code> and <code>rank.Encoder</code>. These retrievers and rankers would not be compatible if we initialized them on GPU. We will have to replace the models on GPU to put them on CPU. We must ensure that the package versions are strictly identical in both environments (GPU and CPU).</p>"},{"location":"serialize/serialize/#saving-on-gpu","title":"Saving on GPU","text":"<pre><code>&gt;&gt;&gt; from cherche import data, retrieve, rank\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; import pickle\n\n&gt;&gt;&gt; documents = data.load_towns()\n\n&gt;&gt;&gt; retriever = retrieve.TfIdf(key=\"id\", on=[\"title\", \"article\"], documents=documents)\n\n&gt;&gt;&gt; ranker = rank.Encoder(\n...    key = \"id\",\n...    on = [\"title\", \"article\"],\n...    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\").encode,\n... )\n\n&gt;&gt;&gt; search = retriever + ranker\n# Pre-compute embeddings of the ranker\n&gt;&gt;&gt; search.add(documents=documents)\n\n# Replace the GPU-based encoder with a CPU-based encoder.\n&gt;&gt;&gt; ranker.encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode\n\nwith open(\"search.pkl\", \"wb\") as search_file:\n    pickle.dump(search, search_file)\n</code></pre>"},{"location":"serialize/serialize/#loading-on-cpu","title":"Loading on CPU","text":"<p>We can load our neural search pipeline using <code>pickle.load</code> in a new session.</p> <pre><code>&gt;&gt;&gt; import pickle\n\n&gt;&gt;&gt; with open(\"search.pkl\", \"rb\") as search_file:\n...    search = pickle.load(search_file)\n\n&gt;&gt;&gt; search(\"bordeaux\", k=10)\n[{'id': 57, 'similarity': 0.69513476},\n {'id': 63, 'similarity': 0.6214991},\n {'id': 65, 'similarity': 0.61809057},\n {'id': 59, 'similarity': 0.61285114},\n {'id': 71, 'similarity': 0.5893674},\n {'id': 67, 'similarity': 0.5893066},\n {'id': 74, 'similarity': 0.58757037},\n {'id': 61, 'similarity': 0.58593774},\n {'id': 70, 'similarity': 0.5854107},\n {'id': 66, 'similarity': 0.56525207}]\n</code></pre>"}]}